id,title,note,excerpt,url,tags,created,cover,highlights,favorite
838975750,CREATE SCHEMA | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/create-schema,,2024-08-20T16:11:46.134Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcreate-schema,"Highlight:TRANSIENT

Specifies a schema as transient.",false
838173872,Estimating Percentile Values | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/querying-approximate-percentile-values,,2024-08-19T12:42:42.454Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquerying-approximate-percentile-values,"Highlight:The following Aggregate functions are provided for using t-Digest to approximate percentile values:

APPROX_PERCENTILE: Returns an approximation of the desired percentile value.

APPROX_PERCENTILE_ACCUMULATE: Skips the final estimation step and, instead, returns the intermediate t-Digest state at the end of an aggregation.

APPROX_PERCENTILE_COMBINE: Combines (i.e. merges) multiple input states into a single output state.

APPROX_PERCENTILE_ESTIMATE: Computes a percentile estimate of a t-Digest state produced by APPROX_PERCENTILE_ACCUMULATE or APPROX_PERCENTILE_COMBINE.",false
838173596,Estimating Frequent Values | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/querying-approximate-frequent-values,,2024-08-19T12:42:23.924Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquerying-approximate-frequent-values,"Highlight:The following Aggregate functions are provided for using Space-Saving to estimate frequent values:

APPROX_TOP_K: Returns an approximation of frequent values in the input.

APPROX_TOP_K_ACCUMULATE: Skips the final estimation step and returns the Space-Saving state at the end of an aggregation.

APPROX_TOP_K_COMBINE: Combines (i.e. merges) input states into a single output state.

APPROX_TOP_K_ESTIMATE: Computes a cardinality estimate of a Space-Saving state produced by APPROX_TOP_K_ACCUMULATE and APPROX_TOP_K_COMBINE.",false
838173107,Estimating Similarity of Two or More Sets | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/querying-approximate-similarity,,2024-08-19T12:41:30.382Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquerying-approximate-similarity,"Highlight:Snowflake uses MinHash for estimating the approximate similarity between two or more data sets. The MinHash scheme compares sets without computing the intersection or union of the sets, which enables efficient and effective estimation.

Highlight:The following Aggregate functions are provided for estimating approximate similarity using MinHash:

MINHASH: Returns a MinHash state containing a MinHash array of length k (input argument).

MINHASH_COMBINE: Combines two (or more) input MinHash states into a single output MinHash state.

APPROXIMATE_SIMILARITY (or APPROXIMATE_JACCARD_INDEX): Returns an estimation of the similarity (Jaccard index) of input sets based on their MinHash states.",false
838172961,Using Arrays to Compute Distinct Values for Hierarchical Aggregations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/querying-arrays-for-distinct-counts,,2024-08-19T12:41:09.407Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquerying-arrays-for-distinct-counts,"Highlight:If you are counting distinct values for hierarchical aggregations (e.g. multiple grouping sets, rollups, or cubes), you can improve performance by producing ARRAYs that contain the distinct values and computing the number of distinct values from these ARRAYs. Using this approach can be faster than using COUNT(DISTINCT <expr>).",false
838172821,Using Bitmaps to Compute Distinct Values for Hierarchical Aggregations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/querying-bitmaps-for-distinct-counts,,2024-08-19T12:40:45.342Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquerying-bitmaps-for-distinct-counts,"Highlight:If you are counting distinct values for hierarchical aggregations (e.g. multiple grouping sets, rollups, or cubes), you can improve performance by producing bitmaps that represent the distinct values and computing the number of distinct values from these bitmaps. Using this approach can be faster than using COUNT(DISTINCT <expr>).",false
838160501,GET_PRESIGNED_URL | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/get_presigned_url,,2024-08-19T12:13:30.394Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fget_presigned_url,Highlight:Server-side encryption is required on the internal or external stage.,false
838160031,BUILD_SCOPED_FILE_URL | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/build_scoped_file_url,,2024-08-19T12:10:05.454Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fbuild_scoped_file_url,Highlight:A scoped URL is encoded and permits access to a specified file for a limited period of time. The scoped URL in the output is valid for the caller until the persisted query result period ends (until the results cache expires). That period is currently 24 hours.,false
837869702,System functions | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions-system,,2024-08-18T19:47:55.326Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions-system,,false
837844780,EXECUTE TASK | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/execute-task,,2024-08-18T19:27:15.834Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fexecute-task,"Highlight:EXECUTE TASK does not automatically resume child tasks in the task graph. The command skips any child tasks that are suspended.

To recursively resume all dependent tasks tied to a root task in a task graph, query the SYSTEM$TASK_DEPENDENTS_ENABLE",false
837843915,DESCRIBE TASK | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/desc-task,,2024-08-18T19:20:18.101Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fdesc-task,,false
837839461,CREATE TASK | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/create-task,,2024-08-18T19:05:51.147Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcreate-task,"Highlight:The following are supported in a task WHEN clause:

SYSTEM$STREAM_HAS_DATA is supported for evaluation in the SQL expression.",false
837802957,SHOW PIPES | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/show-pipes,,2024-08-18T17:05:46.370Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fshow-pipes,,false
837802842,DESCRIBE PIPE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/desc-pipe,,2024-08-18T17:04:45.855Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fdesc-pipe,,false
837801884,ALTER PIPE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/alter-pipe,,2024-08-18T17:01:13.443Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Falter-pipe,,false
837619382,DROP FILE FORMAT | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/drop-file-format,,2024-08-18T10:54:18.348Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fdrop-file-format,,false
837522071,User-defined functions overview | Snowflake Documentation,,,https://docs.snowflake.com/en/developer-guide/udf/udf-overview,,2024-08-18T08:32:53.745Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fdeveloper-guide%2Fudf%2Fudf-overview,"Highlight:Supported languages¶

You write a function’s handler – its logic – in any of several programming languages. Each language allows you to manipulate data within the constraints of the language and its runtime environment. Regardless of the handler language, you create the procedure itself in the same way using SQL, specifying your handler and handler language.

You can write a handler in any of the following languages:

Language

	

Developer Guides




Java

	

Java UDFs

Java UDTFs




JavaScript

	

JavaScript UDFs

JavaScript UDTFs




Python

	

Python UDFs

Python UDTFs




Scala

	

Scala UDFs




SQL

	

SQL UDFs

SQL UDTFs",false
837520747,CREATE PIPE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/create-pipe#usage-notes,,2024-08-18T08:28:02.983Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcreate-pipe%23usage-notes,"Highlight:All COPY INTO <table> copy options are supported except for the following:

FILES = ( 'file_name1' [ , 'file_name2', ... ] )

ON_ERROR = ABORT_STATEMENT

SIZE_LIMIT = num

PURGE = TRUE | FALSE (i.e. automatic purging while loading)

FORCE = TRUE | FALSE

Note that you can manually remove files from an internal (i.e. Snowflake) stage (after they’ve been loaded) using the REMOVE command.

RETURN_FAILED_ONLY = TRUE | FALSE

VALIDATION_MODE = RETURN_n_ROWS | RETURN_ERRORS | RETURN_ALL_ERRORS",false
837515936,Data Integration | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/ecosystem-etl,,2024-08-18T07:52:43.057Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fecosystem-etl,"Highlight:In addition, the scope of data integration has expanded to include a wider range of operations, including:

Data preparation.

Data migration, movement, and management.

Data warehouse automation.",false
837509774,Grant privileges to other roles | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-exchange-marketplace-privileges#label-create-data-exchange-listing-on-account,,2024-08-18T07:20:07.272Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-exchange-marketplace-privileges%23label-create-data-exchange-listing-on-account,"Highlight:Global CREATE DATA EXCHANGE LISTING privilege¶

If the global CREATE DATA EXCHANGE LISTING privilege is granted to a role, any user with the role can create a listing or provider profile. As the creator and therefore owner of the listing, the role can be used to perform all tasks on the listing, including:

Create listings.

Modify listings properties.

View listings.

View incoming listing requests.

Reject listing requests.

Submit listings for approval.

Publish a listings.

Create and view provider profiles.",false
837509668,Prepare data for a listing | Snowflake Documentation,,,https://other-docs.snowflake.com/en/collaboration/provider-listings-preparing,,2024-08-18T07:19:30.627Z,https://rdl.ink/render/https%3A%2F%2Fother-docs.snowflake.com%2Fen%2Fcollaboration%2Fprovider-listings-preparing,"Highlight:If you choose to limit trial consumers to specific data and functionality, create a single share for your paid listing and use secure views and a system function provided by Snowflake, SYSTEM$IS_LISTING_PURCHASED, to control which data is visible to trial consumers and which data is available only to paying consumers.

Highlight:You cannot transfer the OWNERSHIP privilege for a share.

Highlight:When you offer a paid listing on the Snowflake Marketplace, you must offer consumers the ability to trial the listing before they purchase it. Trials are optional for paid private listings.

Highlight:If your listing includes a secure user-defined function (UDF), you cannot limit visibility of the UDF. Both paying customers and trial customers of your listing can view the secure UDF.

Highlight:When you configure your listing, you can choose to offer it in different regions. Offering listings in other regions requires replicating data.",false
837509512,ALTER EXTERNAL TABLE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/alter-external-table,,2024-08-18T07:18:45.800Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Falter-external-table,"Highlight:ALTER EXTERNAL TABLE¶

Modifies the properties, columns, or constraints for an existing external table.",false
837509137,Estimating the Number of Distinct Values | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/querying-approximate-cardinality,,2024-08-18T07:17:52.388Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquerying-approximate-cardinality,"Highlight:HyperLogLog is a state-of-the-art cardinality estimation algorithm, capable of estimating distinct cardinalities of trillions of rows with an average relative error of a few percent.

HyperLogLog can be used in place of COUNT(DISTINCT …) in situations where estimating cardinality is acceptable.",false
837508874,Understanding & using Time Travel | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-time-travel#restoring-objects,,2024-08-18T07:15:17.733Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-time-travel%23restoring-objects,"Highlight:If an object with the same name already exists, UNDROP fails. You must rename the existing object, which then enables you to restore the previous version of the object.",false
837508799,Snowflake Information Schema | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/info-schema,,2024-08-18T07:14:32.691Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Finfo-schema,"Highlight:What is INFORMATION_SCHEMA?¶

Each database created in your account automatically includes a built-in, read-only schema named INFORMATION_SCHEMA. The schema contains the following objects:

Views for all the objects contained in the database, as well as views for account-level objects (i.e. non-database objects such as roles, warehouses, and databases)

Table functions for historical and usage data across your account.

Highlight:COMPLETE_TASK_GRAPHS

	

60 minutes

	

Results returned only for the ACCOUNTADMIN role, the task owner (i.e. the role with the OWNERSHIP privilege on the task), or a role with the global MONITOR EXECUTION privilege.",false
837508690,Overview of Access Control | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-access-control-overview#roles,,2024-08-18T07:13:52.361Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-access-control-overview%23roles,"Highlight:Although additional privileges can be granted to the system-defined roles, it is not recommended. System-defined roles are created with privileges related to account-management. As a best practice, it is not recommended to mix account-management privileges and entity-specific privileges in the same role. If additional privileges are needed, Snowflake recommends granting the additional privileges to a custom role and assigning the custom role to the system-defined role.

Highlight:Discretionary Access Control (DAC): Each object has an owner, who can in turn grant access to that object.

Role-based Access Control (RBAC): Access privileges are assigned to roles, which are in turn assigned to users.

Highlight:in a managed access schema, object owners lose the ability to make grant decisions. Only the schema owner (i.e. the role with the OWNERSHIP privilege on the schema) or a role with the MANAGE GRANTS privilege can grant privileges on objects in the schema.

Highlight:There are a small number of system-defined roles in a Snowflake account. System-defined roles cannot be dropped. In addition, the privileges granted to these roles by Snowflake cannot be revoked.

Highlight:A role owner (i.e. the role that has the OWNERSHIP privilege on the role) does not inherit the privileges of the owned role. Privilege inheritance is only possible within a role hierarchy.

Highlight:Every active user session has a “current role,” also referred to as a primary role. When a session is initiated (e.g. a user connects via JDBC/ODBC or logs in to the Snowflake web interface), the current role is determined based on the following criteria:

If a role was specified as part of the connection and that role is a role that has already been granted to the connecting user, the specified role becomes the current role.

If no role was specified and a default role has been set for the connecting user, that role becomes the current role.

If no role was specified and a default role has not been set for the connecting user, the system role PUBLIC is used.

Highlight:while a session must have exactly one active primary role at a time, one can activate any number of secondary roles at the same time.

Highlight:A database role can neither be a primary nor a secondary role.

Highlight:There is no concept of a “super-user” or “super-role” in Snowflake that can bypass authorization checks. All access requires appropriate access privileges.",false
837279963,RESULT_SCAN | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/result_scan#usage-notes,,2024-08-17T19:07:27.220Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fresult_scan%23usage-notes,"Highlight:Returns the result set of a previous command (within 24 hours of when you executed the query) as if the result was a table.

Highlight:only the user who runs the original query can use the RESULT_SCAN function to process the output of the query. Even a user with the ACCOUNTADMIN privilege cannot access the results of another user’s query by calling RESULT_SCAN.

Highlight:If the original query is executed via a task, the role that owns the task, instead of a specific user, triggers and runs the query. If a user or a task is operating with the same role, they can use RESULT_SCAN to access the query results.",false
837279858,Writing stored procedures in JavaScript | Snowflake Documentation,,,https://docs.snowflake.com/developer-guide/stored-procedure/stored-procedures-javascript,,2024-08-17T19:06:48.184Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fdeveloper-guide%2Fstored-procedure%2Fstored-procedures-javascript,"Highlight:JavaScript delimiters¶

The JavaScript portion of the stored procedure code must be enclosed within either single quotes ' or double dollar signs $$.",false
837279406,Controlling network traffic with network policies | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/network-policies#activating-network-policies-for-individual-users,,2024-08-17T19:03:24.015Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fnetwork-policies%23activating-network-policies-for-individual-users,"Highlight:Only the role with the OWNERSHIP privilege on both the user and the network policy, or a higher role, can activate a network policy for an individual user.

Highlight:Only security administrators (i.e. users with the SECURITYADMIN role) or higher or a role with the global ATTACH POLICY privilege can activate a network policy for an account.

Highlight:A security administrator (or higher) can use a network policy to allow or deny access to a request based on its origin.

Highlight:Network policies that existed before the introduction of network rules still work. However, all new network policies should use network rules, not the ALLOWED_IP_LIST and BLOCKED_IP_LIST parameters, to control access from IP addresses. Best practice is to avoid using both ways to restrict access in the same network policy.

Highlight:Activate the network policy for an account, user, or security integration.

Highlight:Account:

Network policies applied to an account are the most general network policies. They are overridden by network policies applied to a security integration or user.

Security Integration:

Network policies applied to a security integration override network policies applied to the account, but are overridden by a network policy applied to a user.

User:

Network policies applied to a user are the most specific network policies. They override both accounts and security integrations.

Highlight:when a user-level network policy is associated with the user and the user is already logged into Snowflake, if the user’s network location does not match the user-level network policy rules, Snowflake prevents the user from executing further queries.",false
837279143,Managing and using worksheets in Snowsight | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/ui-snowsight-worksheets#label-snowsight-worksheet-session-context,,2024-08-17T19:01:45.079Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fui-snowsight-worksheets%23label-snowsight-worksheet-session-context,"Highlight:Each worksheet is a unique session and can use roles different from the role you select in the user menu (your active role). Changing your active role does not change the role assigned to the worksheet with the context selector.

Highlight:You must be granted the ACCOUNTADMIN role to recover worksheets of dropped users.",false
837278307,Multi-factor authentication (MFA) | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-mfa#label-mfa-token-caching,,2024-08-17T18:57:34.329Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-mfa%23label-mfa-token-caching,"Highlight:A cached MFA token is valid for up to four hours.

Highlight:MFA login is designed primarily for connecting to Snowflake through the web interface, but is also fully-supported by SnowSQL and the Snowflake JDBC and ODBC drivers.",false
837275274,Object Tagging | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/object-tagging#specify-tag-values,,2024-08-17T18:53:44.016Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fobject-tagging%23specify-tag-values,"Highlight:The ALLOWED_VALUES tag property enables specifying the possible string values that can be assigned to the tag when the tag is set on an object. The maximum number of possible string values for a single tag is 300.

Highlight:A tag is a schema-level object that can be assigned to another Snowflake object.

Highlight:The maximum number of 50 unique tags includes dropped tags for a time period of 24 hours starting from when the tag is dropped using a DROP TAG statement. The reason for this time period is to allow the user who dropped the tag to execute an UNDROP TAG statement, if necessary. When the UNDROP TAG operation executes within the 24-hour time interval, Snowflake restores the tag assignments (i.e. references) that were current prior to the drop operation.",false
837273338,LAST_QUERY_ID | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/last_query_id,,2024-08-17T18:53:03.857Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Flast_query_id,"Highlight:Usage notes¶

Positive numbers start with the first query executed in the session. For example:

LAST_QUERY_ID(1) returns the first query.

LAST_QUERY_ID(2) returns the second query.

LAST_QUERY_ID(6) returns the sixth query.

Etc.

Negative numbers start with the most recently-executed query in the session. For example:

LAST_QUERY_ID(-1) returns the most recently-executed query (equivalent to LAST_QUERY_ID()).

LAST_QUERY_ID(-2) returns the second most recently-executed query.",false
837270214,INSERT | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/insert,,2024-08-17T18:51:31.525Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Finsert,"Highlight:OVERWRITE

Specifies that the target table should be truncated before inserting the values into the table. Note that specifying this option does not affect the access control privileges on the table.

INSERT statements with OVERWRITE can be processed within the scope of the current transaction, which avoids DDL statements that commit a transaction, such as:",false
837270018,Overview of the Kafka connector | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/kafka-connector-overview,,2024-08-17T18:49:39.086Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fkafka-connector-overview,"Highlight:Each Kafka message is passed to Snowflake in JSON format or Avro format. The Kafka connector stores that formatted information in a single column of type VARIANT. The data is not parsed, and the data is not split into multiple columns in the Snowflake table.",false
837269882,Monitor query activity with Query History | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/ui-snowsight-activity#label-exploding-join,,2024-08-17T18:48:51.924Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fui-snowsight-activity%23label-exploding-join,"Highlight:“Exploding” joins¶

One of the common mistakes SQL users make is joining tables without providing a join condition (resulting in a “Cartesian product”), or providing a condition where records from one table match multiple records from another table. For such queries, the Join operator produces significantly (often by orders of magnitude) more tuples than it consumes.",false
837269808,Object name resolution | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/name-resolution,,2024-08-17T18:48:00.446Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fname-resolution,Highlight:The SEARCH_PATH is not used inside views or UDFs. All unqualifed objects in a view or UDF definition will be resolved in the view’s or UDF’s schema only.,false
837269330,SAMPLE / TABLESAMPLE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/constructs/sample#syntax,,2024-08-17T18:44:07.981Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fconstructs%2Fsample%23syntax,"Highlight:BERNOULLI (or ROW): Includes each row with a probability of p/100. Similar to flipping a weighted coin for each row.

SYSTEM (or BLOCK): Includes each block of rows with a probability of p/100. Similar to flipping a weighted coin for each block of rows. This method does not support fixed-size sampling.

Highlight:probability specifies the percentage probability to use for selecting the sample. Can be any decimal number between 0 (no rows selected) and 100 (all rows selected) inclusive.

Highlight:num specifies the number of rows (up to 1,000,000) to sample from the table. Can be any integer between 0 (no rows selected) and 1000000 inclusive.",false
837269153,Controlling network traffic with network policies | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/network-policies#label-associating-network-policies,,2024-08-17T18:42:46.865Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fnetwork-policies%23label-associating-network-policies,"Highlight:You can create multiple network policies, however only one network policy can be associated with an account at any one time. Associating a network policy with your account automatically removes the currently-associated network policy (if any).

Highlight:Only a single network policy can be activated for each user at a time. The ability to activate different network policies for different users allows for granular control. Associating a network policy with a user automatically removes the currently-associated network policy (if any).",false
837268405,Object Tagging | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/object-tagging#label-object-tags-ddl,,2024-08-17T18:36:21.920Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fobject-tagging%23label-object-tags-ddl,"Highlight:Snowflake supports the following DDL to create and manage tags:

CREATE TAG

ALTER TAG

ALTER <object> (to set a tag on a Snowflake object)

SHOW TAGS

DROP TAG

UNDROP TAG

Highlight:Note that Snowflake does not support the describe operation for the tag object.",false
837268355,Introduction to Streams | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/streams-intro#label-streams-staleness,,2024-08-17T18:35:39.265Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fstreams-intro%23label-streams-staleness,"Highlight:If the data retention period for a table is less than 14 days and a stream hasn’t been consumed, Snowflake temporarily extends this period to prevent the stream from going stale. The retention period is extended to the stream’s offset, up to a maximum of 14 days by default, regardless of your Snowflake edition. The maximum number of days for which Snowflake can extend the data retention period is determined by the MAX_DATA_EXTENSION_TIME_IN_DAYS parameter value. Once the stream is consumed, the extended data retention period reverts to the table’s default.

Highlight:Multiple queries can independently consume the same change data from a stream without changing the offset. A stream advances the offset only when it is used in a DML transaction.

Highlight:Querying a stream alone does not advance its offset, even within an explicit transaction; the stream contents must be consumed in a DML statement.

Highlight:To ensure multiple statements access the same change records in the stream, surround them with an explicit transaction statement (BEGIN .. COMMIT).

Highlight:METADATA$ACTION:

Indicates the DML operation (INSERT, DELETE) recorded.

METADATA$ISUPDATE:

Indicates whether the operation was part of an UPDATE statement. Updates to rows in the source object are represented as a pair of DELETE and INSERT records in the stream with a metadata column METADATA$ISUPDATE values set to TRUE

Highlight:Types of Streams

Highlight:Standard

Highlight:Append-only

Highlight:Insert-only

Highlight:A stream becomes stale when its offset falls outside of the data retention period for its source table

Highlight:In a stale state, historical data and any unconsumed change records for the source table are no longer accessible. To continue tracking new change records, you must recreate the stream using the CREATE STREAM command.

Highlight:To prevent a stream from becoming stale, consume the stream records within a DML statement during the table’s retention period. Additionally, calling SYSTEM$STREAM_HAS_DATA on the stream prevents it from becoming stale, provided the stream is empty and the SYSTEM$STREAM_HAS_DATA function returns FALSE.

Highlight:This restriction doesn’t apply to streams on directory tables or external tables, which have no data retention period.

Highlight:After the STALE_AFTER timestamp has passed, the stream can become stale at any time, even if it has no unconsumed records.

Highlight:Recreating an object (using the CREATE OR REPLACE TABLE syntax) drops its history, which also makes any stream on the table or view stale. In addition, recreating or dropping any of the underlying tables for a view makes any stream on the view stale.

Highlight:Currently, streams cannot track changes in materialized views.

Highlight:Prior to creating a stream on a view, you must enable change tracking on the underlying tables for the view

Highlight:CHANGES Clause: Read-only Alternative to Streams¶

As an alternative to streams, Snowflake supports querying change tracking metadata for tables or views using the CHANGES clause for SELECT statements. The CHANGES clause enables querying change tracking metadata between two points in time without having to create a stream with an explicit transactional offset. Using the CHANGES clause does not advance the offse",false
836236486,Understanding Dynamic Data Masking | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-column-ddm-intro,,2024-08-16T08:57:27.712Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-column-ddm-intro,"Highlight:Snowflake provides two Account Usage views to obtain information about masking policies:

The MASKING POLICIES view provides a list of all masking policies in your Snowflake account.

The POLICY_REFERENCES view provides a list of all objects in which a masking policy is set.",false
836236104,Working with Materialized Views | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/views-materialized#dropping-the-base-table,,2024-08-16T08:55:11.727Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fviews-materialized%23dropping-the-base-table,"Highlight:If a base table is altered so that existing columns are changed or dropped, then all materialized views on that base table are suspended; the materialized views cannot be used or maintained. (This is true even if the modified or dropped column was not part of the materialized view.)",false
836235162,Understanding data transfer cost | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/cost-understanding-data-transfer,,2024-08-16T08:49:35.540Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fcost-understanding-data-transfer,"Highlight:Snowflake charges a per-byte fee for data egress when users transfer data from a Snowflake account into a different region on the same cloud platform or into a completely different cloud platform. Data transfers within the same region are free.

Highlight:Snowflake does not charge data ingress fees. However, a cloud storage provider might charge a data egress fee for transferring data from the provider to your Snowflake account.

Highlight:Snowflake features that incur transfer costs¶

Highlight:Unloading data from Snowflake to Amazon, Google Cloud Storage, or Microsoft Azure.

Typically this involves the use of COPY INTO <location> to unload data to cloud storage in a region or cloud platform different from where your Snowflake account is hosted.

Highlight:Replication of databases, creating a snapshot of the database to a secondary database.

Highlight:Using auto-fulfillment to offer listings to consumers in other cloud regions

Highlight:Snowflake does not apply data egress charges when a Snowflake client or driver retrieves query results across regions within the same cloud platform or across different cloud platforms.",false
836235079,Introduction to Classification | Snowflake Documentation,,,https://docs.snowflake.com/user-guide/classify-intro,,2024-08-16T08:48:41.058Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fuser-guide%2Fclassify-intro,Highlight:Classification is a multi-step process that associates Snowflake-defined system tags to columns by analyzing the fields and metadata for personal data; this data can be tracked by a data engineer using SQL and Snowsight. A data engineer can classify columns in a table to determine whether the column contains certain kinds of data that need to be tracked or protected,false
836234916,Search Optimization Service | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/search-optimization-service#how-does-the-search-optimization-service-work,,2024-08-16T08:47:04.900Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsearch-optimization-service%23how-does-the-search-optimization-service-work,"Highlight:To improve performance of search queries, the search optimization service creates and maintains a persistent data structure called a search access path. The search access path keeps track of which values of the table’s columns might be found in each of its micro-partitions, allowing some micro-partitions to be skipped when scanning the table.",false
836216480,EXPLAIN | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/explain,,2024-08-16T08:42:01.715Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fexplain,"Highlight:EXPLAIN¶

Returns the logical execution plan for the specified SQL statement.

An explain plan shows the operations (for example, table scans and joins) that Snowflake would perform to execute the query.",false
836210727,Using the Query Acceleration Service | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/query-acceleration-service,,2024-08-16T08:41:18.405Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquery-acceleration-service,"Highlight:Examples of the types of workloads that might benefit from the query acceleration service include:

Ad hoc analytics.

Workloads with unpredictable data volume per query.

Queries with large scans and selective filters.",false
836208193,Manage reader accounts | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-sharing-reader-create#what-is-restricted-allowed-in-a-reader-account,,2024-08-16T08:38:29.722Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-sharing-reader-create%23what-is-restricted-allowed-in-a-reader-account,"Highlight:What is restricted/allowed in a reader account?¶

A reader account is intended primarily for querying data shared by the provider of the account. You can work with data, for example, by creating materialized views.

You cannot perform the following tasks in a reader account:

Set a data metric function on objects in the reader account.

Upload new data.

Modify existing data.

Unload data using a storage integration. However, you can use the COPY INTO <location> command with your connection credentials to unload data into a cloud storage location.

Additionally, you cannot execute the following commands in a reader account:

INSERT

UPDATE

DELETE

MERGE

COPY INTO <table>

CREATE MASKING POLICY

CREATE PIPE

CREATE ROW ACCESS POLICY

CREATE SHARE

CREATE STAGE

SHOW PROCEDURES

All other operations are allowed.",false
836208063,CREATE FILE FORMAT | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/create-file-format,,2024-08-16T08:37:22.777Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcreate-file-format,"Highlight:FIELD_OPTIONALLY_ENCLOSED_BY = 'character' | NONE

Highlight:Definition:

Character used to enclose strings. Value can be NONE, single quote character ('), or double quote character (""). To use the single quote character, use the octal or hex representation (0x27) or the double single-quoted escape ('').

Highlight:{ TEMP | TEMPORARY | VOLATILE }

Specifies that the file format persists only for the duration of the session that you created it in. A temporary file format is dropped at the end of the session.

Highlight:When you load data from files into tables, Snowflake supports either NDJSON (newline delimited JSON) standard format or comma-separated JSON format.

When you unload table data to files, Snowflake outputs only to NDJSON format.

Highlight:If a value is not specified or is AUTO, the value for the TIME_INPUT_FORMAT (data loading) or TIME_OUTPUT_FORMAT (data unloading) parameter is used.

Highlight:ERROR_ON_COLUMN_COUNT_MISMATCH = TRUE | FALSE
Use:

Data loading only

Definition:

Boolean that specifies whether to generate a parsing error if the number of delimited columns (i.e. fields) in an input file does not match the number of columns in the corresponding table.

Highlight:STRIP_OUTER_ARRAY = TRUE | FALSE
Use:

Data loading and external tables

Definition:

Boolean that instructs the JSON parser to remove outer brackets (i.e. [ ]).

Default:

FALSE",false
836207611,Manage reader accounts | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-sharing-reader-create#overview,,2024-08-16T08:36:04.856Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-sharing-reader-create%23overview,"Highlight:Warehouses in a reader account can consume an unlimited number of credits each month, which will be charged to your provider account. To limit usage, set up a resource monitor for the warehouse.",false
836207242,Working with stored procedures | Snowflake Documentation,,,https://docs.snowflake.com/en/developer-guide/stored-procedure/stored-procedures-usage,,2024-08-16T08:32:45.293Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fdeveloper-guide%2Fstored-procedure%2Fstored-procedures-usage,"Highlight:Currently, the following privileges apply to stored procedures:

USAGE

OWNERSHIP

For a role to use a stored procedure, the role must either be the owner or have been granted USAGE privilege on the stored procedure.",false
836207171,Snowflake Community,,"Join our community of data professionals to learn, connect, share and innovate together",https://community.snowflake.com/s/article/Performance-impact-from-local-and-remote-disk-spilling,,2024-08-16T08:31:57.968Z,https://www.snowflake.com/wp-content/themes/snowflake/assets/img/community/community-social-share.jpg,"Highlight:The spilling can't always be avoided, especially for large batches of data, but it can be decreased by:

Reviewing the query for query optimization especially if it is a new query.
Reducing the amount of data processed. For example, by trying to improve partition pruning, or projecting only the columns that are needed in the output.
Decreasing the number of parallel queries running in the warehouse.
Trying to split the processing into several steps (for example by replacing the CTEs with temporary tables).
Using a larger warehouse. This effectively means more memory and more local disk space.",false
836206884,GET_DDL | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/get_ddl#examples,,2024-08-16T08:30:42.270Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fget_ddl%23examples,"Highlight:GET_DDL¶

Returns a DDL statement that can be used to recreate the specified object.",false
836206513,ALTER FILE FORMAT | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/alter-file-format,,2024-08-16T08:29:06.228Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Falter-file-format,"Highlight:ALTER FILE FORMAT does not support the following actions:

Changing the type (CSV, JSON, etc.) for the file format.

Unsetting any format options (i.e. resetting the options to the defaults for the type).

Unsetting (i.e. removing) a comment.

To make any of these changes, you must recreate the file format.",false
836205533,Introduction to Streams | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/streams-intro#data-retention-period-and-staleness,,2024-08-16T08:19:44.582Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fstreams-intro%23data-retention-period-and-staleness,"Highlight:If the data retention period for a table is less than 14 days and a stream hasn’t been consumed, Snowflake temporarily extends this period to prevent the stream from going stale. The retention period is extended to the stream’s offset, up to a maximum of 14 days by default, regardless of your Snowflake edition. The maximum number of days for which Snowflake can extend the data retention period is determined by the MAX_DATA_EXTENSION_TIME_IN_DAYS parameter value. Once the stream is consumed, the extended data retention period reverts to the table’s default.",false
836205196,SAMPLE / TABLESAMPLE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/constructs/sample,,2024-08-16T08:17:38.349Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fconstructs%2Fsample,"Highlight:SAMPLE and TABLESAMPLE are synonymous and can be used interchangeably.

Highlight:Parameters¶
BERNOULLI | ROW or . SYSTEM | BLOCK

Highlight:Return a sample of a table in which each row has a 10% probability of being included in the sample:

SELECT * FROM testtable SAMPLE (10);",false
836205118,Overview of data loading | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-overview#external-stages,,2024-08-16T08:16:45.023Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-overview%23external-stages,"Highlight:External stages¶

Loading data from any of the following cloud storage services is supported regardless of the cloud platform that hosts your Snowflake account:

Amazon S3

Google Cloud Storage

Microsoft Azure

Highlight:The Snowpipe Streaming API writes rows of data directly to Snowflake tables without the requirement of staging files.

Highlight:INFER_SCHEMA

Detects the column definitions in a set of staged data files and retrieves the metadata in a format suitable for creating Snowflake objects.

GENERATE_COLUMN_DESCRIPTION

Generates a list of columns from a set of staged files using the INFER_SCHEMA function output.

These SQL functions support both internal and external stages.

Create tables or external tables with the column definitions derived from a set of staged files using the CREATE TABLE … USING TEMPLATE or CREATE EXTERNAL TABLE … USING TEMPLATE syntax. The USING TEMPLATE clause accepts an expression that calls the INFER_SCHEMA SQL function to detect the column definitions in the files. After the table is created, you can then use a COPY statement with the MATCH_BY_COLUMN_NAME option to load files directly into the structured table.",false
836204694,Naming and overloading procedures and UDFs | Snowflake Documentation,,,https://docs.snowflake.com/developer-guide/udf-stored-procedure-naming-conventions#overloading-procedures-and-functions,,2024-08-16T08:13:17.118Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fdeveloper-guide%2Fudf-stored-procedure-naming-conventions%23overloading-procedures-and-functions,"Highlight:Overloading procedures and functions¶

Snowflake supports overloading procedures and functions. In a given schema, you can define multiple procedures or functions that have the same name but different signatures. The signatures must differ by the number of arguments, the types of the arguments, or both.",false
836204577,Data Sharing Usage | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/data-sharing-usage,,2024-08-16T08:12:14.257Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fdata-sharing-usage,"Highlight:DATA_SHARING_USAGE views¶

The DATA_SHARING_USAGE schema contains the following views:

View

	

Type

	

Latency [1]

	

Retention duration




APPLICATION_STATE

	

Current state

	

up to 10 minutes

	

Not applicable.




LISTING_ACCESS_HISTORY

	

Historical

	

up to 2 days

	

Data retained for 1 year.




LISTING_AUTO_FULFILLMENT_DATABASE_STORAGE_DAILY

	

Historical

	

up to 2 days

	

Data retained for 1 year.




LISTING_AUTO_FULFILLMENT_REFRESH_DAILY

	

Historical

	

up to 2 days

	

Data retained for 1 year.




LISTING_CONSUMPTION_DAILY

	

Historical

	

up to 2 days

	

Data retained for 1 year.




LISTING_EVENTS_DAILY

	

Historical

	

up to 2 days

	

Data retained for 1 year.




LISTING_TELEMETRY_DAILY

	

Historical

	

up to 2 days

	

Data retained for 1 year.




MARKETPLACE_DISBURSEMENT_REPORT

	

Historical

	

up to 2 days

	

Data retained for 1 year.




MARKETPLACE_PAID_USAGE_DAILY

	

Historical

	

up to 2 days

	

Data retained for 1 year.




MONETIZED_USAGE_DAILY

	

Historical

	

up to 2 days

	

Data retained for 1",false
836204374,BUILD_STAGE_FILE_URL | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/build_stage_file_url,,2024-08-16T08:10:19.981Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fbuild_stage_file_url,"Highlight:BUILD_STAGE_FILE_URL¶

Generates a Snowflake file URL to a staged file using the stage name and relative file path as inputs. A file URL permits prolonged access to a specified file. That is, the file URL does not expire.",false
836204177,METERING_HISTORY view | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/account-usage/metering_history,,2024-08-16T08:07:57.636Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Faccount-usage%2Fmetering_history,"Highlight:METERING_HISTORY view¶

The METERING_HISTORY view in the ACCOUNT_USAGE schema can be used to return the hourly credit usage for an account within the last 365 days (1 year).",false
836204122,Clustering Keys & Clustered Tables | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/tables-clustering-keys#defining-a-clustering-key-for-a-table,,2024-08-16T08:07:23.145Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Ftables-clustering-keys%23defining-a-clustering-key-for-a-table,"Highlight:Where each clustering key consists of one or more table columns/expressions, which can be of any data type, except GEOGRAPHY, VARIANT, OBJECT, or ARRAY. A clustering key can contain any of the following:

Base columns.

Expressions on base columns.

Expressions on paths in VARIANT columns.",false
836203401,Account Usage | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/account-usage,,2024-08-16T08:00:05.715Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Faccount-usage,"Highlight:Historical data retention¶

Certain account usage views provide historical usage metrics. The retention period for these views is 1 year (365 days).",false
836203345,TYPEOF | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/typeof,,2024-08-16T07:59:22.976Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Ftypeof,"Highlight:TYPEOF¶

Returns the type of a value stored in a VARIANT column. The type is returned as a string.",false
836203212,Identifying queries that can benefit from search optimization | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/search-optimization/queries-that-benefit,,2024-08-16T07:58:05.082Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsearch-optimization%2Fqueries-that-benefit,"Highlight:Supported predicate types¶

Search optimization can improve the performance of queries using these kinds of predicates:

Point lookup queries using equality and IN.

Character data (text) queries using the SEARCH function.

Substring queries using wildcards and regular expressions.

Searches in semi-structured data.

Geospatial queries.

Queries using conjunctions (AND) and disjunctions (OR).

Other potential improvements¶

Search optimization can also improve the performance of views and of queries that use JOIN.

Highlight:the search optimization service does not support the following:

External tables.

Materialized views.

Columns defined with a COLLATE clause.

Column concatenation.

Analytical expressions.

Casts on table columns (except for fixed-point numbers cast to strings).",false
836199287,Supported Cloud Platforms | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/intro-cloud-platforms,,2024-08-16T07:32:37.013Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fintro-cloud-platforms,"Highlight:The cloud platform you choose for each Snowflake account is completely independent from your other Snowflake accounts. In fact, you can choose to host each Snowflake account on a different platform, although this may have some impact on data transfer billing when loading data",false
836199134,Preparing your data files | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare#preparing-delimited-text-files,,2024-08-16T07:30:43.515Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-considerations-prepare%23preparing-delimited-text-files,"Highlight:Consider the following guidelines when preparing your delimited text (CSV) files for loading:

UTF-8 is the default character set, however, additional encodings are supported. Use the ENCODING file format option to specify the character set for the data files. For more information, see CREATE FILE FORMAT.

Fields that contain delimiter characters should be enclosed in quotes (single or double). If the data contains single or double quotes, then those quotes must be escaped.

Carriage returns are commonly introduced on Windows systems in conjunction with a line feed character to mark the end of a line (\r \n). Fields that contain carriage returns should also be enclosed in quotes (single or double).

The number of columns in each row should be consistent.

Highlight:we recommend aiming to produce data files roughly 100-250 MB (or larger) in size compressed.

Highlight:Loading very large files (e.g. 100 GB or larger) is not recommended.

If you must load a large file, carefully consider the ON_ERROR copy option value. Aborting or skipping a file due to a small number of errors could result in delays and wasted credits. In addition, if a data loading operation continues beyond the maximum allowed duration of 24 hours, it could be aborted without any portion of the file being committed.",false
836198907,Understanding compute cost | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/cost-understanding-compute#serverless-credit-usage,,2024-08-16T07:28:34.342Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fcost-understanding-compute%23serverless-credit-usage,"Highlight:Usage for cloud services is charged only if the daily consumption of cloud services exceeds 10% of the daily usage of virtual warehouses.

Highlight:The 10% adjustment for cloud services is calculated daily (in the UTC time zone) by multiplying daily warehouse usage by 10%.

Highlight:Charges for serverless features are calculated based on total usage of snowflake-managed compute resources measured in compute-hours. Compute-Hours are calculated on a per second basis, rounded up to the nearest whole second.",false
836198736,Supported Formats for Semi-structured Data | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/semistructured-data-formats,,2024-08-16T07:26:57.902Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsemistructured-data-formats,"Highlight:Avro is an open-source data serialization and RPC framework originally developed for use with Apache Hadoop

Highlight:ORC (Optimized Row Columnar) is a binary format used to store Hive data.

Highlight:Parquet is a compressed, efficient columnar data representation designed for projects in the Hadoop ecosystem.

Highlight:JSON (JavaScript Object Notation) is a lightweight, plain-text, data-interchange format based on a subset of the JavaScript Programming Language.",false
836196631,Controlling network traffic with network policies | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/network-policies#creating-network-policies,,2024-08-16T07:23:49.284Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fnetwork-policies%23creating-network-policies,"Highlight:You can apply a network policy to an account, a security integration, or a user.

Highlight:Account:

Network policies applied to an account are the most general network policies. They are overridden by network policies applied to a security integration or user.

Security Integration:

Network policies applied to a security integration override network policies applied to the account, but are overridden by a network policy applied to a user.

User:

Network policies applied to a user are the most specific network policies. They override both accounts and security integrations.",false
835174016,Share data from multiple databases | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-sharing-multiple-db,,2024-08-15T07:29:17.459Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-sharing-multiple-db,"Highlight:you must also grant the REFERENCE_USAGE privilege on each database referenced by a secure view that you wish to share. However, you do not need to grant REFERENCE_USAGE on the database that contains the secure view.",false
835173104,About listings | Snowflake Documentation,,,https://other-docs.snowflake.com/en/collaboration/collaboration-listings-about,,2024-08-15T07:25:46.614Z,https://rdl.ink/render/https%3A%2F%2Fother-docs.snowflake.com%2Fen%2Fcollaboration%2Fcollaboration-listings-about,"Highlight:When you offer a listing, you choose how to make your data product available to consumers:

Highlight:Privately, available only to specific consumers.

Highlight:Publicly, visible on the Snowflake Marketplace.

Highlight:A free listing is available privately to specific consumers, or publicly on the Snowflake Marketplace, and provides instant access to a full published dataset.

Highlight:A limited trial listing is available on the Snowflake Marketplace and provides instant limited access to a data product.

Highlight:A paid listing is available privately or on the Snowflake Marketplace. As a provider, you can create paid listings to charge consumers to access or use your listing.",false
835172610,Set operators | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/operators-query#intersect,,2024-08-15T07:24:00.580Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Foperators-query%23intersect,"Highlight:INTERSECT¶

Returns rows from one query’s result set which also appear in another query’s result set, with duplicate elimination.",false
835171768,Grant privileges to other roles | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-exchange-marketplace-privileges,,2024-08-15T07:22:47.591Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-exchange-marketplace-privileges,"Highlight:Granting administrator privileges in a Data Exchange¶

By default, only an account administrator (a user with the ACCOUNTADMIN role) in the Data Exchange administrator account can manage a Data Exchange, which includes the following tasks:

Add or remove members.

Approve or deny listing approval requests.

Approve or deny provider profile approval requests.

Show categories.",false
835170940,ALTER TASK | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/alter-task,,2024-08-15T07:19:28.377Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Falter-task,"Highlight:ALTER TASK mytask SET SCHEDULE = 'USING CRON */3 * * * * UTC';

Highlight:Before resuming the root task of your Task Graph, resume all child tasks. To recursively resume the root task’s child tasks, use SYSTEM$TASK_DEPENDENTS_ENABLE.

Highlight:Currently, the following functions are supported for evaluation in the SQL expression:

SYSTEM$STREAM_HAS_DATA

Indicates whether a specified stream contains change tracking data. Used to run a triggered task if no schedule is defined for the task. You can also use this to skip the current task run if the stream contains no change data.

If the result is FALSE, then the task does not run.

SYSTEM$GET_PREDECESSOR_RETURN_VALUE

Retrieves the return value for the predecessor task in a task graph. Used to decide whether the task should run based on the returned result.",false
835170811,RESOURCE_MONITORS view | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/account-usage/resource_monitors,,2024-08-15T07:18:04.585Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Faccount-usage%2Fresource_monitors,"Highlight:RESOURCE_MONITORS view¶

This Account Usage view displays the resource monitors that have been created in the reader accounts managed by the account.

Highlight:This view is only available in the READER_ACCOUNT_USAGE schema.",false
835168865,COPY INTO location | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/copy-into-location#type-parquet,,2024-08-15T07:08:23.209Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcopy-into-location%23type-parquet,"Highlight:TYPE = PARQUET¶
COMPRESSION = AUTO | LZO | SNAPPY | NONE

Highlight:TYPE = JSON¶
COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE

Highlight:TYPE = CSV¶
COMPRESSION = AUTO | GZIP | BZ2 | BROTLI | ZSTD | DEFLATE | RAW_DEFLATE | NONE",false
835166910,CREATE TABLE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/create-table,,2024-08-15T06:57:53.422Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcreate-table,Highlight:CREATE TABLE … LIKE (creates an empty copy of an existing table),false
835165765,CURRENT_CLIENT | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/current_client,,2024-08-15T06:49:37.188Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fcurrent_client,"Highlight:CURRENT_CLIENT¶

Returns the version of the client from which the function was called. If called from an application using the JDBC or ODBC driver to connect to Snowflake, returns the version of the driver.",false
835165361,PARSE_JSON | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/parse_json,,2024-08-15T06:46:29.618Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fparse_json,"Highlight:PARSE_JSON¶

Interprets an input string as a JSON document, producing a VARIANT value.

Highlight:This function supports an input expression with a maximum size of 8 MB compressed.",false
835164834,ALTER TABLE … ALTER COLUMN | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/alter-table-column,,2024-08-15T06:45:07.254Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Falter-table-column,"Highlight:When setting a column to NOT NULL, if the column contains NULL values, an error is returned and no changes are applied to the column.",false
835164621,AT | BEFORE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/constructs/at-before,,2024-08-15T06:43:40.671Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fconstructs%2Fat-before,"Highlight:OFFSET => time_difference

Specifies the difference in seconds from the current time to use for Time Travel, in the form -N where N can be an integer or arithmetic expression (e.g. -120 is 120 seconds, -30*60 is 1800 seconds or 30 minutes).",false
835164204,GRANT privileges | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/grant-privilege,,2024-08-15T06:39:13.299Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fgrant-privilege,"Highlight:FUTURE

Specifies that privileges are granted on new (i.e. future) database or schema objects of a specified type (e.g. tables or views) rather than existing objects.",false
835163274,Understanding end-to-end encryption in Snowflake | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-encryption-end-to-end,,2024-08-15T06:37:08.645Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-encryption-end-to-end,"Highlight:Client-side encryption means that a client encrypts data before copying it into a cloud storage staging area. Client-side encryption provides a secure system for managing data in cloud storage.

Highlight:We recommend client-side encryption for data files in external stages; but if the data is not encrypted, Snowflake immediately encrypts the data when it is loaded into a table.",false
835162926,Geospatial data types | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/data-types-geospatial#geometry-data-type,,2024-08-15T06:33:29.509Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fdata-types-geospatial%23geometry-data-type,"Highlight:Snowflake provides the following data types for geospatial data:

The GEOGRAPHY data type, which models Earth as though it were a perfect sphere.

The GEOMETRY data type, which represents features in a planar (Euclidean, Cartesian) coordinate system.",false
835161956,Overview of federated authentication and SSO | Snowflake Documentation,,,https://docs.snowflake.com/user-guide/admin-security-fed-auth-overview,,2024-08-15T06:26:53.024Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fuser-guide%2Fadmin-security-fed-auth-overview,"Highlight:In a federated environment, user authentication is separated from user access through the use of one or more external entities that provide independent authentication of user credentials. The authentication is then passed to one or more services, enabling users to access the services through SSO. A federated environment consists of the following components:

Service provider (SP):

In a Snowflake federated environment, Snowflake serves as the SP.

Identity provider (IdP):

The external, independent entity responsible for providing the following services to the SP:

Creating and maintaining user credentials and other profile information.

Authenticating users for SSO access to the SP.",false
835161778,Monitor data loading activity by using Copy History | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-monitor,,2024-08-15T06:25:21.625Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-monitor,"Highlight:You can monitor data loading activity for all tables in your account, or for a specific table, by using Snowsight or SQL.

Monitor data loading for your account by using Copy History.

Monitor data loading for a table by using Copy History.",false
835161535,Configuring SnowSQL | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/snowsql-config,,2024-08-15T06:23:08.401Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsnowsql-config,"Highlight:SnowSQL supports multiple configuration files that allow organizations to define base values for connection parameters, default settings, and variables while allowing individual users to customize their personal settings in their own <HOME_DIR>/.snowsql/config files. When SnowSQL starts, it loads configuration parameter values from the following configuration file locations in order, overriding values from files loaded previously:

/etc/snowsql.cnf

/etc/snowflake/snowsql.cnf

/usr/local/etc/snowsql.cnf

<HOME_DIR>/.snowsql.cnf (supported only for backward compatibility)

<HOME_DIR>/.snowsql/config

For example, if the /etc/snowsql.cnf configuration file sets the log_level parameter to info, you can override this by setting the parameter to debug in your file <HOME_DIR>/.snowsql/config file.",false
834789500,About Secure Data Sharing | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-sharing-intro,,2024-08-14T12:58:20.748Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-sharing-intro,"Highlight:The only charges to consumers are for the compute resources (i.e. virtual warehouses) used to query the shared data.

Highlight:With Secure Data Sharing, no actual data is copied or transferred between accounts. All sharing uses Snowflake’s services layer and metadata store.

Highlight:What is a share?¶

Shares are named Snowflake objects that encapsulate all of the information required to share a database.",false
834788099,Parameters | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/parameters#min-data-retention-time-in-days,,2024-08-14T12:48:27.010Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fparameters%23min-data-retention-time-in-days,"Highlight:MIN_DATA_RETENTION_TIME_IN_DAYS¶
Type:

Account — Can be set only for Account

Highlight:DATA_RETENTION_TIME_IN_DAYS¶
Type:

Object (for databases, schemas, and tables) — Can be set for Account » Database » Schema » Table",false
834787780,Unloading into Amazon S3 | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-unload-s3#configuring-an-s3-bucket-for-unloading-data,,2024-08-14T12:45:32.377Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-unload-s3%23configuring-an-s3-bucket-for-unloading-data,"Highlight:Configuring an S3 bucket for unloading data¶

Snowflake requires the following permissions on an S3 bucket and folder to create new files in the folder (and any sub-folders):

s3:DeleteObject

s3:PutObject",false
834787585,Snowpark API | Snowflake Documentation,,,https://docs.snowflake.com/en/developer-guide/snowpark/index#key-features,,2024-08-14T12:43:54.611Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fdeveloper-guide%2Fsnowpark%2Findex%23key-features,"Highlight:In comparison to using the Snowflake Connector for Spark, developing with Snowpark includes the following benefits:

Support for interacting with data within Snowflake using libraries and patterns purpose built for different languages without compromising on performance or functionality.

Support for authoring Snowpark code using local tools such as Jupyter, VS Code, or IntelliJ.

Support for pushdown for all operations, including Snowflake UDFs. This means Snowpark pushes down all data transformation and heavy lifting to the Snowflake data cloud, enabling you to efficiently work with data of any size.

No requirement for a separate cluster outside of Snowflake for computations. All of the computations are done within Snowflake. Scale and compute management are handled by Snowflake.",false
834787402,Understanding Snowflake Table Structures | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/tables-micro-partitions,,2024-08-14T12:42:29.544Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Ftables-micro-partitions,"Highlight:All data in Snowflake is stored in database tables, logically structured as collections of columns and rows. To best utilize Snowflake tables, particularly large tables, it is helpful to have an understanding of the physical structure behind the logical structure.",false
834786730,Working with Materialized Views | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/views-materialized#effects-of-changes-to-base-tables-on-materialized-views,,2024-08-14T12:39:44.315Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fviews-materialized%23effects-of-changes-to-base-tables-on-materialized-views,"Highlight:When a base table changes, all materialized views defined on the table are updated by a background service that uses compute resources provided by Snowflake.

Highlight:Defining a clustering key on a materialized view is supported and can increase performance in many situations. However, it also adds costs.",false
834786422,Using the History Page to Monitor Queries | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/ui-history,,2024-08-14T12:37:36.574Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fui-history,"Highlight:You can view results only for queries you have executed. If you have privileges to view queries executed by another user, the Query Detail page displays the details for the query, but, for data privacy reasons, the page does not display the actual query result.",false
834782917,About listing consumers | Snowflake Documentation,,,https://other-docs.snowflake.com/en/collaboration/consumer-becoming,,2024-08-14T12:26:54.512Z,https://rdl.ink/render/https%3A%2F%2Fother-docs.snowflake.com%2Fen%2Fcollaboration%2Fconsumer-becoming,"Highlight:Accept the Snowflake Provider and Consumer Terms of Service¶

The organization administrator only needs to accept the Snowflake Provider and Consumer Terms once for your organization. After the terms have been accepted, anyone in your organization that has a role with the necessary privileges can become a consumer of listings.

Highlight:You must be an organization administrator (i.e. a user granted the ORGADMIN role) to accept the terms.

Highlight:To access a listing, you must use the ACCOUNTADMIN role or another role with the CREATE DATABASE and IMPORT SHARE privileges. To pay for a paid listing, your role must also have the PURCHASE DATA EXCHANGE LISTING privilege.",false
834782808,Considerations for Semi-structured Data Stored in VARIANT | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/semistructured-considerations,,2024-08-14T12:26:12.986Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsemistructured-considerations,"Highlight:If the data exceeds 16 MB, enable the STRIP_OUTER_ARRAY file format option for the COPY INTO <table> command to remove the outer array structure and load the records into separate table rows",false
834782752,SHOW NETWORK POLICIES | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/show-network-policies,,2024-08-14T12:25:43.805Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fshow-network-policies,"Highlight:SHOW NETWORK POLICIES¶

Lists all network policies defined in the system.",false
834782706,Consume shared data | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-share-consumers,,2024-08-14T12:25:15.761Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-share-consumers,"Highlight:Shared databases have the following limitations for consumers:

Shared databases are read-only. Users in a consumer account can view/query data, but cannot insert or update data, or create any objects in the database.

The following actions are not supported:

Creating a clone of a shared database or any schemas/tables in the database.

Time Travel for a shared database or any schemas/tables in the database.

Editing the comments for a shared database.

Shared databases and all the objects in the database cannot be re-shared with other accounts.

Shared databases cannot be replicated.",false
834782144,Working with Materialized Views | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/views-materialized#limitations-on-creating-materialized-views,,2024-08-14T12:21:08.779Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fviews-materialized%23limitations-on-creating-materialized-views,"Highlight:The following limitations apply to creating materialized views:

A materialized view can query only a single table.

Joins, including self-joins, are not supported.",false
834781108,Manage your user profile by using Snowsight | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/ui-snowsight-profile,,2024-08-14T12:13:33.219Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fui-snowsight-profile,"Highlight:On your profile, you can review and set the following user details:

Highlight:Default role & warehouse:

Highlight:Default experience: Select the default user interface you see when you sign in to Snowflake, Snowsight or the Classic Console.

Highlight:Language

Highlight:Notifications

Highlight:Multi-factor authentication",false
834780934,TASK_HISTORY | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/task_history,,2024-08-14T12:12:30.149Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Ftask_history,"Highlight:This function returns results only for the ACCOUNTADMIN role, the task owner, or a role with the global MONITOR EXECUTION privilege.",false
834780196,Access control considerations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-access-control-considerations,,2024-08-14T12:08:20.803Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-access-control-considerations,"Highlight:Assign this role to at least two users. We follow strict security procedures for resetting a forgotten or lost password for users with the ACCOUNTADMIN role. These procedures can take up to two business days. Assigning the ACCOUNTADMIN role to more than one user avoids having to go through these procedures because the users can reset each other’s passwords.

Highlight:A user cannot view the result set from a query that another user executed. This behavior is intentional. For security reasons, only the user who executed a query can access the query results.

Note

This behavior is not connected to the Snowflake access control model for objects. Even a user with the ACCOUNTADMIN role cannot view the results for a query run by another user.",false
834779977,Writing external functions | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/external-functions,,2024-08-14T12:06:50.841Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fexternal-functions,Highlight:External functions are user-defined functions that are stored and executed outside of Snowflake.,false
834779885,ALTER TABLE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/alter-table,,2024-08-14T12:06:08.219Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Falter-table,"Highlight:DROP CLUSTERING KEY

Drops the clustering key for the table.

Highlight:ALTER TABLE",false
834779775,Choosing whether to write a stored procedure or a user-defined function | Snowflake Documentation,,,https://docs.snowflake.com/en/developer-guide/stored-procedures-vs-udfs,,2024-08-14T12:05:03.445Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fdeveloper-guide%2Fstored-procedures-vs-udfs,"Highlight:A single SQL statement can call multiple UDFs.

A single SQL statement can call only one stored procedure.",false
834778758,JAROWINKLER_SIMILARITY | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/jarowinkler_similarity,,2024-08-14T11:59:53.408Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fjarowinkler_similarity,"Highlight:JAROWINKLER_SIMILARITY¶

Computes the Jaro-Winkler similarity between two input strings. The function returns an integer between 0 and 100, where 0 indicates no similarity and 100 indicates an exact match.

Note

The similarity computation is case-insensitive.

The computation is sensitive to all formatting characters, including white space characters.

The default scaling factor of 0.1 is used for the computation.",false
834778410,Working with Temporary and Transient Tables | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/tables-temp-transient#potential-naming-conflicts-with-other-table-types,,2024-08-14T11:56:39.090Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Ftables-temp-transient%23potential-naming-conflicts-with-other-table-types,Highlight:the temporary table takes precedence in the session over any other table with the same name in the same schema.,false
834778331,Visualizing worksheet data | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/ui-snowsight-visualizations,,2024-08-14T11:55:55.175Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fui-snowsight-visualizations,"Highlight:Snowsight supports the following types of charts:

Bar charts

Line charts

Scatterplots

Heat grids

Scorecards",false
834778283,Enable non-ACCOUNTADMIN roles to perform data sharing tasks | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-access-privileges-shares,,2024-08-14T11:55:21.894Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-access-privileges-shares,"Highlight:By default, the privileges required to create and manage shares are granted only to the ACCOUNTADMIN role, ensuring that only account administrators can perform these tasks. However, the privileges can also be granted to other roles, enabling the tasks to be delegated to other users in the account.",false
834778204,Tag-based masking policies | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/tag-based-masking-policies#overview,,2024-08-14T11:54:29.558Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Ftag-based-masking-policies%23overview,"Highlight:A tag can support one masking policy for each data type. For example, if a tag already has a masking policy for the NUMBER data type, you cannot assign another masking policy with the NUMBER data type to the same tag.",false
834777946,Overview of Access Control | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-access-control-overview#label-access-control-role-enforcement,,2024-08-14T11:52:28.736Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-access-control-overview%23label-access-control-role-enforcement,"Highlight:Enforcement model with primary role and secondary roles¶

Every active user session has a “current role,” also referred to as a primary role. When a session is initiated (e.g. a user connects via JDBC/ODBC or logs in to the Snowflake web interface), the current role is determined based on the following criteria:

If a role was specified as part of the connection and that role is a role that has already been granted to the connecting user, the specified role becomes the current role.

If no role was specified and a default role has been set for the connecting user, that role becomes the current role.

If no role was specified and a default role has not been set for the connecting user, the system role PUBLIC is used",false
834777569,Overview of data unloading | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-unload-overview#bulk-unloading-into-single-or-multiple-files,,2024-08-14T11:49:08.221Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-unload-overview%23bulk-unloading-into-single-or-multiple-files,Highlight:The COPY INTO <location> command provides a copy option (SINGLE) for unloading data into a single file or multiple files. The default is SINGLE = FALSE (i.e. unload into multiple files).,false
834777473,Key Concepts & Architecture | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/intro-key-concepts#database-storage,,2024-08-14T11:48:25.646Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fintro-key-concepts%23database-storage,"Highlight:When data is loaded into Snowflake, Snowflake reorganizes that data into its internal optimized, compressed, columnar format. Snowflake stores this optimized data in cloud storage.",false
834777355,Key-pair authentication and key-pair rotation | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/key-pair-auth,,2024-08-14T11:47:24.556Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fkey-pair-auth,"Highlight:Client

	

Key Pair Authentication

	

Key Pair Rotation

	

Unencrypted Private Keys

	


SnowSQL (CLI client)

	

✔

	

✔

	

✔

	


Snowflake Connector for Python

	

✔

	

✔

	

✔

	


Snowflake Connector for Spark

	

✔

	

✔

	

✔

	


Snowflake Connector for Kafka

	

✔

	

✔

	

✔

	


Go driver

	

✔

	

✔

	

✔

	


JDBC Driver

	

✔

	

✔

	

✔

	


ODBC Driver

	

✔

	

✔

	

✔

	


Node.js Driver

	

✔

	

✔

	

✔

	


.NET Driver

	

✔

	

✔

	

✔

	


PHP PDO Driver for Snowflake

	

✔

	

✔

	

✔

Highlight:The following table summarizes support for key pair authentication among Snowflake Clients.",false
834775631,DROP STAGE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/drop-stage,,2024-08-14T11:44:50.015Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fdrop-stage,Highlight:Dropped stages cannot be recovered; they must be recreated.,false
834775507,Understanding Column-level Security | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-column-intro,,2024-08-14T11:43:41.816Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-column-intro,"Highlight:Column-level Security includes two features:

Dynamic Data Masking

External Tokenization",false
834775226,Troubleshooting tasks | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/tasks-ts,,2024-08-14T11:40:59.351Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Ftasks-ts,"Highlight:Task timed out or exceeded the schedule window¶

There is a 60 minute default limit on a single run of a task.",false
834774644,Understanding caller’s rights and owner’s rights stored procedures | Snowflake Documentation,,,https://docs.snowflake.com/en/developer-guide/stored-procedure/stored-procedures-rights,,2024-08-14T11:37:43.503Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fdeveloper-guide%2Fstored-procedure%2Fstored-procedures-rights,Highlight:A stored procedure runs with either the caller’s rights or the owner’s rights.,false
834774517,Snowflake Community,,"Join our community of data professionals to learn, connect, share and innovate together",https://community.snowflake.com/s/article/Using-session-variables-in-a-stored-procedure,,2024-08-14T11:36:53.992Z,https://www.snowflake.com/wp-content/themes/snowflake/assets/img/community/community-social-share.jpg,"Highlight:By default, when a stored procedure is created in snowflake it consumes the Creator's rights(execute as owner) which do not allow the use of session variables.

Highlight:This problem can be resolved if we create the stored procedure with the option ""EXECUTE AS CALLER"", it would use caller rights at the time of execution and would not cause any issues.",false
834774187,Analyzing Queries Using Query Profile | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/ui-query-profile#query-operator-details,,2024-08-14T11:34:43.268Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fui-query-profile%23query-operator-details,"Highlight:Execution Time¶

Execution time provides information about “where the time was spent” during the processing of a query. Time spent can be broken down into the following categories, displayed in the following order:

Processing — time spent on data processing by the CPU.

Local Disk IO — time when the processing was blocked by local disk access.

Remote Disk IO — time when the processing was blocked by remote disk access.

Network Communication — time when the processing was waiting for the network data transfer.

Synchronization — various synchronization activities between participating processes.

Initialization — time spent setting up the query processing.",false
834774059,Querying Semi-structured Data | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/querying-semistructured#dot-notation,,2024-08-14T11:33:37.670Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquerying-semistructured%23dot-notation,Highlight:Use dot notation to traverse a path in a JSON object,false
834720415,Configuring access control | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-access-control-configure#creating-custom-read-only-roles,,2024-08-14T08:55:55.023Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-access-control-configure%23creating-custom-read-only-roles,"Highlight:SELECT

	

Table

	

To operate on any object in a schema, a role must have the USAGE privilege on the container database and schema.",false
834716422,Understanding Encryption Key Management in Snowflake | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-encryption-manage,,2024-08-14T08:49:45.764Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-encryption-manage,"Highlight:All Snowflake customer data is encrypted by default using the latest security standards and best practices. Snowflake uses strong AES 256-bit encryption with a hierarchical key model rooted in a hardware security module.

Highlight:When enabled, the combination of a Snowflake-maintained key and a customer-managed key creates a composite master key to protect the Snowflake data. This is called Tri-Secret Secure.

Highlight:Snowflake’s hierarchical key model consists of four levels of keys:

The root key

Account master keys

Table master keys

File keys

Highlight:Encryption key rotation¶

All Snowflake-managed keys are automatically rotated by Snowflake when they are more than 30 days old.",false
834713486,Managing and using worksheets in Snowsight | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/ui-snowsight-worksheets#label-worksheets-manage,,2024-08-14T08:43:19.782Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fui-snowsight-worksheets%23label-worksheets-manage,"Highlight:After you open a worksheet, select the
visible when you hover over the tab for a worksheet to manage the open worksheet in the following ways:

Rename the worksheet.

Move the worksheet to a folder or a dashboard.

Duplicate the worksheet.

Delete the worksheet.",false
834712936,WAREHOUSE_METERING_HISTORY view | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/account-usage/warehouse_metering_history,,2024-08-14T08:41:56.965Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Faccount-usage%2Fwarehouse_metering_history,"Highlight:WAREHOUSE_METERING_HISTORY view¶

This Account Usage view can be used to return the hourly credit usage for a single warehouse (or all the warehouses in your account) within the last 365 days (1 year).",false
834712745,CURRENT_TASK_GRAPHS | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/current_task_graphs,,2024-08-14T08:40:37.131Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fcurrent_task_graphs,"Highlight:CURRENT_TASK_GRAPHS¶

Returns the status of a graph run that is currently scheduled or is executing.

Highlight:This function returns details for graph runs that are currently executing or are next scheduled to run within the next 8 days. To retrieve the details for graph runs that have completed in the past 60 minutes, query the COMPLETE_TASK_GRAPHS table function.

Highlight:Returns results only for the ACCOUNTADMIN role, the task owner (i.e. the role with the OWNERSHIP privilege on the task) or a role with the global MONITOR EXECUTION privilege. Note that unless a role with the MONITOR EXECUTION privilege also has the USAGE privilege on the database and schema that store the task, the DATABASE_NAME and SCHEMA_NAME values in the output are NULL.",false
834712678,Access History | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/access-history,,2024-08-14T08:39:54.609Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Faccess-history,"Highlight:Records in the Account Usage QUERY_HISTORY view do not always get recorded in the ACCESS_HISTORY view. The structure of the SQL statement determines whether Snowflake records an entry in the ACCESS_HISTORY view.

Highlight:The user access history can be found by querying the Account Usage ACCESS_HISTORY view. The records in this view facilitate regulatory compliance auditing and provide insights on popular and frequently accessed tables and columns because there is a direct link between the user (i.e. query operator), the query, the table or view, the column, and the data.",false
834712119,Overview of Access Control | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-access-control-overview#custom-roles,,2024-08-14T08:34:03.120Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-access-control-overview%23custom-roles,"Highlight:When creating roles that will serve as the owners of securable objects in the system, Snowflake recommends creating a hierarchy of custom roles, with the top-most custom role assigned to the system role SYSADMIN.",false
834451665,About Secure Data Sharing | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-sharing-intro#about-providers,,2024-08-13T17:40:24.513Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-sharing-intro%23about-providers,Highlight:A data provider is any Snowflake account that creates shares and makes them available to other Snowflake accounts to consume.,false
834447181,CREATE PIPE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/create-pipe,,2024-08-13T17:38:40.198Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcreate-pipe,"Highlight:Using a query as the source for the COPY statement for column reordering, column omission, and casts (i.e. transforming data during a load) is supported.

Highlight:Identifier for the pipe; must be unique for the schema in which the pipe is created.",false
834447035,SHOW GRANTS | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/show-grants#variants,,2024-08-13T17:37:53.091Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fshow-grants%23variants,Highlight:SHOW GRANTS TO USER current_user. Lists all the roles granted to the current user.,false
834446951,Stored procedures overview | Snowflake Documentation,,,https://docs.snowflake.com/developer-guide/stored-procedure/stored-procedures-overview#what-is-a-stored-procedure,,2024-08-13T17:37:16.283Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fdeveloper-guide%2Fstored-procedure%2Fstored-procedures-overview%23what-is-a-stored-procedure,Highlight:You might want to use a stored procedure to automate a task that requires multiple SQL statements and is performed frequently.,false
834446718,CREATE NETWORK POLICY | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/create-network-policy,,2024-08-13T17:35:48.353Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcreate-network-policy,"Highlight:Only security administrators (i.e. users with the SECURITYADMIN role) or higher or
 a role with the global CREATE NETWORK POLICY privilege can create network policies.",false
834446637,Tabular SQL UDFs (UDTFs) | Snowflake Documentation,,,https://docs.snowflake.com/en/developer-guide/udf/sql/udf-sql-tabular-functions,,2024-08-13T17:35:12.705Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fdeveloper-guide%2Fudf%2Fsql%2Fudf-sql-tabular-functions,"Highlight:CREATE OR REPLACE FUNCTION <name> ( [ <arguments> ] ) RETURNS
 TABLE
 ( <output_col_name> <output_col_type> [, <output_col_name> <output_col_type> ... ] ) AS '<sql_expression>'",false
834446284,GET_STAGE_LOCATION | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/get_stage_location,,2024-08-13T17:32:39.275Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fget_stage_location,"Highlight:GET_STAGE_LOCATION¶

Retrieves the URL for an external or internal named stage using the stage name as the input.",false
834446074,Introduction to external functions | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/external-functions-introduction,,2024-08-13T17:31:10.988Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fexternal-functions-introduction,"Highlight:An external function is a type of UDF
. Unlike other UDFs, an external function does not contain its own code; instead, the external function calls code that is stored and executed outside Snowflake.",false
834445740,Working with Materialized Views | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/views-materialized#label-limitations-on-creating-materialized-views,,2024-08-13T17:27:54.699Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fviews-materialized%23label-limitations-on-creating-materialized-views,"Highlight:The following limitations apply to creating materialized views:

A materialized view can query only a single table.

Joins, including self-joins, are not supported.

A materialized view cannot query:

A materialized view.

A non-materialized view.

A UDTF (user-defined table function).",false
834445617,SYSTEM$CLUSTERING_INFORMATION | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/system_clustering_information,,2024-08-13T17:26:45.391Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fsystem_clustering_information,"Highlight:SYSTEM$CLUSTERING_INFORMATION¶

Returns clustering information, including average clustering depth, for a table based on one or more columns in the table.

Highlight:average_overlaps

Average number of overlapping micro-partitions for each micro-partition in the table. A high number indicates the table is not well-clustered.",false
834442946,Create and configure shares | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-sharing-provider#general-data-sharing-considerations-and-usage,,2024-08-13T17:18:30.118Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-sharing-provider%23general-data-sharing-considerations-and-usage,"Highlight:For data security and privacy reasons, only secure views
 are supported in shares at this time. If a standard view is added to a share, Snowflake returns an error.",false
834442700,Search Optimization Service | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/search-optimization-service#understanding-the-search-optimization-service,,2024-08-13T17:16:54.359Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsearch-optimization-service%23understanding-the-search-optimization-service,"Highlight:The search optimization service aims to significantly improve the performance of certain types of queries on tables, including:

Selective point lookup queries on tables.",false
834434812,Working with resource monitors | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/resource-monitors#actions,,2024-08-13T17:14:29.474Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fresource-monitors%23actions,"Highlight:Resource monitors support the following actions:

Notify & Suspend:

Send a notification and suspend all assigned warehouses after all statements being executed by the warehouse(s) have completed.

Notify & Suspend Immediately:

Send a notification and suspend all assigned warehouses immediately, which cancels any statements being executed by the warehouses at the time.

Notify:

Perform no action on warehouses, but send a notification.",false
834433922,Working with resource monitors | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/resource-monitors#monitor-level,,2024-08-13T17:13:10.118Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fresource-monitors%23monitor-level,Highlight:A single monitor can be set at the account level to control credit usage for all warehouses in your account.,false
834422120,Snowflake's Security and Compliance Reports,,Explore our comprehensive Security and Compliance reports to understand how we protect your data and meet industry standards.,https://www.snowflake.com/en/legal/snowflakes-security-and-compliance-reports/,,2024-08-13T17:09:22.298Z,https://publish-p57963-e462109.adobeaemcloud.com/content/dam/snowflake-site/general/technical/default-og-image/snowflake-social-share.png,Highlight:SNOWFLAKE’S SECURITY & COMPLIANCE REPORTS,false
834416395,SHOW TABLES | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/show-tables,,2024-08-13T17:07:03.497Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fshow-tables,"Highlight:SHOW TABLES

Highlight:bytes

	

Number of bytes that will be scanned if the entire table is scanned in a query. Note that this number may be different than the number of actual physical bytes (i.e. bytes stored on-disk) for the table.",false
834414167,About Secure Data Sharing | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-sharing-intro#reader-accounts-for-third-party-access,,2024-08-13T17:04:31.814Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-sharing-intro%23reader-accounts-for-third-party-access,"Highlight:Data sharing is only supported between Snowflake accounts. As a data provider, you might want to share data with a consumer who does not already have a Snowflake account or is not ready to become a licensed Snowflake customer.

To facilitate sharing data with these consumers, you can create reader accounts. Reader accounts (formerly known as “read-only accounts”) provide a quick, easy, and cost-effective way to share data without requiring the consumer to become a Snowflake customer.",false
834409680,Warehouse considerations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/warehouses-considerations#how-are-credits-charged-for-warehouses,,2024-08-13T17:02:13.040Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fwarehouses-considerations%23how-are-credits-charged-for-warehouses,Highlight:The minimum billing charge for provisioning compute resources is 1 minute (i.e. 60 seconds).,false
834409158,GET | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/get,,2024-08-13T16:59:54.515Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fget,"Highlight:If the directory path includes special characters, the entire file URI must be enclosed in single quotes. Note that the drive and path separator is a forward slash (/
) in enclosed URIs (e.g. 'file://C:/temp/load
 data'
 for a path in Windows that includes a directory named load
 data
).",false
834408957,Micro-partitions & Data Clustering | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions#what-are-micro-partitions,,2024-08-13T16:58:28.122Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Ftables-clustering-micropartitions%23what-are-micro-partitions,"Highlight:All data in Snowflake tables is automatically divided into micro-partitions, which are contiguous units of storage. Each micro-partition contains between 50 MB and 500 MB of uncompressed data (note that the actual size in Snowflake is smaller because data is always stored compressed). Groups of rows in tables are mapped into individual micro-partitions, organized in a columnar fashion.",false
834408861,Best Practices for Data Ingestion with Snowflake - Blog,,"Snowflake offers a range of methods to meet different data pipeline needs, from batch ingestion to continuous ingestion, informed by customer best practices.",https://www.snowflake.com/blog/best-practices-for-data-ingestion/,,2024-08-13T16:57:25.937Z,https://www.snowflake.com/wp-content/uploads/2022/06/SF_Data_Ingestion.jpeg,"Highlight:Recommended file size for Snowpipe and cost considerations

There is a fixed, per-file overhead charge for Snowpipe in addition to the compute processing costs. We recommend files at least above 10 MB on average, with files in the 100 to 250 MB range offering the best cost-to-performance ratio.",false
834408658,SYSTEM$CLUSTERING_DEPTH | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/system_clustering_depth,,2024-08-13T16:55:59.742Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fsystem_clustering_depth,"Highlight:SYSTEM$CLUSTERING_DEPTH¶

Computes the average depth of the table according to the specified columns (or the clustering key defined for the table). The average depth of a populated table (i.e. a table containing data) is always 1 or more. The smaller the average depth, the better clustered the table is with regards to the specified columns.",false
834406095,Staging data files from a local file system | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-local-file-system-stage,,2024-08-13T16:55:06.836Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-local-file-system-stage,"Highlight:Note that the @~
 character combination identifies a user stage.

Highlight:Note that the @%
 character combination identifies a table stage.

Highlight:Note that the @
 character by itself identifies a named stage.",false
834403651,TABLE_STORAGE_METRICS view | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/account-usage/table_storage_metrics,,2024-08-13T16:50:13.455Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Faccount-usage%2Ftable_storage_metrics,"Highlight:TABLE_STORAGE_METRICS view¶

This Account Usage view displays table-level storage utilization information, which is used to calculate the storage billing for each table in the account, including tables that have been dropped, but are still incurring storage costs.",false
834403225,Access and install listings as a consumer | Snowflake Documentation,,,https://other-docs.snowflake.com/en/collaboration/consumer-listings-access#accessing-listings-on-the-marketplace,,2024-08-13T16:49:21.891Z,https://rdl.ink/render/https%3A%2F%2Fother-docs.snowflake.com%2Fen%2Fcollaboration%2Fconsumer-listings-access%23accessing-listings-on-the-marketplace,Highlight:You must use the ACCOUNTADMIN role or another role with the CREATE DATABASE and IMPORT SHARE privileges to access a listing.,false
834402920,ACCESS_HISTORY view | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/account-usage/access_history,,2024-08-13T16:46:38.937Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Faccount-usage%2Faccess_history,"Highlight:This Account Usage view can be used to query the access history of Snowflake objects (e.g. table, view, column) within the last 365 days (1 year).",false
834401816,Using External Tokenization | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-column-ext-token-use,,2024-08-13T16:45:06.053Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-column-ext-token-use,Highlight:Protegrity,false
834400102,Understanding and viewing Fail-safe | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-failsafe#what-is-fail-safe,,2024-08-13T16:43:19.316Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-failsafe%23what-is-fail-safe,Highlight:Fail-safe provides a (non-configurable) 7-day period during which historical data may be recoverable by Snowflake.,false
834399929,Querying Metadata for Staged Files | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/querying-metadata,,2024-08-13T16:41:43.543Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquerying-metadata,"Highlight:Metadata Columns¶

Currently, the following metadata columns can be queried or copied into tables:

METADATA$FILENAME

Name of the staged data file the current row belongs to. Includes the full path to the data file.

METADATA$FILE_ROW_NUMBER

Row number for each record in the staged data file.

METADATA$FILE_CONTENT_KEY

Checksum of the staged data file the current row belongs to.

METADATA$FILE_LAST_MODIFIED

Last modified timestamp of the staged data file the current row belongs to. Returned as TIMESTAMP_NTZ.

METADATA$START_SCAN_TIME

Start timestamp of operation for each record in the staged data file. Returned as TIMESTAMP_LTZ.",false
834399841,Introduction to Loading Semi-structured Data | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/semistructured-intro,,2024-08-13T16:41:01.699Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsemistructured-intro,"Highlight:Semi-structured data is typically stored in the following Snowflake data types:

ARRAY: similar to an array in other languages.

OBJECT: similar to a JSON object, also called a “dictionary”, “hash”, or “map” in many languages. This contains key-value pairs.

VARIANT: a data type that can hold a value of any other data type (including ARRAY and OBJECT). VARIANT is used to build and store hierarchical data.",false
834399703,Share data from multiple databases | Snowflake Documentation,,,https://docs.snowflake.com/user-guide/data-sharing-multiple-db,,2024-08-13T16:40:21.190Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fuser-guide%2Fdata-sharing-multiple-db,"Highlight:A provider stores customer data in separate databases and does not want to create new objects in those databases. To share data, the provider creates a new database with a secure view. The secure view references objects (schema, table, view) in the databases with customer data.",false
834399464,SnowCD (Connectivity Diagnostic Tool) | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/snowcd,,2024-08-13T16:38:31.842Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsnowcd,"Highlight:SnowCD (i.e. Snowflake Connectivity Diagnostic Tool) helps users to diagnose and troubleshoot their network connection to Snowflake.

Highlight:For example, users can integrate SnowCD into these use cases:

Automated deployment scripts.

A prerequisite check before deploying a service that connects to Snowflake.

Environment checks while starting a new machine.

Periodic checks on running machines.",false
834399301,Parameters | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/parameters#label-allow-client-mfa-caching,,2024-08-13T16:37:48.437Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fparameters%23label-allow-client-mfa-caching,"Highlight:ALLOW_CLIENT_MFA_CACHING¶
Type:

Account — Can only be set for Account

Highlight:Specifies whether an MFA token can be saved in the client-side operating system keystore to promote continuous, secure connectivity without users needing to respond to an MFA prompt at the start of each connection attempt to Snowflake.",false
834399116,Snowflake Editions | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/intro-editions#enterprise-edition,,2024-08-13T16:36:14.466Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fintro-editions%23enterprise-edition,Highlight:Column-level Security to apply masking policies to columns in tables or views.,false
834399023,Warehouse considerations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/warehouses-considerations#scaling-up-vs-scaling-out,,2024-08-13T16:35:38.701Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fwarehouses-considerations%23scaling-up-vs-scaling-out,"Highlight:Decreasing the size of a running warehouse removes compute resources from the warehouse. When the computer resources are removed, the cache associated with those resources is dropped, which can impact performance in the same way that suspending the warehouse can impact performance after it is resumed.",false
834398834,Preparing to unload data | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-unload-prepare#supported-file-formats,,2024-08-13T16:33:56.286Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-unload-prepare%23supported-file-formats,"Highlight:When unloading to JSON files, Snowflake outputs to the NDJSON
 (newline delimited JSON) standard format.",false
834398098,Getting started with Snowsight | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/ui-snowsight-gs,,2024-08-13T16:27:40.782Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fui-snowsight-gs,"Highlight:Your active role determines which pages in Snowsight you can access, as well as which databases, tables, and other objects you can see and the actions you can perform on them.

Highlight:To determine the fully qualified URL and port for Snowsight, run the SYSTEM$ALLOWLIST
 function and review the SNOWSIGHT_DEPLOYMENT
 entry in the return value.",false
834398006,Snowflake Sessions & Session Policies | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/session-policies#session-policies,,2024-08-13T16:26:52.591Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsession-policies%23session-policies,"Highlight:The minimum configurable idle timeout value for a session policy is 5 minutes.

If a session policy is not set, Snowflake uses a default value of 240 minutes (four hours).",false
834393007,Overview of Access Control | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-access-control-overview#system-defined-roles,,2024-08-13T16:22:23.748Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-access-control-overview%23system-defined-roles,"Highlight:USERADMIN:

(aka User and Role Administrator)

Role that is dedicated to user and role management only.

Highlight:SECURITYADMIN:

(aka Security Administrator)

Role that can manage any object grant globally, as well as create, monitor, and manage users and roles.",false
834392898,Snowpipe costs | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-snowpipe-billing,,2024-08-13T16:21:24.762Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-snowpipe-billing,"Highlight:Snowflake tracks the resource consumption of loads for all pipes in an account, with per-second/per-core granularity,

Highlight:er-core refers to the physical CPU cores in a compute server. The utilization recorded is then translated into familiar Snowflake credits, which are listed on the bill for your account.",false
834392830,Understanding and viewing Fail-safe | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-failsafe,,2024-08-13T16:20:48.369Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-failsafe,Highlight:Fail-safe provides a (non-configurable) 7-day period during which historical data may be recoverable by Snowflake.,false
834392717,"Database, schema, & share DDL | Snowflake Documentation",,,https://docs.snowflake.com/en/sql-reference/ddl-database,,2024-08-13T16:19:57.269Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fddl-database,"Highlight:Together, a database and schema comprise a namespace
 in Snowflake.

Highlight:Databases and schemas are used to organize data stored in Snowflake:

A database is a logical grouping of schemas. Each database belongs to a single Snowflake account.

A schema is a logical grouping of database objects (tables, views, etc.). Each schema belongs to a single database.",false
834392664,Snowflake Editions | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/intro-editions#data-sharing,,2024-08-13T16:19:32.315Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fintro-editions%23data-sharing,Highlight:Data Sharing,false
834392066,Sharing unstructured data with a secure view | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/unstructured-data-sharing,,2024-08-13T16:17:39.224Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Funstructured-data-sharing,"Highlight:Sharing unstructured data with a secure view

Highlight:Scoped URL¶

This example calls the BUILD_SCOPED_FILE_URL function to create a secure view with the scoped URLs for a set of staged files.

Highlight:Pre-signed URL¶

This example calls the GET_PRESIGNED_URL function to retrieve the pre-signed URLs for a set of staged files.",false
834391923,Key Concepts & Architecture | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/intro-key-concepts#cloud-services,,2024-08-13T16:16:25.542Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fintro-key-concepts%23cloud-services,"Highlight:Services managed in this layer include:

Authentication

Infrastructure management

Metadata management

Query parsing and optimization

Access control",false
834391866,COPY INTO location | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/copy-into-location,,2024-08-13T16:15:54.402Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcopy-into-location,"Highlight:VALIDATION_MODE = RETURN_ROWS

String (constant) that instructs the COPY command to return the results of the query in the SQL statement instead of unloading the results to the specified cloud storage location. The only supported validation option is RETURN_ROWS. This option returns all rows produced by the query.

When you have validated the query, you can remove the VALIDATION_MODE to perform the unload operation.

Highlight:SINGLE = TRUE | FALSE
Definition:

Boolean that specifies whether to generate a single file or multiple files. If FALSE, a filename prefix must be included in path.",false
834391745,COPY INTO table | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/copy-into-table,,2024-08-13T16:14:58.761Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcopy-into-table,"Highlight:VALIDATION_MODE = RETURN_n_ROWS | RETURN_ERRORS | RETURN_ALL_ERRORS

String (constant) that instructs the COPY command to validate the data files instead of loading them into the specified table; i.e. the COPY command tests the files for errors but does not load them. The command validates the data to be loaded and returns results based on the validation option specified:

Supported Values

	

Notes




RETURN_n_ROWS (e.g. RETURN_10_ROWS)

	

Validates the specified number of rows, if no errors are encountered; otherwise, fails at the first error encountered in the rows.




RETURN_ERRORS

	

Returns all errors (parsing, conversion, etc.) across all files specified in the COPY statement.




RETURN_ALL_ERRORS

	

Returns all errors across all files specified in the COPY statement, including files with errors that were partially loaded during an earlier load because the ON_ERROR copy option was set to CONTINUE during the load.

Highlight:VALIDATION_MODE does not support COPY statements that transform data during a load. If the parameter is specified, the COPY statement returns an error.",false
834391627,Data unloading considerations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-unload-considerations#empty-strings-and-null-values,,2024-08-13T16:13:50.919Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-unload-considerations%23empty-strings-and-null-values,"Highlight:NULL_IF = ( 'string1' [ , 'string2' ... ] )

When unloading data from tables: Snowflake converts SQL NULL values to the first value in the list. Be careful to specify a value that you want interpreted as NULL.

Highlight:EMPTY_FIELD_AS_NULL = TRUE | FALSE

When unloading empty string data from tables, choose one of the following options:

Preferred: Enclose strings in quotes by setting the FIELD_OPTIONALLY_ENCLOSED_BY option, to distinguish empty strings from NULLs in output CSV files.

Leave string fields unenclosed by setting the FIELD_OPTIONALLY_ENCLOSED_BY option to NONE (default), and set the EMPTY_FIELD_AS_NULL value to FALSE to unload empty strings as empty fields.",false
834391359,Overview of warehouses | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/warehouses-overview#default-warehouse-for-client-utilities-drivers-connectors,,2024-08-13T16:11:39.021Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fwarehouses-overview%23default-warehouse-for-client-utilities-drivers-connectors,"Highlight:In addition to default warehouses for users, any of the Snowflake clients (SnowSQL, JDBC driver, ODBC driver, Python connector, etc.) can have a default warehouse:

SnowSQL supports both a configuration file and command line option for specifying a default warehouse.

The drivers and connectors support specifying a default warehouse as a connection parameter when initiating a session.",false
834390519,Data Governance in Snowflake | Snowflake Documentation,,,https://docs.snowflake.com/en/guides-overview-govern,,2024-08-13T16:10:00.354Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fguides-overview-govern,"Highlight:Data Classification

Allows categorizing potentially personal and/or sensitive data to support compliance and privacy regulations.",false
834389045,Understanding Encryption Key Management in Snowflake | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-encryption-manage#hierarchical-key-model,,2024-08-13T16:08:58.605Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-encryption-manage%23hierarchical-key-model,"Highlight:A hierarchical key model provides a framework for Snowflake’s encryption key management. The hierarchy is composed of several layers of keys in which each higher layer of keys (parent keys) encrypts the layer below (child keys). In security terminology, a parent key encrypting all child keys is known as “wrapping”.

Snowflake’s hierarchical key model consists of four levels of keys:

The root key

Account master keys

Table master keys

File keys",false
834387030,Snowflake SCIM support | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/scim-intro,,2024-08-13T16:07:30.779Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fscim-intro,"Highlight:The Snowflake SCIM API can address the following use cases.

Highlight:Managing users: Administrators can provision and manage their users from their organization’s identity provider to Snowflake. User management is a one-to-one mapping from the identity provider to Snowflake.

Managing groups: Administrators can provision and manage their groups (i.e. Roles) from their organization’s identity provider to Snowflake. Role management is a one-to-one mapping from the identity provider to Snowflake.

Auditing SCIM API requests: Administrators can query the rest_event_history table to determine whether the identity provider is sending updates (i.e. SCIM API requests) to Snowflake.

SCIM API",false
834384110,Preparing your data files | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare#semi-structured-data-size-limitations,,2024-08-13T16:04:50.417Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-considerations-prepare%23semi-structured-data-size-limitations,"Highlight:If the data exceeds 16 MB, enable the STRIP_OUTER_ARRAY file format option for the COPY INTO <table> command to remove the outer array structure and load the records into separate table rows",false
834382624,Working with Temporary and Transient Tables | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/tables-temp-transient,,2024-08-13T16:01:53.053Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Ftables-temp-transient,"Highlight:Temporary tables only exist within the session in which they were created and persist only for the remainder of the session. As such, they are not visible to other users or sessions.

Highlight:because transient tables do not utilize Fail-safe, there are no Fail-safe costs",false
834365667,Querying Semi-structured Data | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/querying-semistructured,,2024-08-13T15:17:39.115Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquerying-semistructured,"Highlight:Regardless of which notation you use, the column name is case-insensitive but element names are case-sensitive. For example, in the following list, the first two paths are equivalent, but the third is not:

src:salesperson.name

SRC:salesperson.name

SRC:Salesperson.Name

Highlight:FLATTEN is a table function that produces a lateral view of a VARIANT, OBJECT, or ARRAY column. The function returns a row for each object, and the LATERAL modifier joins the data with any information outside of the object.",false
834365500,Introduction to organizations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/organizations#orgadmin-role,,2024-08-13T15:16:23.426Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Forganizations%23orgadmin-role,"Highlight:A user with the ORGADMIN role can perform the following actions:

Create an account in the organization. For more information, refer to Creating an account.

View/show all accounts within the organization. For more information, refer to Viewing accounts in your organization.

View/show a list of regions enabled for the organization. For more information, see Viewing a List of Regions Available for an Organization.

View usage information for all accounts in the organization. For more information, see Organization Usage.

Enable replication for an account in the organization. For more information, see Prerequisite: Enable replication for accounts in the organization.",false
834365245,Analyzing Queries Using Query Profile | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/ui-query-profile#inefficient-pruning,,2024-08-13T15:15:18.275Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fui-query-profile%23inefficient-pruning,Highlight:Snowflake collects rich statistics on data allowing it not to read unnecessary parts of a table based on the query filters.,false
834364810,Optimizing warehouses for performance | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/performance-query-warehouse,,2024-08-13T15:13:58.704Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fperformance-query-warehouse,"Highlight:Reduce queues

	

Minimizing queuing can improve performance because the time between submitting a query and getting its results is longer when the query must wait in a queue before starting.




Resolve memory spillage

	

Adjusting the available memory of a warehouse can improve performance because a query runs substantially slower when a warehouse runs out of memory, which results in bytes “spilling” onto storage.




Increase warehouse size

	

The larger a warehouse, the more compute resources are available to execute a query or set of queries.




Try query acceleration

	

The query acceleration service offloads portions of query processing to serverless compute resources, which speeds up the processing of a query while reducing its demand on the warehouse’s compute resources.




Optimize the warehouse cache

	

Query performance improves if a query can read from the warehouse’s cache instead of from tables.




Limit concurrently running queries

	

Limiting the number of queries that are running concurrently in a warehouse can improve performance because there are fewer queries putting demands on the warehouse’s resources.",false
834362543,Clustering Keys & Clustered Tables | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/tables-clustering-keys,,2024-08-13T15:07:43.060Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Ftables-clustering-keys,"Highlight:Whether you want faster response times or lower overall costs, clustering is best for a table that meets all
 of the following criteria:

The table contains a large number of micro-partitions
. Typically, this means that the table contains multiple terabytes (TB) of data.

Highlight:Snowflake recommends prioritizing keys in the order below:

Highlight:Cluster columns that are most actively used in selective filters.

Highlight:If there is room for additional cluster keys, then consider columns frequently used in join predicates

Highlight:The number of distinct values (i.e. cardinality) in a column/expression is a critical aspect of selecting it as a clustering key. It is important to choose a clustering key that has:

A large enough number of distinct values to enable effective pruning on the table.

A small enough number of distinct values to allow Snowflake to effectively group rows in the same micro-partitions.

Highlight:Defining a Clustering Key for a Table¶

A clustering key can be defined when a table is created by appending a CLUSTER BY clause to CREATE TABLE

Highlight:Therefore, clustering is generally most cost-effective for tables that are queried frequently and do not change frequently.

Highlight:For most tables, Snowflake recommends a maximum of 3 or 4 columns (or expressions) per key. Adding more than 3-4 columns tends to increase costs more than benefits.

Highlight:If you are defining a multi-column clustering key for a table, the order in which the columns are specified in the CLUSTER BY clause is important. As a general rule, Snowflake recommends ordering the columns from lowest cardinality to highest cardinality.

Highlight:In some cases, clustering on columns used in GROUP BY or ORDER BY clauses can be helpful. However, clustering on these columns is usually less helpful than clustering on columns that are heavily used in filter or JOIN operations.

Highlight:Use the system function, SYSTEM$CLUSTERING_INFORMATION, to calculate clustering details, including clustering depth, for a given table. This function can be run on any columns on any table, regardless of whether the table has an explicit clustering key",false
834362005,Preparing to unload data | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-unload-prepare,,2024-08-13T15:04:20.361Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-unload-prepare,"Highlight:Structured

	

Delimited (CSV, TSV, etc.)

	

Any valid singlebyte delimiter is supported; default is comma (i.e. CSV).




Semi-structured

	

JSON, Parquet",false
834361746,Snowflake Releases | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/intro-releases,,2024-08-13T15:03:08.522Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fintro-releases,"Highlight:This topic describes the process we follow for weekly releases, including the option to request 12-hour early access for Enterprise Edition and Business Critical Edition accounts, or 24-hour early access for Virtual Private Snowflake (VPS) accounts to enable additional release testing (if desired).

Highlight:Each week, Snowflake deploys two planned/scheduled releases

Highlight:Each month (except for November and December), Snowflake selects one of the weekly full releases for the month to introduce behavior changes.",false
834361297,DROP ROLE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/drop-role,,2024-08-13T15:00:46.922Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fdrop-role,Highlight:Ownership of any objects owned by the dropped role is transferred to the role that executes the DROP ROLE command.,false
834360889,Working with resource monitors | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/resource-monitors,,2024-08-13T14:59:09.237Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fresource-monitors,"Highlight:Select the Monitor Level. Choose Account to create an account monitor or choose Warehouse to select the warehouses to monitor.

Highlight:Only users with the ACCOUNTADMIN role can create a resource monitor, but an account administrator can grant privileges to other roles to allow other users to view and modify resource monitors.

Highlight:You can use a resource monitor to monitor credit usage by virtual warehouses and the cloud services needed to support those warehouses.

Highlight:However, roles that have been granted the following privileges on specific resource monitors can view and modify the resource monitor as needed using SQL:

MONITOR

MODIFY

Highlight:A single monitor can be set at the account level to control credit usage for all warehouses in your account.

In addition, a one or more warehouses can be assigned to a resource monitor, thereby controlling the credit usage for each assigned warehouse. Note, however, that a warehouse can be assigned to only a single resource monitor below the account level.",false
834360188,Multi-cluster warehouses | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/warehouses-multicluster#maximized-vs-auto-scale,,2024-08-13T14:57:09.810Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fwarehouses-multicluster%23maximized-vs-auto-scale,"Highlight:You can choose to run a multi-cluster warehouse in either of the following modes:

Highlight:Maximized:

This mode is enabled by specifying the same value for both maximum and minimum number of clusters (note that the specified value must be larger than 1). In this mode, when the warehouse is started, Snowflake starts all the clusters so that maximum resources are available while the warehouse is running.

Highlight:Auto-scale:

This mode is enabled by specifying different values for maximum and minimum number of clusters. In this mode, Snowflake starts and stops clusters as needed to dynamically manage the load on the warehouse

Highlight:Snowflake supports the following scaling policies:

Highlight:Standard (default)

Highlight:After 2 to 3 consecutive successful checks (performed at 1 minute intervals), which determine whether the load on the least-loaded cluster could be redistributed to the other clusters without spinning up the cluster again.

Highlight:Economy

Highlight:After 5 to 6 consecutive successful checks (performed at 1 minute intervals), which determine whether the load on the least-loaded cluster could be redistributed to the other clusters without spinning up the cluster again.

Note",false
834360045,Warehouse considerations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/warehouses-considerations#automating-warehouse-suspension,,2024-08-13T14:56:33.283Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fwarehouses-considerations%23automating-warehouse-suspension,"Highlight:ou might want to consider disabling auto-suspend for a warehouse if:

You have a heavy, steady workload for the warehouse.

You require the warehouse to be available with no delay or lag time. Warehouse provisioning is generally very fast (e.g. 1 or 2 seconds); however, depending on the size of the warehouse and the availability of compute resources to provision, it can take longer.",false
834359609,Planning a data load | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-considerations-plan,,2024-08-13T14:55:47.816Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-considerations-plan,Highlight:The number of data files that can be processed in parallel is determined by the amount of compute resources in a warehouse. I,false
834359253,CREATE STAGE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/create-stage,,2024-08-13T14:54:52.344Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcreate-stage,"Highlight:Specifies that the stage created is temporary and will be dropped at the end of the session in which it was created.

Highlight:All files are also automatically encrypted using AES-256 strong encryption on the server side.

Highlight:SNOWFLAKE_FULL: Client-side and server-side encryption. The files are encrypted by a client when it uploads them to the internal stage using PUT. Snowflake uses a 128-bit encryption key by default. You can configure a 256-bit key by setting the CLIENT_ENCRYPTION_KEY_SIZE parameter.

Highlight:If you require Tri-Secret Secure for security compliance, use the SNOWFLAKE_FULL encryption type for internal stages. SNOWFLAKE_SSE does not support Tri-Secret Secure.",false
834358171,Controlling network traffic with network policies | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/network-policies,,2024-08-13T14:53:48.973Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fnetwork-policies,"Highlight:You need the CREATE NETWORK RULE privilege on the schema to create a network rule. By default, only the ACCOUNTADMIN and SECURITYADMIN roles, along with the schema owner, have this privilege.

Highlight:Only security administrators (i.e. users with the SECURITYADMIN role) or higher or a role with the global CREATE NETWORK POLICY privilege can create network policies. Ownership of a network policy can be transferred to another role.

Highlight:If a network policy has the same IP address values in both the ALLOWED_IP_LIST and the BLOCKED_IP_LIST parameters, Snowflake applies the values in the BLOCKED_IP_LIST parameter first. This behavior also applies to the ALLOWED_NETWORK_RULE_LIST and the BLOCKED_NETWORK_RULE_LIST parameters.",false
834356244,Snowflake Community,,"Join our community of data professionals to learn, connect, share and innovate together",https://community.snowflake.com/s/article/How-to-Load-Terabytes-Into-Snowflake-Speeds-Feeds-and-Techniques,,2024-08-13T14:51:55.394Z,https://www.snowflake.com/wp-content/themes/snowflake/assets/img/community/community-social-share.jpg,Highlight:Loading from Gzipped CSV is several times faster than loading from ORC and Parquet at an impressive 15 TB/Hour.,false
834356128,Warehouse considerations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/warehouses-considerations#multi-cluster-warehouses-improve-concurrency,,2024-08-13T14:51:09.477Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fwarehouses-considerations%23multi-cluster-warehouses-improve-concurrency,Highlight:Multi-cluster warehouses are designed specifically for handling queuing and performance issues related to large numbers of concurrent users and/or queries.,false
834354867,Installing and configuring the Kafka connector | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/kafka-connector-install,,2024-08-13T14:50:12.697Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fkafka-connector-install,Highlight:The package includes all dependencies required to use either an encrypted or unencrypted private key for key pair authentication.,false
834354558,SPLIT_TO_TABLE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/split_to_table,,2024-08-13T14:48:47.004Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fsplit_to_table,"Highlight:SPLIT_TO_TABLE¶

This table function splits a string (based on a specified delimiter) and flattens the results into rows.",false
834353649,Configuring access control | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-access-control-configure#label-managed-access-schemas,,2024-08-13T14:45:42.176Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-access-control-configure%23label-managed-access-schemas,"Highlight:In regular (i.e. non-managed) schemas, object owners (i.e. a role with the OWNERSHIP privilege on an object) can grant access on their objects to other roles, with the option to further grant those roles the ability to manage object grants

Highlight:With managed access schemas, object owners lose the ability to make grant decisions. Only the schema owner (i.e. the role with the OWNERSHIP privilege on the schema) or a role with the MANAGE GRANTS privilege can grant privileges on objects in the schema, including future grants, centralizing privilege management.

Highlight:The following table indicates which roles can manage object privileges in a regular or managed access schema:

Role

	

Can grant object privileges in a regular schema

	

Can grant object privileges in a managed access schema




SYSADMIN

	

No

	

No




SECURITYADMIN or higher

	

Yes

	

Yes




Database owner

	

No

	

No




Schema owner

	

No

	

Yes




Object owner

	

Yes

	

No




Any role with the MANAGE GRANTS privilege

	

Yes

	

Yes

Highlight:Execute an ALTER SCHEMA statement with the ENABLE | DISABLE MANAGED ACCESS keywords.",false
834351510,CREATE SHARE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/create-share#access-control-requirements,,2024-08-13T14:44:15.430Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcreate-share%23access-control-requirements,"Highlight:CREATE SHARE

	

Account

	

Only the ACCOUNTADMIN role has this privilege by default. The privilege can be granted to additional roles as needed.",false
834351243,Data unloading considerations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-unload-considerations#unloading-a-relational-table-to-json,,2024-08-13T14:43:41.745Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-unload-considerations%23unloading-a-relational-table-to-json,Highlight:You can use the OBJECT_CONSTRUCT function combined with the COPY command to convert the rows in a relational table to a single VARIANT column and unload the rows into a file.,false
834351089,VALIDATE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/validate,,2024-08-13T14:42:38.956Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fvalidate,"Highlight:VALIDATE¶

Validates the files loaded in a past execution of the COPY INTO <table> command and returns all the errors encountered during the load, rather than just the first error.

Highlight:JOB_ID => query_id | _last

The ID for the COPY INTO <table> command to be validated:",false
834348242,Account Usage | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/account-usage#differences-between-account-usage-and-information-schema,,2024-08-13T14:38:00.126Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Faccount-usage%23differences-between-account-usage-and-information-schema,"Highlight:Certain account usage views provide historical usage metrics. The retention period for these views is 1 year (365 days).

In contrast, the corresponding views and table functions in the Snowflake Information Schema have much shorter retention periods, ranging from 7 days to 6 months, depending on the view.

Highlight:Due to the process of extracting the data from Snowflake’s internal metadata store, the account usage views have some natural latency:

For most of the views, the latency is 2 hours (120 minutes).

For the remaining views, the latency varies between 45 minutes and 3 hours.

Highlight:In contrast, views/table functions in the Snowflake Information Schema do not have any latency.

Highlight:Account usage views include records for all objects that have been dropped.",false
834346571,Create and configure shares | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-sharing-provider,,2024-08-13T14:35:06.895Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-sharing-provider,"Highlight:Before creating a share, Snowflake recommends identifying the Snowflake objects you plan to share:

Databases

Tables

Dynamic tables

External tables

Iceberg tables

Secure views

Secure materialized views

Secure user-defined functions (UDFs)

Highlight:You can remove objects from an existing share at any time. Any objects that you remove from a share are instantly unavailable to the consumers accounts who have created databases from the share.

For example, if you remove a table from a share, users in consumer accounts can no longer query the data in the table as soon as the table is removed from the share.

Highlight:Creating a share¶

You must use the ACCOUNTADMIN role or a role that has been granted the CREATE SHARE global privilege to create shares

Highlight:Use the CREATE SHARE command to create an empty share.",false
834345618,ALTER SHARE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/alter-share,,2024-08-13T14:32:52.276Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Falter-share,"Highlight:ADD | REMOVE ACCOUNTS = consumer_account [ , consumer_account , ... ]

Specifies the name of the account(s) to add or remove from the list of accounts for the share",false
834345244,Queries too large to fit in memory | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/performance-query-warehouse-memory,,2024-08-13T14:30:31.067Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fperformance-query-warehouse-memory,"Highlight:Data spilling to storage can have a negative impact on query performance (especially if the query has to spill to remote storage). To alleviate this, Snowflake recommends:

Using a larger warehouse (effectively increasing the available memory/local storage space for the operation)

Processing data in smaller batches.",false
834345121,Scalar functions | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions,,2024-08-13T14:29:37.629Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions,Highlight:A scalar function is a function that returns one value per invocation,false
834345053,Understanding Column-level Security | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-column-intro#what-are-masking-policies,,2024-08-13T14:29:08.048Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-column-intro%23what-are-masking-policies,"Highlight:Object owners (i.e. the role that has the OWNERSHIP privilege on the object) do not have the privilege to unset masking policies.

Object owners cannot view column data in which a masking policy applies.",false
834344914,Snowpipe | Snowflake Documentation,,,https://docs.snowflake.com/user-guide/data-load-snowpipe-intro,,2024-08-13T14:28:13.499Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fuser-guide%2Fdata-load-snowpipe-intro,"Highlight:Snowpipe loads the new data files into the target table in a continuous, serverless fashion based on the parameters defined in a specified pipe object.",false
834344797,Manage reader accounts | Snowflake Documentation,,,https://docs.snowflake.com/user-guide/data-sharing-reader-create,,2024-08-13T14:27:10.079Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fuser-guide%2Fdata-sharing-reader-create,"Highlight:Reader accounts (formerly known as “read-only accounts”) enable providers to share data with consumers who are not already Snowflake customers, without requiring the consumers to become Snowflake customers.",false
834343567,Working with loops | Snowflake Documentation,,,https://docs.snowflake.com/en/developer-guide/snowflake-scripting/loops,,2024-08-13T14:18:27.029Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fdeveloper-guide%2Fsnowflake-scripting%2Floops,"Highlight:Snowflake Scripting supports the following types of loops:

FOR

WHILE

REPEAT

LOOP

Highlight:A FOR loop repeats a sequence of steps for a specified number of times or for each row in a result set.

Highlight:A WHILE loop iterates while a condition is true.

Highlight:A REPEAT loop iterates until a condition is true.

Highlight:A LOOP loop executes until a BREAK command is executed. A BREAK command is normally embedded inside branching logic (e.g. IF statements or CASE statements).",false
834343477,Understanding Column-level Security | Snowflake Documentation,,,https://docs.snowflake.com/user-guide/security-column-intro#what-is-column-level-security,,2024-08-13T14:17:55.711Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fuser-guide%2Fsecurity-column-intro%23what-is-column-level-security,"Highlight:Currently, Column-level Security includes two features:

Dynamic Data Masking

External Tokenization",false
834343405,Snowflake Editions | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/intro-editions#security-governance-data-protection,,2024-08-13T14:17:25.596Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fintro-editions%23security-governance-data-protection,"Highlight:Business Critical Edition, formerly known as Enterprise for Sensitive Data (ESD), offers even higher levels of data protection to support the needs of organizations with extremely sensitive data, particularly PHI data that must comply with HIPAA and HITRUST CSF regulations.",false
834343296,Understanding & using Time Travel | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-time-travel#data-retention-period,,2024-08-13T14:16:26.001Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-time-travel%23data-retention-period,"Highlight:When the retention period ends for an object, the historical data is moved into Snowflake Fail-safe:

Historical data is no longer available for querying.

Past objects can no longer be cloned.

Past objects that were dropped can no longer be restored.",false
834342970,Share data securely across regions and cloud platforms | Snowflake Documentation,,,https://docs.snowflake.com/user-guide/secure-data-sharing-across-regions-platforms,,2024-08-13T14:15:33.378Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fuser-guide%2Fsecure-data-sharing-across-regions-platforms,"Highlight:replication to allow data providers to securely share data with data consumers across different regions and cloud platforms.

Highlight:Since cross-region data sharing utilizes Snowflake data replication functionality, understand how replication works in Snowflake as part of your planning process. For more information, see:

Introduction to replication and failover across multiple accounts

Replication considerations

Replicating databases and account objects across multiple accounts

Highlight:Data providers only need to create one copy of the dataset per region; and not a copy per consumer.

Highlight:When sharing a view that references objects in multiple databases, each of these other databases must be included in the replication group. Sharing data from more than one database requires additional steps. For instructions, see Share data from multiple databases.

You can share content to a Virtual Private Snowflake (VPS) using a listing if the VPS customer has enabled auto-fulfillment. See Support for Auto-fulfillment in Virtual Private Snowflake for more details. Sharing to and from VPS is not supported using a direct share.

Highlight:Before configuring data replication, you must create an account in a region where you wish to share data and link it to your local account.

Highlight:ORGADMIN role must enable replication for the source account",false
834341689,CREATE object … CLONE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/create-clone,,2024-08-13T14:12:08.134Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcreate-clone,"Highlight:CREATE <object> … CLONE¶

Creates a copy of an existing object in the system. This command is primarily used for creating zero-copy clones of databases, schemas, and tables; however, it can also be used to quickly/easily create clones of other schema objects , such as external stages, file formats, and sequences, and database roles.

Highlight:The clone of the view references the source table with the same fully-qualified name",false
834338802,All Partners & Technologies (Alphabetical) | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/ecosystem-all,,2024-08-13T14:08:25.079Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fecosystem-all,"Highlight:Alation — enterprise data catalog

	

Security, Governance & Observability

	

Snowflake Partner Connect

Snowflake Ready Validated

Highlight:If you need to connect to Snowflake using a tool or technology that is not listed here, we suggest attempting to connect through our JDBC
 or ODBC
 drivers.",false
834336264,Overview of federated authentication and SSO | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/admin-security-fed-auth-overview#what-is-a-federated-environment,,2024-08-13T14:01:21.331Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fadmin-security-fed-auth-overview%23what-is-a-federated-environment,"Highlight:In a federated environment, user authentication is separated from user access",false
834335897,Querying directory tables | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-dirtables-query,,2024-08-13T14:00:33.090Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-dirtables-query,"Highlight:The output from a directory table query can include the following columns:

Highlight:RELATIVE_PATH

	

TEXT

	

Path to the files to access using the file URL.




SIZE

	

NUMBER

	

Size of the file (in bytes).




LAST_MODIFIED

	

TIMESTAMP_LTZ

	

Timestamp when the file was last updated in the stage.




MD5

	

HEX

	

MD5 checksum for the file.




ETAG

	

HEX

	

ETag header for the file.




FILE_URL

	

TEXT

	

Snowflake file URL to the file.

Highlight:Syntax for querying a directory table:

SELECT * FROM DIRECTORY( @<stage_name> )",false
834335314,Managing regular data loads | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-considerations-manage,,2024-08-13T13:56:25.028Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-considerations-manage,"Highlight:Staged files can be deleted from a Snowflake stage (user stage, table stage, or named stage) using the following methods:

Files that were loaded successfully can be deleted from the stage during a load by specifying the PURGE copy option in the COPY INTO <table>
 command.

After the load completes, use the REMOVE
 command to remove the files in the stage.",false
834335132,Snowsight quick tour | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/ui-snowsight-quick-tour,,2024-08-13T13:55:02.293Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fui-snowsight-quick-tour,"Highlight:Monitor activity in Snowsight¶

You can monitor and view query details, explore the performance of executed queries, monitor data loading status and errors, review task graphs, and debug and re-run them as needed. You can also monitor the refresh state of your Dynamic Tables and review the various tagging and security policies that you create to maintain data governance.",false
834334674,Redirecting client connections | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/client-redirect,,2024-08-13T13:52:17.781Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fclient-redirect,"Highlight:Client Redirect enables redirecting your client connections to Snowflake accounts in different regions
 for business continuity and disaster recovery, or when migrating your account to another region or cloud platform.",false
834333065,Object Tagging | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/object-tagging,,2024-08-13T13:40:27.243Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fobject-tagging,"Highlight:This topic provides concepts and instructions on how to use tags in Snowflake.

To learn more about using a masking policy with a tag, see Tag-based masking policies.

 Enterprise Edition Feature

This feature requires Enterprise Edition or higher.

Highlight:The string value for each tag can be up to 256 characters

Highlight:Tags enable data stewards to monitor sensitive data for compliance, discovery, protection, and resource usage use cases through either a centralized or decentralized data governance management approach.",false
834332496,Introduction to replication and failover across multiple accounts | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/account-replication-intro,,2024-08-13T13:36:33.668Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Faccount-replication-intro,"Highlight:This feature enables the replication of objects from a source account to one or more target accounts in the same organization. Replicated objects in each target account are referred to as secondary objects and are replicas of the primary objects in the source account. Replication is supported across regions and across cloud platforms.

Highlight:Database and share replication are available to all accounts.

Replication of other account objects & failover/failback require Business Critical Edition (or higher). To inquire about upgrading, please contact Snowflake Support.",false
834332165,Introduction to business continuity & disaster recovery | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/replication-intro,,2024-08-13T13:35:09.217Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Freplication-intro,"Highlight:Database and share replication are available to all accounts.

Replication of other account objects, failover/failback, and Client Redirect require Business Critical (or higher).",false
834329637,Getting started with organizations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/organizations-gs,,2024-08-13T13:32:57.303Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Forganizations-gs,"Highlight:If you want to change the name of an organization, for example to change a system-generated name to a more user-friendly one, contact Snowflake Support.

Highlight:Once enabled in an account, the ORGADMIN role can be granted to any user or role in the account by an ACCOUNTADMIN using the GRANT ROLE command.",false
834329562,Introduction to organizations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/organizations,,2024-08-13T13:32:32.804Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Forganizations,"Highlight:Once an account is created, ORGADMIN can view the account properties but does not have access to the account data.",false
834329259,CREATE USER | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/create-user,,2024-08-13T13:30:29.680Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcreate-user,Highlight:MINS_TO_BYPASS_MFA = <integer>,false
834320660,REST API for unstructured data support | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-unstructured-rest-api,,2024-08-13T12:47:15.471Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-unstructured-rest-api,"Highlight:Scoped URL:

Only the user who generated the scoped URL can use the URL to access the referenced file.

Highlight:GET
 /api/files/
¶

Highlight:File URL:

Any role that has sufficient privileges on the stage can access the file:

External stage: USAGE

Internal stage: READ",false
834320453,Multi-factor authentication (MFA) | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-mfa,,2024-08-13T12:46:15.962Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-mfa,"Highlight:Snowflake supports MFA token caching with the following drivers and connectors on macOS and Windows. This feature is not supported on Linux.

ODBC driver version 2.23.0 (or later).

JDBC driver version 3.12.16 (or later).

Python Connector for Snowflake version 2.3.7 (or later).

Highlight:At a minimum, Snowflake strongly recommends that all users with the ACCOUNTADMIN role be required to use MFA.

Highlight:MFA is enabled on a per-user basis; however, at this time, users are not automatically enrolled in MFA. To use MFA, users must enroll themselves.

Highlight:MFA support is provided as an integrated Snowflake feature, powered by the Duo Security service, which is managed completely by Snowflake.",false
834319881,Parameters | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/parameters#statement-queued-timeout-in-seconds,,2024-08-13T12:43:30.582Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fparameters%23statement-queued-timeout-in-seconds,"Highlight:Amount of time, in seconds, a SQL statement (query, DDL, DML, etc.) remains queued for a warehouse before it is canceled by the system. This parameter can be used in conjunction with the MAX_CONCURRENCY_LEVEL parameter to ensure a warehouse is never backlogged.

Highlight:STATEMENT_QUEUED_TIMEOUT_IN_SECONDS

Highlight:Type:

Session and Object (for warehouses)",false
834319647,Controlling network traffic with network policies | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/network-policies#bypassing-a-network-policy,,2024-08-13T12:41:27.844Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fnetwork-policies%23bypassing-a-network-policy,"Highlight:It is possible to temporarily bypass a network policy for a set number of minutes by configuring the user object property MINS_TO_BYPASS_NETWORK_POLICY
, which can be viewed by executing DESCRIBE USER
. Only Snowflake can set the value for this object property. Please contact Snowflake Support
 to set a value for this property.",false
834319434,Data unloading considerations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-unload-considerations,,2024-08-13T12:40:27.081Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-unload-considerations,"Highlight:When floating-point number
 columns are unloaded to CSV or JSON files, Snowflake truncates the values to approximately (15,9).

The values are not
 truncated when unloading floating-point number columns to Parquet files.

Highlight:exporting data from Snowflake tables into files in stages using the COPY INTO <location> command.

Highlight:You can use the OBJECT_CONSTRUCT function combined with the COPY command to convert the rows in a relational table to a single VARIANT column and unload the rows into a file.

Highlight:By default, COPY INTO location statements separate table data into a set of output files to take advantage of parallel operations. The maximum size for each file is set using the MAX_FILE_SIZE copy option. The default value is 16777216 (16 MB) but can be increased to accommodate larger files. The maximum file size supported is 5 GB for Amazon S3, Google Cloud Storage, or Microsoft Azure stages.",false
834319182,FLATTEN | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/flatten#output,,2024-08-13T12:38:13.488Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fflatten%23output,"Highlight:Output¶

The returned rows consist of a fixed set of columns:

+-----+------+------+-------+-------+------+
| SEQ |  KEY | PATH | INDEX | VALUE | THIS |
|-----+------+------+-------+-------+------|

SEQ:

A unique sequence number associated with the input record; the sequence is not guaranteed to be gap-free or ordered in any particular way.

KEY:

For maps or objects, this column contains the key to the exploded value.

PATH:

The path to the element within a data structure which needs to be flattened.

INDEX:

The index of the element, if it is an array; otherwise NULL.

VALUE:

The value of the element of the flattened array/object.

THIS:

The element being flattened (useful in recursive flattening).

Note

The columns of the original (correlated) table which was used as the source of data for FLATTEN are also accessible. If a single row from the original table resulted in multiple rows in the flattened view, the values in this input row are replicated to match the number of rows produced by FLATTEN.

Highlight:The columns of the original (correlated) table which was used as the source of data for FLATTEN are also accessible. If a single row from the original table resulted in multiple rows in the flattened view, the values in this input row are replicated to match the number of rows produced by FLATTEN.

Highlight:Output¶

The returned rows consist of a fixed set of columns:

+-----+------+------+-------+-------+------+
| SEQ |  KEY | PATH | INDEX | VALUE | THIS |
|-----+------+------+-------+-------+------|

SEQ:

A unique sequence number associated with the input record; the sequence is not guaranteed to be gap-free or ordered in any particular way.

KEY:

For maps or objects, this column contains the key to the exploded value.

PATH:

The path to the element within a data structure which needs to be flattened.

INDEX:

The index of the element, if it is an array; otherwise NULL.

VALUE:

The value of the element of the flattened array/object.

THIS:

The element being flattened (useful in recursive flattening).

Highlight:Output¶

Highlight:The returned rows consist of a fixed set of columns:

Highlight:SEQ:

A unique sequence number associated with the input record; the sequence is not guaranteed to be gap-free or ordered in any particular way.

KEY:

For maps or objects, this column contains the key to the exploded value.

PATH:

The path to the element within a data structure which needs to be flattened.

INDEX:

The index of the element, if it is an array; otherwise NULL.

VALUE:

The value of the element of the flattened array/object.

THIS:

The element being flattened (useful in recursive flattening).",false
834318984,REMOVE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/remove,,2024-08-13T12:37:12.884Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fremove,"Highlight:REMOVE¶

Removes files from either an external (external cloud storage) or internal (i.e. Snowflake) stage.

For internal stages, the following stage types are supported:

Named internal stage

Stage for a specified table

Stage for the current user

REMOVE can be abbreviated to RM.",false
834318717,Manage reader accounts | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-sharing-reader-create,,2024-08-13T12:35:18.144Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-sharing-reader-create,"Highlight:A reader account is intended primarily for querying data shared by the provider of the account. You can work with data, for example, by creating materialized views.

You cannot perform the following tasks in a reader account:

Set a data metric function on objects in the reader account.

Upload new data.

Modify existing data.

Unload data using a storage integration. However, you can use the COPY INTO <location> command with your connection credentials to unload data into a cloud storage location.",false
834318554,SHOW MANAGED ACCOUNTS | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/show-managed-accounts,,2024-08-13T12:34:05.799Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fshow-managed-accounts,"Highlight:SHOW MANAGED ACCOUNTS

Highlight:Lists the managed accounts created for your account. Currently used by data providers to create reader accounts for their consumers.",false
834317745,"LOGIN_HISTORY , LOGIN_HISTORY_BY_USER | Snowflake Documentation",,,https://docs.snowflake.com/en/sql-reference/functions/login_history,,2024-08-13T12:30:18.818Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Flogin_history,"Highlight:LOGIN_HISTORY , LOGIN_HISTORY_BY_USER¶

The LOGIN_HISTORY family of table functions can be used to query login attempts by Snowflake users along various dimensions:

LOGIN_HISTORY returns login events within a specified time range.

LOGIN_HISTORY_BY_USER returns login events of a specified user within a specified time range.

Highlight:These functions return login activity within the last 7 days.",false
834316113,SHOW FILE FORMATS | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/show-file-formats,,2024-08-13T12:26:52.653Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fshow-file-formats,"Highlight:SHOW FILE FORMATS¶

Lists the file formats for which you have access privileges. This command can be used to list the file formats for a specified database or schema (or the current database/schema for the session), or your entire account.

See also:",false
834315571,Access control considerations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-access-control-considerations#using-the-accountadmin-role,,2024-08-13T12:26:18.739Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-access-control-considerations%23using-the-accountadmin-role,"Highlight:By default, when your account is provisioned, the first user is assigned the ACCOUNTADMIN role. This user should then create one or more additional users who are assigned the USERADMIN role. All remaining users should be created by the user(s) with the USERADMIN role or another role that is granted the global CREATE USER privilege.",false
834315077,Managing Snowpipe | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-snowpipe-manage#label-snowpipe-management-recreate-pipes,,2024-08-13T12:25:47.348Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-snowpipe-manage%23label-snowpipe-management-recreate-pipes,"Highlight:When a pipe is recreated, the load history is dropped. In general, this condition only affects users if they subsequently execute an ALTER PIPE … REFRESH statement on the pipe. Doing so could load duplicate data from staged files in the storage location for the pipe if the data was already loaded successfully and the files were not deleted subsequently.",false
834312114,LOGIN_HISTORY view | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/account-usage/login_history,,2024-08-13T12:23:44.838Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Faccount-usage%2Flogin_history,Highlight:This Account Usage view can be used to query login attempts by Snowflake users within the last 365 days (1 year).,false
834311904,CREATE STORAGE INTEGRATION | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration,,2024-08-13T12:21:59.226Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcreate-storage-integration,"Highlight:A storage integration is a Snowflake object that stores a generated identity and access management (IAM) entity for your external cloud storage, along with an optional set of allowed or blocked storage locations (Amazon S3, Google Cloud Storage, or Microsoft Azure).",false
834311643,Introduction to Streams | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/streams-intro#types-of-streams,,2024-08-13T12:19:25.221Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fstreams-intro%23types-of-streams,"Highlight:Types of Streams

Highlight:Standard:

Supported for streams on standard tables, dynamic tables, Snowflake-managed Iceberg tables, directory tables, or views.

Highlight:Append-only:

Supported for streams on standard tables, dynamic tables, Snowflake-managed Iceberg tables, or views.

Highlight:Insert-only:

Supported for streams on dynamic tables, Iceberg tables, or external tables.",false
834311539,SYSTEM$GLOBAL_ACCOUNT_SET_PARAMETER | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/system_global_account_set_parameter,,2024-08-13T12:18:26.315Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fsystem_global_account_set_parameter,"Highlight:SYSTEM$GLOBAL_ACCOUNT_SET_PARAMETER¶

Enables replication and failover features for a specified account in an organization.

Once an organization administrator (a user with the ORGADMIN role) has called this function, the following features are enabled for the account:

Replication

Client Redirect

Highlight:Only organization administrators (i.e. users with the ORGADMIN role) can call this SQL function.",false
834309124,CURRENT_WAREHOUSE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/current_warehouse,,2024-08-13T12:13:39.638Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fcurrent_warehouse,"Highlight:CURRENT_WAREHOUSE¶

Returns the name of the warehouse in use for the current session.",false
834308534,SYSTEM$GLOBAL_ACCOUNT_SET_PARAMETER | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/system_global_account_set_parameter#usage-notes,,2024-08-13T12:11:49.726Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fsystem_global_account_set_parameter%23usage-notes,"Highlight:Only organization administrators (i.e. users with the ORGADMIN role) can call this SQL function.

Highlight:SYSTEM$GLOBAL_ACCOUNT_SET_PARAMETER¶

Enables replication and failover features for a specified account in an organization.

Once an organization administrator (a user with the ORGADMIN role) has called this function, the following features are enabled for the account:

Replication

Client Redirect",false
834308375,Renaming an account | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/organizations-manage-accounts-rename,,2024-08-13T12:10:20.928Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Forganizations-manage-accounts-rename,"Highlight:An organization administrator (i.e. a user granted the ORGADMIN role) can rename an account.

Highlight:When an account is renamed, Snowflake creates a new account URL that is used to access the account. During the renaming, the administrator can accept the default to save the original account URL so users can continue to use it, or they can delete the original URL to force users to use the new URL. Saved URLs can be deleted at a later time. You cannot save the original URL for a reader account.",false
834304092,Micro-partitions & Data Clustering | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions,,2024-08-13T12:03:38.038Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Ftables-clustering-micropartitions,"Highlight:In other words, the closer the ratio of scanned micro-partitions and columnar data is to the ratio of actual data selected, the more efficient is the pruning performed on the table.

Highlight:Columns are also compressed individually within micro-partitions. Snowflake automatically determines the most efficient compression algorithm for the columns in each micro-partition.

Highlight:All data in Snowflake tables is automatically divided into micro-partitions, which are contiguous units of storage. Each micro-partition contains between 50 MB and 500 MB of uncompressed data (note that the actual size in Snowflake is smaller because data is always stored compressed). Groups of rows in tables are mapped into individual micro-partitions, organized in a columnar fashion.",false
834302674,Working with Materialized Views | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/views-materialized#deciding-when-to-create-a-materialized-view,,2024-08-13T11:59:18.384Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fviews-materialized%23deciding-when-to-create-a-materialized-view,"Highlight:Materialized views are particularly useful when:

Query results contain a small number of rows and/or columns relative to the base table (the table on which the view is defined).

Query results contain results that require significant processing, including:

Analysis of semi-structured data.

Aggregates that take a long time to calculate.

The query is on an external table, which might have slower performance compared to querying native database tables or Iceberg tables.

The view’s base table does not change frequently.",false
834302483,Semi-structured data types | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/data-types-semistructured,,2024-08-13T11:57:48.731Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fdata-types-semistructured,"Highlight:VARIANT (can contain a value of any other data type).

OBJECT (can directly contain a VARIANT value, and thus indirectly contain a value of any other data type, including itself).

ARRAY (can directly contain a VARIANT value, and thus indirectly contain a value of any other data type, including itself).

Highlight:A VARIANT value can have a maximum size of up to 16 MB of uncompressed data.",false
834213733,REVOKE ROLE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/revoke-role,,2024-08-13T09:03:29.191Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Frevoke-role,"Highlight:REVOKE ROLE¶

Removes a role from another role or a user.",false
834213494,SHOW GRANTS | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/show-grants,,2024-08-13T09:01:41.082Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fshow-grants,"Highlight:SHOW
 GRANTS

Syntactically equivalent to SHOW
 GRANTS
 TO
 USER
 current_user
. Lists all the roles granted to the current user.

Highlight:ROLE
 role_name

Lists all privileges and roles granted to the role. If the role has a grant on a temporary object, then the grant only exists in the session that the temporary object was created.

Highlight:SHOW GRANTS OF...

Highlight:ROLE role_name

Lists all users and roles to which the role has been granted.

Highlight:SHOW GRANTS TO ...",false
834213404,Snowflake Editions | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/intro-editions,,2024-08-13T09:01:08.073Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fintro-editions,"Highlight:Row-level Security
 to apply row access policies to determine which rows are visible in a query result.
		

✔

	

✔

	

✔

Highlight:Extended Time Travel (up to 90 days).

Highlight:Dedicated metadata store and pool of compute resources (used in virtual warehouses).

Highlight:Network policies for limiting/controlling site access by user IP address.

Highlight:Column-level Security to apply masking policies to columns in tables or views.

Highlight:Customer-managed encryption keys through Tri-Secret Secure.

Highlight:Support for private connectivity to Snowflake internal stages using AWS PrivateLink and Azure Private Link.

Highlight:Support for private connectivity to the Snowflake service using AWS PrivateLink, Azure Private Link, or Google Cloud Private Service Connect.

Highlight:Multi-cluster virtual warehouses for scaling compute resources to meet concurrency needs.

Highlight:Audit the user access history through the Account Usage ACCESS_HISTORY view.

Highlight:By default, VPS does not permit data sharing outside of the VPS.

Highlight:Snowflake Marketplace and Listings, where providers and consumers meet to share data securely.

Highlight:Failover and failback between Snowflake accounts for business continuity and disaster recovery.

Highlight:Database and share replication between Snowflake accounts (within an organization) to keep database and share objects and stored data synchronized.

Highlight:Periodic rekeying of encrypted data for increased protection.

Highlight:Support for classifying potentially sensitive data using classification.

Highlight:Object Tagging to apply tags to Snowflake objects to facilitate tracking sensitive data and resource usage.

Highlight:Query acceleration for parallel processing portions of eligible queries.

		

✔

	

✔

	

✔




Search optimization for point lookup queries, with automatic maintenance.

Highlight:Materialized views, with automatic maintenance of results.

Highlight:Snowflake Connector for Kafka for loading data from Apache Kafka topics.

Highlight:Snowpipe Streaming for low-latency loading of streaming data.

Highlight:Snowpipe for continuous micro-batch loading.

Highlight:Bulk unloading to delimited flat files and JSON files.

Highlight:Bulk loading from delimited flat files (CSV, TSV, etc.) and semi-structured data files (JSON, Avro, ORC, Parquet, and XML).

Highlight:As a provider, create and share a Snowflake Data Clean Room.

Highlight:As a consumer, install and use a Snowflake Data Clean Room.",false
834213289,PIPE_USAGE_HISTORY | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/functions/pipe_usage_history,,2024-08-13T09:00:29.973Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ffunctions%2Fpipe_usage_history,"Highlight:This function returns pipe activity within the last 14 days.

Highlight:PIPE_USAGE_HISTORY",false
834213241,Overview of data unloading | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-unload-overview,,2024-08-13T09:00:13.570Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-unload-overview,"Highlight:Partitioned data unloading¶

The COPY INTO <location>
 command includes a PARTITION BY copy option for partitioned unloading of data to stages.

The ability to partition data during the unload operation enables a variety of use cases, such as using Snowflake to transform data for output to a data lake. In addition, partitioning unloaded data into a directory structure in cloud storage can increase the efficiency with which third-party tools consume the data.

The PARTITION BY copy option accepts an expression by which the unload operation partitions table rows into separate files unloaded to the specified stage.",false
834213102,Understanding Encryption Key Management in Snowflake | Snowflake Documentation,,,https://docs.snowflake.com/user-guide/security-encryption-manage,,2024-08-13T08:59:32.919Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fuser-guide%2Fsecurity-encryption-manage,Highlight:All Snowflake customer data is encrypted by default using the latest security standards and best practices. Snowflake uses strong AES 256-bit encryption with a hierarchical key model rooted in a hardware security module.,false
834213012,LIST | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/list,,2024-08-13T08:58:54.534Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Flist,"Highlight:Returns a list of files that have been staged

Highlight:Use the abbreviated form of the command to list all the files in the stage for the current user:

LS
 @~;",false
834212984,Preparing to load data | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-prepare,,2024-08-13T08:58:38.079Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-prepare,"Highlight:Structured/Semi-structured

	

Type

	

Notes




Structured

	

Delimited (CSV, TSV, etc.)

	

Any valid singlebyte delimiter is supported; default is comma (i.e. CSV).




Semi-structured

	

JSON

	
	

Avro

	

Includes automatic detection and processing of compressed Avro files.


	

ORC

	

Includes automatic detection and processing of compressed ORC files.


	

Parquet

	

Includes automatic detection and processing of compressed Parquet files. .
 .
 Currently, Snowflake supports the schema of Parquet files produced using the Parquet writer v1. Files produced using v2 of the writer are not supported.


	

XML

	

Supported as a preview
 feature.",false
834212778,Transactions | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/transactions,,2024-08-13T08:56:40.559Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Ftransactions,"Highlight:A transaction is a sequence of SQL statements that are committed or rolled back as a unit.

Introduction

Highlight:Aborting transactions¶

If a transaction is running in a session and the session disconnects abruptly, preventing the transaction from committing or rolling back, the transaction is left in a detached state, including any locks that the transaction is holding on resources. If this happens, you might need to abort the transaction.

To abort a running transaction, the user who started the transaction or an account administrator can call the system function, SYSTEM$ABORT_TRANSACTION.

If the transaction is not aborted by the user:

And it blocks another transaction from acquiring a lock on the same table and is idle for 5 minutes, it is automatically aborted and rolled back.

And it does not block other transactions from modifying the same table and is older than 4 hours, it is automatically aborted and rolled back.

And it reads from or writes to hybrid tables, and is idle for 5 minutes, it is automatically aborted and rolled back, regardless of whether it blocks other transactions from modifying the same table.

Highlight:A transaction can be ended explicitly by executing COMMIT or ROLLBACK. Snowflake supports the synonym COMMIT WORK for COMMIT, and the synonym ROLLBACK WORK for ROLLBACK.

Highlight:Transactions are never nested.

Highlight:A transaction is associated with a single session.

Highlight:A transaction can be started explicitly by executing a BEGIN statement.",false
834212678,Configuring Snowflake to use federated authentication | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/admin-security-fed-auth-security-integration,,2024-08-13T08:55:57.613Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fadmin-security-fed-auth-security-integration,"Highlight:Snowflake uses a SAML2 security integration to integrate with the IdP you are using to implement federated authentication. Use the CREATE SECURITY INTEGRATION
 command to start configuring Snowflake for SSO.",false
834212544,Using Persisted Query Results | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/querying-persisted-results,,2024-08-13T08:55:30.750Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquerying-persisted-results,"Highlight:For persisted query results of all sizes, the cache expires after 24 hours.

Highlight:Each time the persisted result for a query is reused, Snowflake resets the 24-hour retention period for the result, up to a maximum of 31 days from the date and time that the query was first executed. After 31 days, the result is purged and the next time the query is submitted, a new result is generated and persisted.

Highlight:By default, result reuse is enabled, but can be overridden at the account, user, and session level using the USE_CACHED_RESULT session parameter.",false
834212444,Using SnowSQL | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/snowsql-use,,2024-08-13T08:55:10.643Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsnowsql-use,"Highlight:Exporting data¶

Output query results to a file in a defined format using the following configuration options
:

output_format=
output_format

output_file=
output_filename

Highlight:To execute a SQL script while connecting to Snowflake, use the -f <input_filename> connection parameter.

An output file for the script can be specified using -o output_file=<output_filename>. In addition, you can use -o quiet=true to turn off the standard output and -o friendly=false to turn off the startup and exit messages.",false
834212278,Snowpark API | Snowflake Documentation,,,https://docs.snowflake.com/en/developer-guide/snowpark/index,,2024-08-13T08:54:45.958Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fdeveloper-guide%2Fsnowpark%2Findex,"Highlight:In comparison to using the Snowflake Connector for Spark
, developing with Snowpark includes the following benefits:

Support for interacting with data within Snowflake using libraries and patterns purpose built for different languages without compromising on performance or functionality.

Support for authoring Snowpark code using local tools such as Jupyter, VS Code, or IntelliJ.

Support for pushdown for all operations, including Snowflake UDFs. This means Snowpark pushes down all data transformation and heavy lifting to the Snowflake data cloud, enabling you to efficiently work with data of any size.

No requirement for a separate cluster outside of Snowflake for computations. All of the computations are done within Snowflake. Scale and compute management are handled by Snowflake.

Highlight:Snowflake currently provides Snowpark libraries for three languages: Java, Python, and Scala.",false
834211967,Key Concepts & Architecture | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/intro-key-concepts,,2024-08-13T08:54:09.606Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fintro-key-concepts,"Highlight:When data is loaded into Snowflake, Snowflake reorganizes that data into its internal optimized, compressed, columnar format.

Highlight:Services managed in this layer include:

Authentication

Infrastructure management

Metadata management

Query parsing and optimization

Access control

Highlight:Snowflake’s unique architecture consists of three key layers:

Database Storage

Query Processing

Cloud Services

Highlight:Each virtual warehouse is an MPP compute cluster composed of multiple compute nodes allocated by Snowflake from a cloud provider.

Each virtual warehouse is an independent compute cluster that does not share compute resources with other virtual warehouses. As a result, each virtual warehouse has no impact on the performance of other virtual warehouses.",false
834211906,Unloading into Amazon S3 | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-unload-s3,,2024-08-13T08:53:31.381Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-unload-s3,"Highlight:Snowflake requires the following permissions on an S3 bucket and folder to create new files in the folder (and any sub-folders):

s3:DeleteObject

s3:PutObject",false
834211808,PUT | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/put,,2024-08-13T08:52:29.551Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fput,"Highlight:The command cannot be executed from the Worksheets
page in either Snowflake web interface; instead, use the SnowSQL client
 or Drivers
 to upload data files, or check the documentation for a specific Snowflake client to verify support for this command.

Highlight:OVERWRITE = TRUE | FALSE

Specifies whether Snowflake overwrites an existing file with the same name during upload:

TRUE: An existing file with the same name is overwritten.

FALSE: An existing file with the same name is not overwritten.

Highlight:All files stored on internal stages for data loading and unloading operations are automatically encrypted using AES-256 strong encryption on the server side. By default, Snowflake provides additional client-side encryption with a 128-bit key (with the option to configure a 256-bit key).",false
834211741,Understanding compute cost | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/cost-understanding-compute,,2024-08-13T08:51:58.603Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fcost-understanding-compute,"Highlight:The credit numbers shown above are for a full hour of usage; however, credits are billed per-second, with a 60-second (i.e. 1-minute) minimum:",false
834209350,Analyzing Queries Using Query Profile | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/ui-query-profile,,2024-08-13T08:50:18.833Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fui-query-profile,"Highlight:A collapsible panel in the operator tree pane lists nodes by execution time in descending order, enabling users to quickly locate the costliest operator nodes in terms of execution time. The panel lists all nodes that lasted for 1% or longer of the total execution time of the query (or the execution time for the displayed query step, if the query was executed in multiple processing steps).

Highlight:Metadata Operators¶

Some queries include steps that are pure metadata/catalog operations rather than data-processing operations. These steps consist of a single operator. Some examples include:

DDL and Transaction Commands:

Used for creating or modifying objects, session, transactions, etc. Typically, these queries are not processed by a virtual warehouse and result in a single-step profile that corresponds to the matching SQL statement. For example:

CREATE DATABASE | SCHEMA | …

ALTER DATABASE | SCHEMA | TABLE | SESSION | …

DROP DATABASE | SCHEMA | TABLE | …

COMMIT

Table Creation Command:

DDL command for creating a table. For example:

CREATE TABLE

Similar to other DDL commands, these queries result in a single-step profile; however, they can also be part of a multi-step profile, such as when used in a CTAS statement. For example:

CREATE TABLE … AS SELECT …

Query Result Reuse:

A query that reuses the result of a previous query.

Metadata-based Result:

A query whose result is computed based purely on metadata, without accessing any data. These queries are not processed by a virtual warehouse. For example:

SELECT COUNT(*) FROM …

SELECT CURRENT_DATABASE()

Highlight:Execution Time¶

Execution time provides information about “where the time was spent” during the processing of a query. Time spent can be broken down into the following categories, displayed in the following order:

Processing
 — time spent on data processing by the CPU.

Local Disk IO
 — time when the processing was blocked by local disk access.

Remote Disk IO
 — time when the processing was blocked by remote disk access.

Network Communication
 — time when the processing was waiting for the network data transfer.

Synchronization
 — various synchronization activities between participating processes.

Initialization
 — time spent setting up the query processing.

Highlight:Total invocations
 — number of times that an external function was called. (This can be different from the number of external function calls in the text of the SQL statement due to the number of batches that rows are divided into, the number of retries (if there are transient network problems), etc.)

Highlight:For some operations (e.g. duplicate elimination for a huge data set), the amount of memory available for the compute resources used to execute the operation might not be sufficient to hold intermediate results. As a result, the query processing engine will start spilling
 the data to local disk. If the local disk space is not sufficient, the spilled data is then saved to remote disks.

This spilling can have a profound effect on query performance (especially if remote disk is used for spilling). To alleviate this, we recommend:

Using a larger warehouse (effectively increasing the available memory/local disk space for the operation), and/or

Processing data in smaller batches.

Highlight:Snowflake collects rich statistics on data allowing it not to read unnecessary parts of a table based on the query filters. However, for this to have an effect, the data storage order needs to be correlated with the query filter attributes.

The efficiency of pruning can be observed by comparing Partitions scanned
 and Partitions total
 statistics in the TableScan
 operators. If the former is a small fraction of the latter, pruning is efficient. If not, the pruning did not have an effect.

Highlight:Query Profile, available through the Classic Console, provides execution details for a query. For the selected query, it provides a graphical representation of the main components of the processing plan for the query, with statistics for each component, along with details and statistics for the overall query.

Highlight:Common Query Problems Identified by Query Profile¶

Highlight:“Exploding” Joins

Highlight:UNION Without ALL¶

Highlight:Queries Too Large to Fit in Memory

Highlight:Inefficient Pruning¶

Highlight:Spilling — information about disk usage for operations where intermediate results do not fit in memory:

Bytes spilled to local storage — volume of data spilled to local disk.

Bytes spilled to remote storage — volume of data spilled to remote disk.

Highlight:TableScan:

Represents access to a single table.",false
834208905,UNDROP object | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/undrop,,2024-08-13T08:49:46.213Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fundrop,"Highlight:Organization Objects:

UNDROP ACCOUNT

Account Objects:

UNDROP DATABASE

Database Objects:

UNDROP DYNAMIC TABLE

UNDROP EXTERNAL VOLUME

UNDROP ICEBERG TABLE

UNDROP SCHEMA

UNDROP TABLE

UNDROP TAG",false
834208816,Virtual warehouses | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/warehouses,,2024-08-13T08:48:52.373Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fwarehouses,"Highlight:A virtual warehouse, often referred to simply as a “warehouse”, is a cluster of compute resources in Snowflake. A virtual warehouse is available in two types:

Standard

Snowpark-optimized

Highlight:A warehouse provides the required resources, such as CPU, memory, and temporary storage, to perform the following operations in a Snowflake session:

Executing SQL SELECT
 statements that require compute resources (e.g. retrieving rows from tables and views).

Performing DML operations, such as:

Updating rows in tables (DELETE
 , INSERT
 , UPDATE
).

Loading data into tables (COPY INTO <table>
).

Unloading data from tables (COPY INTO <location>
).",false
834208611,Overview of warehouses | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/warehouses-overview,,2024-08-13T08:46:59.722Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fwarehouses-overview,"Highlight:Warehouses can be started and stopped at any time. They can also be resized at any time, even while running, to accommodate the need for more or less compute resources, based on the type of operations being performed by the warehouse.

Highlight:Default size for warehouses created in Snowsight and using CREATE WAREHOUSE
.

Highlight:X-Small

Highlight:X-Large

Highlight:Default size for warehouses created using the Classic Console.

Highlight:The additional resources do not impact any queries that are already running, but once they are fully provisioned they become available for use by any queries that are queued or newly submitted.

Highlight:SnowSQL supports both a configuration file and command line option for specifying a default warehouse.

The drivers and connectors support specifying a default warehouse as a connection parameter when initiating a session.

Highlight:Auto-suspension and auto-resumption¶

A warehouse can be set to automatically resume or suspend, based on activity:

By default, auto-suspend is enabled. Snowflake automatically suspends the warehouse if it is inactive for the specified period of time.

By default, auto-resume is enabled. Snowflake automatically resumes the warehouse when any statement that requires a warehouse is submitted and the warehouse is the current warehouse for the session.

Highlight:Auto-suspend and auto-resume apply only to the entire warehouse and not to the individual clusters in the warehouse.

Highlight:Snowflake provides some object-level parameters that can be set to help control query processing and concurrency:

STATEMENT_QUEUED_TIMEOUT_IN_SECONDS

STATEMENT_TIMEOUT_IN_SECONDS",false
834208440,Multi-cluster warehouses | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/warehouses-multicluster,,2024-08-13T08:45:31.746Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fwarehouses-multicluster,"Highlight:Auto-scale:

This mode is enabled by specifying different
 values for maximum and minimum number of clusters.

Highlight:You can choose to run a multi-cluster warehouse in either of the following modes:

Maximized:

This mode is enabled by specifying the same
 value for both maximum and minimum number of clusters (note that the specified value must be larger than 1). In this mode, when the warehouse is started, Snowflake starts all the clusters so that maximum resources are available while the warehouse is running.

Highlight:Snowflake supports the following scaling policies:

Highlight:Standard (default)

Highlight:After 2 to 3 consecutive successful checks (performed at 1 minute intervals), which determine whether the load on the least-loaded cluster could be redistributed to the other clusters without spinning up the cluster again.

Highlight:Economy

Highlight:After 5 to 6 consecutive successful checks (performed at 1 minute intervals), which determine whether the load on the least-loaded cluster could be redistributed to the other clusters without spinning up the cluster again.

Highlight:Multi-cluster warehouses are best utilized for scaling resources to improve concurrency for users/queries. They are not as beneficial for improving the performance of slow-running queries or data loading. For these types of operations, resizing the warehouse provides more benefits.

Highlight:Multi-cluster warehouses enable you to scale compute resources to manage your user and query concurrency needs as they change, such as during peak and off hours.

Highlight:Only if the system estimates there’s enough query load to keep the cluster busy for at least 6 minutes.

Highlight:The first cluster starts immediately when either a query is queued or the system detects that there’s one more query than the currently-running clusters can execute.

Each successive cluster waits to start 20 seconds after the prior one has started. For example, if your warehouse is configured with 10 max clusters, it can take a full 200+ seconds to start all 10 clusters.",false
834208305,Warehouse considerations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/warehouses-considerations,,2024-08-13T08:44:18.322Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fwarehouses-considerations,"Highlight:The overall size of the tables being queried has more impact than the number of rows.

Highlight:the larger the warehouse and, therefore, more compute resources in the warehouse, the larger the cache).

Highlight:This cache is dropped when the warehouse is suspended, which may result in slower initial performance for some queries after the warehouse is resumed.

Highlight:Decreasing the size of a running warehouse removes compute resources from the warehouse. When the computer resources are removed, the cache associated with those resources is dropped, which can impact performance in the same way that suspending the warehouse can impact performance after it is resumed.",false
834208248,Working with warehouses | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/warehouses-tasks,,2024-08-13T08:43:40.676Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fwarehouses-tasks,"Highlight:Credits are billed on a per-second basis while the warehouse is running, with a 1-minute minimum each time the warehouse is resumed; however, credit consumption is reported in 60-minute (i.e. hourly) increments.

Highlight:Compute resources waiting to shut down are considered to be in “quiesce” mode.

Highlight:To verify the additional compute resources for your warehouse have been fully provisioned, add the WAIT_FOR_COMPLETION
 parameter to the ALTER WAREHOUSE
 command. You can also use SHOW WAREHOUSES
 to check its state
.

Highlight:Compute resources are removed from a warehouse only when they are no longer being used to execute any current statements.

Highlight:Resizing a warehouse doesn’t have any impact on statements that are currently being executed by the warehouse. When resizing to a larger size, the new compute resources, once fully provisioned, are used only to execute statements that are already in the warehouse queue, as well as all future statements submitted to the warehouse.

Highlight:Resizing a suspended warehouse does not provision any new compute resources for the warehouse. It simply instructs Snowflake to provision the additional compute resources when the warehouse is next resumed, at which time all the usage and credit rules associated with starting a warehouse apply.

Highlight:Snowflake does not begin executing SQL statements submitted to a warehouse until all of the compute resources for the warehouse are successfully provisioned, unless any of the resources fail to provision

Highlight:If any of the compute resources for the warehouse fail to provision during start-up, Snowflake attempts to repair the failed resources.

During the repair process, the warehouse starts processing SQL statements once 50% or more of the requested compute resources are successfully provisioned.",false
834208108,Monitoring warehouse load | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/warehouses-load-monitoring,,2024-08-13T08:42:16.012Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fwarehouses-load-monitoring,"Highlight:To view the load monitoring chart, you must be using a role that has the MONITOR privilege on the warehouse.

Highlight:Query load is calculated by dividing the execution time (in seconds) of all queries in an interval by the total time (in seconds) for the interval.",false
834208081,Snowpark-optimized warehouses | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/warehouses-snowpark-optimized,,2024-08-13T08:42:00.874Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fwarehouses-snowpark-optimized,"Highlight:Changing the warehouse type using the ALTER WAREHOUSE command is only supported for a warehouse in the SUSPENDED
 state. T",false
834208030,Limiting concurrently running queries | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/performance-query-warehouse-max-concurrency,,2024-08-13T08:41:44.085Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fperformance-query-warehouse-max-concurrency,"Highlight:You can use the MAX_CONCURRENCY_LEVEL
 parameter to limit the number of concurrent queries running in a warehouse.",false
834207730,CREATE WAREHOUSE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/create-warehouse,,2024-08-13T08:40:33.634Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fcreate-warehouse,"Highlight:AUTO_SUSPEND
 =
 {
 <num>
 |
 NULL
 }
 AUTO_RESUME
 =
 {
 TRUE
 |
 FALSE
 }
 INITIALLY_SUSPENDED
 =
 {
 TRUE
 |
 FALSE
 }

Highlight:Creating a virtual warehouse automatically sets it as the warehouse in use for the current session (equivalent to using the USE WAREHOUSE
 command for the warehouse).

Highlight:Snowpark-optimized warehouses are not supported on XSMALL
, SMALL
, X5LARGE
, or X6LARGE
 warehouse sizes.",false
834207657,ALTER WAREHOUSE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/alter-warehouse,,2024-08-13T08:40:03.373Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Falter-warehouse,"Highlight:The granting of the global MANAGE WAREHOUSES privilege is equivalent to granting the MODIFY, MONITOR, and OPERATE privileges on all warehouses in an account.

Highlight:When resizing a warehouse, you can use this parameter to block the return of the ALTER WAREHOUSE command until the resize has finished provisioning all its compute resources. Blocking the return of the command when resizing to a larger warehouse serves to notify you that your compute resources have been fully provisioned and the warehouse is now ready to execute queries using all the new resources.",false
834207607,DROP WAREHOUSE | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/drop-warehouse,,2024-08-13T08:39:35.678Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fdrop-warehouse,"Highlight:Dropped warehouses cannot be recovered; they must be recreated.

Highlight:To prevent in-progress queries from being aborted for a dropped warehouse (i.e. you wish the queries to be completed):

First suspend the warehouse.

After all the queries have completed, drop the warehouse.",false
834207512,Introduction to unstructured data | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/unstructured-intro,,2024-08-13T08:39:01.091Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Funstructured-intro,"Highlight:The following types of URLs are available to access files in cloud storage:

Scoped URL:

Encoded URL that permits temporary access to a staged file without granting privileges to the stage.

The URL expires when the persisted query result period
 ends (i.e. the results cache expires), which is currently 24 hours.

File URL:

URL that identifies the database, schema, stage, and file path to a set of files. A role that has sufficient privileges on the stage can access the files.

Pre-signed URL:

Simple HTTPS URL used to access a file via a web browser. A file is temporarily accessible to users via this URL using a pre-signed access token. The expiration time for the access token is configurable.

Highlight:Used to download or access files without authenticating into Snowflake or passing an authorization token.

Highlight:Note

You cannot change the encryption type for an internal stage after you create the stage.

Highlight:Permanent URL to a file on a stage. To download or access a file, users send the file URL in a GET request to the REST API endpoint along with the authorization token. Ideal for custom applications that require access to unstructured data file",false
834207225,Introduction to external tables | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/tables-external-intro,,2024-08-13T08:37:03.448Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Ftables-external-intro,"Highlight:External tables are read-only. You cannot perform data manipulation language (DML) operations on them. However, you can use external tables for query and join operations. You can also create views against external tables.

Highlight:If Snowflake encounters an error while scanning a file in cloud storage during a query operation, the file is skipped and scanning continues on the next file. A query can partially scan a file and return the rows scanned before the error was encountered.

Highlight:All external tables include the following columns:

VALUE:

A VARIANT type column that represents a single row in the external file.

METADATA$FILENAME:

A pseudocolumn that identifies the name of each staged data file included in the external table, including its path in the stage.

METADATA$FILE_ROW_NUMBER:

A pseudocolumn that shows the row number for each record in a staged data file.

Highlight:Parquet files

	

256 - 512 MB

Highlight:For optimal performance when querying large data files, create and query materialized views over external tables
.

Highlight:Parquet row groups

	

16 - 256 MB

	

Note that when Parquet files include multiple row groups, Snowflake can operate on each row group in a different server. For improved query performance, we recommend sizing Parquet files in the recommended range; or, if large file sizes are necessary, including multiple row groups in each file.




All other supported file formats

Highlight:Partition columns are defined when an external table is created, using the CREATE EXTERNAL TABLE … PARTITION BY syntax. After an external table is created, the method by which partitions are added cannot be changed.

Highlight:To optimize the number of parallel scanning operations when querying external tables, we recommend the following file or row group sizes per format:",false
834207191,Search Optimization Service | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/search-optimization-service,,2024-08-13T08:36:44.259Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsearch-optimization-service,"Highlight:The search optimization service aims to significantly improve the performance of certain types of queries on tables, including:

Selective point lookup queries on tables. A point lookup query returns only one or a small number of distinct rows. Use case examples include:

Business users who need fast response times for critical dashboards with highly selective filters.

Data scientists who are exploring large data volumes and looking for specific subsets of data.

Data applications retrieving a small set of results based on an extensive set of filtering predicates.

For more information, see Speeding up point lookup queries with search optimization
.

Substring and regular expression searches (e.g. [ NOT ] LIKE
, [ NOT ] ILIKE
, [ NOT ] RLIKE
, etc.). For more information, see Speeding up substring and regular expression queries with search optimization
.

Queries on fields in VARIANT, OBJECT, and ARRAY
 (semi-structured) columns that use the following types of predicates:

Equality predicates.

IN predicates.

Predicates that use ARRAY_CONTAINS
.

Predicates that use ARRAYS_OVERLAP
.

Substring and regular expression predicates.

Predicates that check for NULL values.

For more information, see Speeding up queries of semi-structured data with search optimization
.

Queries that use selected geospatial functions with GEOGRAPHY
 values. For more information, see Speeding up geospatial queries with search optimization
.

Highlight:To improve performance of search queries, the search optimization service creates and maintains a persistent data structure called a search access path.

Highlight:The search access path keeps track of which values of the table’s columns might be found in each of its micro-partitions, allowing some micro-partitions to be skipped when scanning the table.",false
834207082,Overview of Views | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/views-introduction,,2024-08-13T08:36:12.546Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fviews-introduction,"Highlight:If the schema is not specified, then Snowflake assumes that the table is in the same schema as the view
. (If the table were assumed to be in the active schema, then the view could refer to different tables at different times.)",false
834207048,Working with Secure Views | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/views-secure,,2024-08-13T08:35:50.371Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fviews-secure,"Highlight:When using secure views with Secure Data Sharing
, use the CURRENT_ACCOUNT
 function to authorize users from a specific account to access rows in a base table.

Highlight:Why Should I Use Secure Views?¶

For a non-secure view, internal optimizations can indirectly expose data.

Highlight:Secure views do not utilize these optimizations, ensuring that users have no access to the underlying data.

Highlight:With secure views, the view definition and details are visible only to authorized users (i.e. users who are granted the role that owns the view).",false
834206736,Working with Materialized Views | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/views-materialized,,2024-08-13T08:33:03.959Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fviews-materialized,"Highlight:Materialized views are particularly useful when:

Query results contain a small number of rows and/or columns relative to the base table (the table on which the view is defined).

Query results contain results that require significant processing, including:

Analysis of semi-structured data.

Aggregates that take a long time to calculate.

The query is on an external table
, which might have slower performance compared to querying native database tables or Iceberg tables
.

The view’s base table does not change frequently.

Highlight:If a query is run before the materialized view is up-to-date, Snowflake either updates the materialized view or uses the up-to-date portions of the materialized view and retrieves any required newer data from the base table.

Highlight:Materialized views are faster than tables because of their “cache” (i.e. the query results for the view); in addition, if data has changed, they can use their “cache” for data that hasn’t changed and use the base table for any data that has changed.

Highlight:Performance Benefits

	

Security Benefits

	

Simplifies Query Logic

	

Supports Clustering

	

Uses Storage

	

Uses Credits for Maintenance

	

Notes




Regular table

				

✔

	

✔

		


Regular view

		

✔

	

✔

				


Cached query result

	

✔

						

Used only if data has not changed and if query only uses deterministic functions (e.g. not CURRENT_DATE).




Materialized view

	

✔

	

✔

	

✔

	

✔

	

✔

	

✔

	

Storage and maintenance requirements typically result in increased costs
.




External table

							

Data is maintained outside Snowflake and, therefore, does not incur any storage charges within Snowflake.

Highlight:As with non-materialized views, a materialized view does not automatically inherit the privileges of its base table. You should explicitly grant privileges on the materialized view to the roles that should use that view.

Highlight:The following limitations apply to creating materialized views:

A materialized view can query only a single table.

Joins, including self-joins, are not supported.

A materialized view cannot query:

A materialized view.

A non-materialized view.

A UDTF (user-defined table function).

A materialized view cannot include:

UDFs (this limitation applies to all types of user-defined functions, including external functions).

Window functions.

HAVING clauses.

ORDER BY clause.

LIMIT clause.

GROUP BY keys that are not within the SELECT list. All GROUP BY keys in a materialized view must be part of the SELECT list.

GROUP BY GROUPING SETS.

GROUP BY ROLLUP.

GROUP BY CUBE.

Nesting of subqueries within a materialized view.

The MINUS, EXCEPT, or INTERSECT set operators
.

Many aggregate functions are not allowed in a materialized view definition.

The aggregate functions that are supported
 in materialized views are:

APPROX_COUNT_DISTINCT (HLL)
.

AVG
 (except when used in PIVOT
).

BITAND_AGG
.

BITOR_AGG
.

BITXOR_AGG
.

COUNT
.

COUNT_IF
.

MAX
.

MIN
.

STDDEV, STDDEV_SAMP
.

STDDEV_POP
.

SUM
.

VARIANCE (VARIANCE_SAMP, VAR_SAMP)
.

VARIANCE_POP (VAR_POP)
.

The other aggregate functions are not supported
 in materialized views.

Highlight:Functions used in a materialized view must be deterministic. For example, using CURRENT_TIME
 or CURRENT_TIMESTAMP
 is not permitted.

A materialized view should not be defined using a function that produces different results for different settings of parameters, such as the session-level parameter TIMESTAMP_TYPE_MAPPING.

Highlight:You cannot directly clone a materialized view by using the CREATE
 MATERIALIZED
 VIEW
 ...
 CLONE...
 command. However, if you clone a schema or a database that contains a materialized view, the materialized view will be cloned and included in the new schema or database.

Highlight:If columns are added to the base table, those new columns are not
 propagated to the materialized view automatically.

This is true even if the materialized view was defined with SELECT
 *
 (e.g. CREATE MATERIALIZED VIEW AS SELECT * FROM table2 ...).

Highlight:A materialized view is a pre-computed data set derived from a query specification (the SELECT in the view definition) and stored for later use. Because the data is pre-computed, querying a materialized view is faster than executing a query against the base table of the view.",false
834206613,Table Design Considerations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/table-considerations,,2024-08-13T08:32:31.461Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Ftable-considerations,"Highlight:For better pruning and less storage consumption, Snowflake recommends flattening your object and key data into separate relational columns if your semi-structured data includes:

Dates and timestamps, especially non-ISO 8601 dates and timestamps, as string values

Numbers within strings

Arrays

Highlight:Currently, it is not possible to change a permanent table to a transient
 table using the ALTER TABLE
 command. The TRANSIENT property is set at table creation and cannot be modified.

Similarly, it is not possible to directly change a transient table to a permanent table.",false
834205854,Cloning considerations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/object-clone,,2024-08-13T08:30:30.092Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fobject-clone,"Highlight:If the source object is a database or schema, the clone inherits all
 granted privileges on the clones of all child objects contained in the source object:

For databases, contained objects include schemas, tables, views, etc.

For schemas, contained objects include tables, views, etc.

Note that the clone of the container itself (database or schema) does not inherit the privileges granted on the source container.

Highlight:In a table, a column can reference a sequence that generates default values. When a table is cloned, the cloned table references the source or cloned sequence:

If the database or schema containing both the table and sequence is cloned, the cloned table references the cloned sequence.

Otherwise, the cloned table references the source sequence.

Highlight:By default, Automatic Clustering
 is suspended for the new table. To resume automatic clustering for the new table, run the following command:

Highlight:Individual external named stages can be cloned.

Highlight:Internal (i.e. Snowflake) named stages cannot
 be cloned.

Highlight:When a database or schema is cloned, any pipes in the source container that reference an internal (i.e. Snowflake) stage are not
 cloned.

However, any pipes that reference an external stage are cloned. This includes any pipe objects where the INTEGRATION parameter is set.

Highlight:If a table is fully qualified in the COPY statement in the pipe definition (in the form of db_name
.
schema_name
.
table_name
 or schema_name
.
table_name
), then Snowpipe loads duplicate data into the source table (i.e. the database
.
schema
.
table
 in the COPY statement) for each pipe.

If a table is not
 fully qualified in the pipe definition, then Snowpipe loads the data into the table (e.g. mytable
) in the source and cloned databases/schemas.

Highlight:When a database or schema that contains tasks is cloned, the tasks in the clone are suspended by default.

Highlight:When a database or schema that contains alerts is cloned, the alerts in the clone are suspended
 by default.

Highlight:Cloning an individual policy object is not supported.

Cloning a schema results in the cloning of all policies within the schema.

A cloned table maps to the same policies as the source table. In other words, if a policy is set on the base table or its columns, the policy is attached to the cloned table or its columns.

When a table is cloned in the context of its parent schema cloning, if the source table has a reference to a policy in the same parent schema (i.e. a local reference), the cloned table will have a reference to the cloned policy.

If the source table refers to a policy in a different schema (i.e. a foreign reference), then the cloned table retains the foreign reference.

Highlight:Cloning is fast, but not instantaneous, particularly for large objects (e.g. tables). As such, if DDL statements are executed on source objects (e.g. renaming tables in a schema) while the cloning operation is in progress, the changes may not be represented in the clone. This is because DDL statements are atomic and not part of multi-statement transactions.

Highlight:In the following example, a CREATE TABLE … CLONE statement attempts to clone the source table at a point in the past (30 minutes prior) when it didn’t exist:

Highlight:For example, the data retention period
 for database db1
 is seven days and the data retention period for table t1
 in db1
 is one day. If you clone db1
 using Time Travel at a point 12 hours in the past, the cloning operation successfully creates a clone of db1
 and it contains the cloned table t1
.

However, if you try to clone db1
 at a point two days in the past, the historical data for table t1
 at that point is no longer available in Time Travel and the cloning operation fails.

As a workaround, use the IGNORE TABLES WITH INSUFFICIENT DATA RETENTION
 parameter of the CREATE <object> … CLONE
 command to clone a database or schema. The parameter skips tables that no longer have historical data available in Time Travel at the time specified for the cloning operation.

Highlight:If the source object is a database or schema, the clone inherits all granted privileges on the clones of all child objects contained in the source object:

For databases, contained objects include schemas, tables, views, etc.

For schemas, contained objects include tables, views, etc.",false
834205391,Data storage considerations | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/tables-storage-considerations,,2024-08-13T08:29:01.724Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Ftables-storage-considerations,"Highlight:Of the three methods, TABLE_STORAGE_METRICS
 provides the most detailed information because it includes a breakdown of the physical storage (in bytes) for table data in the following three states of the CDP life-cycle:

Active (ACTIVE_BYTES column)

Time Travel (TIME_TRAVEL_BYTES column)

Fail-safe (FAILSAFE_BYTES column)

Highlight:Every Snowflake table has an ID that uniquely identifies the table. In addition, every table is also associated with a CLONE_GROUP_ID. If a table has no clones, then the ID and CLONE_GROUP_ID are identical. These IDs are displayed in the TABLE_STORAGE_METRICS
 view.

Highlight:Snowflake temporary tables have no Fail-safe and have a Time Travel retention period of only 0 or 1 day; however, the Time Travel period ends when the table is dropped.

Thus, the maximum total CDP charges incurred for a temporary table are 1 day (or less if the table is explicitly dropped or dropped as a result of terminating the session). During this period, Time Travel can be performed on the table.

Highlight:A Snowflake session is only terminated if the user explicitly terminates the session or the session times out due to inactivity after 4 hours. Disconnecting from Snowflake does not terminate the active sessions.

Highlight:High-churn dimension tables can be identified by calculating the ratio of FAILSAFE_BYTES divided by ACTIVE_BYTES in the TABLE_STORAGE_METRICS
 view. Any table with a large ratio is considered to be a high-churn table.

Highlight:TABLE_STORAGE_METRICS view (in the Snowflake Information Schema).

TABLE_STORAGE_METRICS view view (in Account Usage).

Highlight:Any user with the appropriate privileges can view data storage for individual tables. Snowflake provides the following methods for viewing table data storage:",false
834205065,Numeric data types | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/data-types-numeric,,2024-08-13T08:28:11.840Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fdata-types-numeric,"Highlight:NUMBER¶

Numbers up to 38 digits, with an optional precision and scale:

Precision:

Total number of digits allowed.

Scale:

Number of digits allowed to the right of the decimal point.

Highlight:However, scale (number of digits following the decimal point) does have an impact on storage.

Highlight:Precision (total number of digits) does not impact storage.",false
834204840,Overview of data loading | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-overview,,2024-08-13T08:27:39.903Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-overview,"Highlight:A table stage has no grantable privileges of its own. To stage files to a table stage, list the files, query them on the stage, or drop them, you must be the table owner (have the role with the OWNERSHIP privilege on the table).

Highlight:Snowflake supports transforming data while loading it into a table using the COPY command. Options include:

Column reordering

Column omission

Casts

Truncating text strings that exceed the target column length",false
834204340,Preparing your data files | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare,,2024-08-13T08:26:39.120Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-considerations-prepare,"Highlight:Loading very large files (e.g. 100 GB or larger) is not recommended.

If you must load a large file, carefully consider the ON_ERROR
 copy option value. Aborting or skipping a file due to a small number of errors could result in delays and wasted credits. In addition, if a data loading operation continues beyond the maximum allowed duration of 24 hours, it could be aborted without any portion of the file being committed.

Highlight:A VARIANT value can have a maximum size of up to 16 MB of uncompressed data.

Highlight:If the data exceeds 16 MB, enable the STRIP_OUTER_ARRAY file format option for the COPY INTO <table>
 command to remove the outer array structure and load the records into separate table rows:

Highlight:Consider the following guidelines when preparing your delimited text (CSV) files for loading:

UTF-8 is the default character set, however, additional encodings are supported. Use the ENCODING file format option to specify the character set for the data files. For more information, see CREATE FILE FORMAT
.

Fields that contain delimiter characters should be enclosed in quotes (single or double). If the data contains single or double quotes, then those quotes must be escaped.

Carriage returns are commonly introduced on Windows systems in conjunction with a line feed character to mark the end of a line (\r
 \n
). Fields that contain carriage returns should also be enclosed in quotes (single or double).

The number of columns in each row should be consistent.

Highlight:If the element was extracted into a column, the engine scans only the extracted column.

If the element was not
 extracted into a column, the engine must scan the entire JSON structure, and then for each row traverse the structure to output values. This impacts performance.

Highlight:To optimize the number of parallel operations for a load, we recommend aiming to produce data files roughly 100-250 MB (or larger) in size compressed.",false
834204205,Loading data | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-considerations-load,,2024-08-13T08:25:58.490Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-considerations-load,"Highlight:Of the three options for identifying/specifying data files to load from a stage, providing a discrete list of files is generally the fastest; however, the FILES parameter supports a maximum of 1,000 files, meaning a COPY command executed with the FILES parameter can only load up to 1,000 files.

Highlight:Workarounds¶

To load files whose metadata has expired, set the LOAD_UNCERTAIN_FILES copy option to true. The copy option references load metadata, if available, to avoid data duplication, but also attempts to load files with expired load metadata.

Alternatively, set the FORCE option to load all files, ignoring load metadata if it exists. Note that this option reloads files, potentially duplicating data in a table.

Highlight:Load metadata¶

Snowflake maintains detailed metadata for each table into which data is loaded, including:

Name of each file from which data was loaded

File size

ETag for the file

Number of rows parsed in the file

Timestamp of the last load for the file

Information about any errors encountered in the file during loading

This load metadata expires after 64 days. If the LAST_MODIFIED date for a staged data file is less than or equal to 64 days, the COPY command can determine its load status for a given table and prevent reloading (and data duplication). The LAST_MODIFIED date is the timestamp when the file was initially staged or when it was last modified, whichever is later.",false
834204155,Staging files using Snowsight | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-local-file-system-stage-ui,,2024-08-13T08:25:27.046Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-local-file-system-stage-ui,"Highlight:You can’t upload files onto user stages or table stages using Snowsight.

Highlight:The maximum file size is 250 MB.

To upload files onto an internal stage, you must use a role that is granted or inherits the USAGE privilege on the database and schema and the WRITE privilege on the stage. For more information, see Stage privileges
.",false
834204064,Choosing an internal stage for local files | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-local-file-system-create-stage,,2024-08-13T08:24:32.468Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-local-file-system-create-stage,"Highlight:Each user has a Snowflake stage allocated to them by default for storing files.

Highlight:User stages are referenced using @~
; e.g. use LIST
 @~
 to list the files in a user stage.

Highlight:User stages do not support setting file format options. Instead, you must specify file format and copy options as part of the COPY INTO <table>
 command.

Highlight:By default, each table has a Snowflake stage allocated to it for storing files.

Highlight:A table stage has the same name as the table. For example, a table named mytable
 has a stage referenced as @%mytable
.

Highlight:By default, each user and table in Snowflake is automatically allocated an internal stage for staging data files to be loaded. In addition, you can create named internal stages.",false
834204008,Copying data from an internal stage | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-local-file-system-copy,,2024-08-13T08:23:54.994Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-local-file-system-copy,"Highlight:Note that when copying data from files in a table stage, the FROM clause can be omitted because Snowflake automatically checks for files in the table stage.

Highlight:Named stage¶

The following example loads data from all files from the my_stage
 named stage, which was created in Choosing an internal stage for local files
:

COPY
 INTO
 mytable
 from
 @
my_stage
;



Note that a file format does not need to be specified because it is included in the stage definition.

Highlight:To validate data in an uploaded file, execute COPY INTO <table>
 in validation mode using the VALIDATION_MODE parameter. The VALIDATION_MODE parameter returns any errors that it encounters in a file. You can then modify the data in the file to ensure it loads without error.",false
834203655,Snowpipe | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-snowpipe-intro,,2024-08-13T08:20:50.435Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-snowpipe-intro,"Highlight:Snowpipe enables loading data from files as soon as they’re available in a stage.

Highlight:Snowpipe loads the new data files into the target table in a continuous, serverless fashion based on the parameters defined in a specified pipe object.

Highlight:When calling the REST endpoints:
 Requires key pair authentication with JSON Web Token (JWT). JWTs are signed using a public/private key pair with RSA encryption.

Highlight:Snowpipe uses file loading metadata associated with each pipe object to prevent reloading the same files (and duplicating data) in a table. This metadata stores the path (i.e. prefix) and name of each loaded file, and prevents loading files with the same name even if they were later modified (i.e. have a different eTag).

Highlight:Snowpipe:

Uses Snowflake-supplied compute resources.

Highlight:Bulk data load:

Stored in the metadata of the target table for 64 days. Available upon completion of the COPY statement as the statement output.

Snowpipe:

Stored in the metadata of the pipe for 14 days. Must be requested from Snowflake via a REST endpoint, SQL table function, or ACCOUNT_USAGE view.",false
834203573,Snowpipe REST API | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-snowpipe-rest-apis,,2024-08-13T08:19:52.621Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-snowpipe-rest-apis,"Highlight:The Snowpipe API provides REST endpoints for fetching load reports.

Endpoint: insertReport
¶

Highlight:Endpoint: loadHistoryScan¶

Highlight:Endpoint: insertFiles¶",false
834203526,Managing Snowpipe | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-snowpipe-manage,,2024-08-13T08:19:24.502Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-snowpipe-manage,"Highlight:Pipe objects do not support the PURGE copy option.

Highlight:The load history for Snowpipe operations is stored in the metadata of the pipe object. When a pipe is recreated, the load history is dropped.",false
834203485,Directory tables | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-dirtables,,2024-08-13T08:18:52.444Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-dirtables,"Highlight:A directory table is an implicit object layered on a stage (not a separate database object) and is conceptually similar to an external table because it stores file-level metadata about the data files in the stage. A directory table has no grantable privileges of its own.

Both external (external cloud storage) and internal (Snowflake) stages support directory tables. You can add a directory table to a stage when you create a stage (using CREATE STAGE
) or later (using ALTER STAGE
).

Highlight:Table functions¶
AUTO_REFRESH_REGISTRATION_HISTORY

Retrieve the history of data files registered in the metadata of specified objects and the credits billed for these operations.

STAGE_DIRECTORY_FILE_REGISTRATION_HISTORY

Retrieve information about the metadata history for a directory table, including any errors found when refreshing the metadata.",false
834203421,Managing directory tables | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-dirtables-manage,,2024-08-13T08:18:25.740Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-dirtables-manage,"Highlight:Directory tables on internal stages require manual metadata refreshes
. You could also choose to include a directory table on external stages and refresh the metadata manually.",false
834203357,Automated directory table metadata refreshes | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-dirtables-auto,,2024-08-13T08:18:05.918Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-dirtables-auto,"Highlight:Snowflake doesn’t support refreshing the directory table metadata on an internal stage. You must manually refresh the directory table metadata for an internal stage.

Highlight:You can automatically refresh the metadata for a directory table by using the following event notification services:

Amazon S3: Amazon SQS (Simple Queue Service)

Google Cloud Storage: Google Cloud Pub/Sub

Microsoft Azure: Microsoft Azure Event Grid",false
834203278,Transforming data during a load | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-load-transform,,2024-08-13T08:17:31.901Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-load-transform,"Highlight:The VALIDATION_MODE parameter does not support COPY statements that transform data during a load.

Highlight:The COPY command supports:

Column reordering, column omission, and casts using a SELECT statement. There is no requirement for your data files to have the same number and ordering of columns as your target table.

The ENFORCE_LENGTH | TRUNCATECOLUMNS option, which can truncate text strings that exceed the target column length.",false
834203161,Dynamic tables | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/dynamic-tables-intro,,2024-08-13T08:16:37.166Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdynamic-tables-intro,Highlight:Easy switching: Transition seamlessly from batch to streaming with a single ALTER DYNAMIC TABLE command.,false
834203079,How dynamic tables work | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/dynamic-tables-about,,2024-08-13T08:16:02.838Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdynamic-tables-about,"Highlight:Dynamic tables are best used when:

Highlight:You do need to materialize the results of a query of multiple base tables.

Highlight:You don’t need to use unsupported dynamic query constructs such as stored procedures
, non-deterministic functions not
 listed in Supported non-deterministic functions in full refresh
, or external functions
, or need to use sources for dynamic tables that are external tables, streams, or materialized views.

Note

Highlight:Dynamic tables can be used as the source of a stream.",false
834202971,Understanding dynamic table refresh | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/dynamic-tables-refresh,,2024-08-13T08:15:16.094Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdynamic-tables-refresh,"Highlight:The dynamic table refresh process operates in one of two ways:

Incremental refresh:
 This automated process analyzes the dynamic table’s query and calculates changes since the last refresh. It then merges these changes into the table. See Supported queries in incremental refresh
 for details on supported queries.

Full refresh:
 When the automated process can’t perform an incremental refresh, it conducts a full refresh. This involves executing the query for the dynamic table and completely replacing the previous materialized results.

Highlight:Snowflake schedules refreshes to keep the actual lag of your dynamic tables below their target lag. The duration of each refresh depends on the query, data pattern, and warehouse size. When choosing a target lag, consider the time needed to refresh each dynamic table in a chain
 to the root. If you don’t, some refreshes might be skipped, leading to a higher actual lag.

Highlight:Target lag is specified in one of following ways:

Measure of freshness
: Defines the maximum amount of time that the dynamic table’s content should lag behind updates to the base tables.

Highlight:Downstream
: Specifies that the dynamic table should refresh on demand when other dependent dynamic tables refresh. This refresh can be triggered by a manual or scheduled refresh of a downstream dynamic table.",false
834202934,Dynamic tables compared to streams and tasks and to materialized views | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/dynamic-tables-comparison,,2024-08-13T08:14:53.550Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdynamic-tables-comparison,"Highlight:Dynamic tables are designed to build multi-level data pipelines.

Highlight:Materialized views are designed to improve query performance transparently.",false
834202610,Create dynamic tables | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/dynamic-tables-create,,2024-08-13T08:13:27.197Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdynamic-tables-create,"Highlight:Before you begin, ensure you have the privileges for creating dynamic tables
, and all objects used by the dynamic table query have change tracking
 enabled.

Highlight:Snowflake automatically attempts to enable change tracking on them.

Highlight:Snowflake doesn’t automatically attempt to enable change tracking on dynamic tables created with full refresh mode.",false
834202342,About managing dynamic tables | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/dynamic-tables-manage,,2024-08-13T08:11:41.317Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdynamic-tables-manage,"Highlight:Dynamic tables are shareable objects.

Highlight:A dynamic table is suspended if the system observes five continuous refresh errors.",false
834202281,Best practices for dynamic tables | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/dynamic-tables-best-practices,,2024-08-13T08:11:07.329Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdynamic-tables-best-practices,"Highlight:If a grouping key contains a compound expression rather than a base column, materialize the expression in one dynamic table and then apply the grouping operation on the materialized column in another dynamic table.

Highlight:Dynamic tables can be used to implement Type 1 and 2 slowly changing dimensions (SCDs)

Highlight:By default, dynamic table data is retained for 7 days in fail-safe
 storage. For dynamic tables with high refresh throughput, this can significantly increase storage consumption. Therefore, you should make a dynamic table transient only if its data doesn’t need the same level of data protection and recovery provided by permanent tables.",false
834202191,Known limitations for dynamic tables | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/dynamic-tables-limitations,,2024-08-13T08:10:27.164Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdynamic-tables-limitations,"Highlight:You can’t create a temporary dynamic table.

Highlight:You can’t set the DATA_RETENTION_TIME_IN_DAYS
 object parameter in your source tables to zero.

Highlight:The following constructs are not currently supported in the query for a dynamic table. If you specify these in the query, an error occurs:

External functions
.

Sequences
.

Functions that rely on CURRENT_USER
. Dynamic table refreshes act as their owner role with a special SYSTEM user.

Sources that include directory tables, external tables, streams, and materialized views.

Views on dynamic tables or other unsupported objects.

User-defined functions (UDFs and UDTFs) written in SQL and containing a subquery.

UNPIVOT
 constructs are not supported in dynamic table incremental or full refresh.

SAMPLE / TABLESAMPLE
 constructs are not supported in dynamic table incremental or full refresh.

Highlight:Non-deterministic functions are not supported with incremental refreshes, but some non-deterministic functions are supported with full refreshes
.",false
834202160,How refresh mode affects dynamic table performance | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/dynamic-tables-performance-refresh-mode,,2024-08-13T08:10:10.198Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdynamic-tables-performance-refresh-mode,"Highlight:As an extreme example, consider the effect of deleting all of the data from a source: a full refresh just sees an empty table, which can be processed very quickly. In contrast, an incremental refresh has to process every deleted row, making it much slower.",false
834202135,How warehouse configurations affect dynamic table performance | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/dynamic-tables-performance-warehouses,,2024-08-13T08:09:54.659Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdynamic-tables-performance-warehouses,"Highlight:If your dynamic tables refresh incrementally, the initial refresh
 often requires a larger warehouse than subsequent refreshes. Adjust your workflow by starting with a larger warehouse size, creating your dynamic tables, and then sizing down the warehouse again.",false
834202115,Troubleshooting dynamic tables | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/dynamic-tables-troubleshooting,,2024-08-13T08:09:37.668Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdynamic-tables-troubleshooting,"Highlight:My dynamic table is reinitializing.

	

Your dynamic table might be reinitializing due to one of the following reasons:

One or more of the inputs of the dynamic table are replaced. For example, if your dynamic table is defined on a view, and you replace the view, the dynamic table has to reinitialize.

If the schema of the inputs changed and your dynamic table relies on the changed columns.

Data access policies
 are added, removed, or changed on the dynamic table’s inputs.

Cloned incremental dynamic tables
 might need to reinitialize on their first refresh after being created.

Replicated dynamic tables
 with incremental refresh reinitialize after failover before they can resume incremental refresh.",false
834201691,Introduction to Streams | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/streams-intro,,2024-08-13T08:05:28.782Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fstreams-intro,"Highlight:Streams can be created to query change data on the following objects:

Standard tables, including shared tables.

Views, including secure views

Directory tables

Dynamic tables

Iceberg tables
 with Limitations
.

Event tables

External tables

Highlight:The change tracking system utilized by the stream then records information about the DML changes after this snapshot was taken. Change records provide the state of a row before and after the change.

Highlight:When the first stream for a table is created, several hidden columns are added to the source table and begin storing change tracking metadata. These columns consume a small amount of storage.

Highlight:Note that for streams on views, change tracking must be enabled explicitly for the view and underlying tables to add the hidden columns to these tables.

Highlight:Querying a stream alone does not advance its offset, even within an explicit transaction; the stream contents must be consumed in a DML statement.

Highlight:To ensure multiple statements access the same change records in the stream, surround them with an explicit transaction statement (BEGIN
 .. COMMIT
).

Highlight:When queried, a stream accesses and returns the historic data in the same shape as the source object (i.e. the same column names and ordering) with the following additional columns:

METADATA$ACTION:

Indicates the DML operation (INSERT, DELETE) recorded.

METADATA$ISUPDATE:

Indicates whether the operation was part of an UPDATE statement. Updates to rows in the source object are represented as a pair of DELETE and INSERT records in the stream with a metadata column METADATA$ISUPDATE values set to TRUE.

Note that streams record the differences between two offsets. If a row is added and then updated in the current offset, the delta change is a new row. The METADATA$ISUPDATE row records a FALSE value.

METADATA$ROW_ID:

Specifies the unique and immutable ID for the row, which can be used to track changes to specific rows over time.

Highlight:Snowflake provides the following guarantees with respect to METADATA$ROW_ID:

The METADATA$ROW_ID depends on the stream’s source object.

For instance, a stream stream1
 on table table1
 and stream stream2
 on table table1
 produce the same METADATA$ROW_IDs for the same rows, but a stream stream_view
 on view view1
 is not guaranteed to produce the same METADATA$ROW_IDs as stream1
, even if view
 is defined using the statement CREATE
 VIEW
 view
 AS
 SELECT
 *
 FROM
 table1
.

A stream on a source object and a stream on the source object’s clone produce the same METADATA$ROW_IDs for the rows that exist at the time of the cloning.

A stream on a source object and a stream on the source object’s replica produce the same METADATA$ROW_IDs for the rows that were replicated.

Highlight:Standard:

Supported for streams on standard tables, dynamic tables, Snowflake-managed Iceberg tables, directory tables, or views.

Highlight:Types of Streams¶

The following stream types are available based on the metadata recorded by each:

Highlight:Standard streams cannot retrieve change data for geospatial data. We recommend creating append-only streams on objects that contain geospatial data.

Highlight:Append-only:

Supported for streams on standard tables, dynamic tables, Snowflake-managed Iceberg tables, or views.
 An append-only stream exclusively tracks row inserts. Update, delete, and truncate operations are not captured by append-only streams.

Highlight:Insert-only:

Supported for streams on dynamic tables, Iceberg tables, or external tables.

Highlight:A stream becomes stale when its offset falls outside of the data retention period for its source table

Highlight:To continue tracking new change records, you must recreate the stream using the CREATE STREAM
 command.

Highlight:This restriction doesn’t apply to streams on directory tables or external tables, which have no data retention period.

In addition, streams on shared tables or views don’t extend the data retention period for the table or underlying tables, respectively.

Highlight:If the data retention period for a table is less than 14 days and a stream hasn’t been consumed, Snowflake temporarily extends this period to prevent the stream from going stale. The retention period is extended to the stream’s offset, up to a maximum of 14 days by default, regardless of your Snowflake edition
.

Highlight:Recreating an object (using the CREATE OR REPLACE TABLE syntax) drops its history, which also makes any stream on the table or view stale. In addition, recreating or dropping any of the underlying tables for a view makes any stream on the view stale.

Highlight:Renaming a source object does not break a stream or cause it to go stale. In addition, if a source object is dropped and a new object is created with the same name, any streams linked to the original object are not
 linked to the new object.

Highlight:We recommend that users create a separate stream for each consumer of change records for an object. “Consumer” refers to a task, script, or other mechanism that consumes the change records for an object using a DML transaction.

Highlight:Streams on views support both local views and views shared using Snowflake Secure Data Sharing, including secure views. Currently, streams cannot track changes in materialized views.

Highlight:Views with the following operations are not yet supported:

GROUP BY clauses

QUALIFY clauses

Subqueries not in the FROM clause

Correlated subqueries

LIMIT clauses

DISTINCT clauses

Functions:

Functions in the select list must be system-defined, scalar functions.

Highlight:A stream object records data manipulation language (DML) changes made to tables, including inserts, updates, and deletes, as well as metadata about each change, so that actions can be taken using the changed data. This process is referred to as change data capture (CDC).",false
834201541,Introduction to tasks | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/tasks-intro,,2024-08-13T08:03:56.227Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Ftasks-intro,"Highlight:The maximum compute size for a serverless task is equivalent to an XXLARGE user-managed virtual warehouse.

Highlight:To avoid unexpected task executions due to daylight saving time, consider the following:

Don’t schedule tasks to run between 1 AM and 3 AM.

Manually adjust the cron expression for tasks scheduled between 1 AM and 3 AM twice each year to compensate for the time change.

Use a time format that does not apply daylight savings time, such as UTC.

Highlight:Triggered Tasks considerations¶

The following are details about managing, configuring, and monitoring Triggered Tasks:

In the SHOW
 TASKS
 and DESC
 TASK
 output, the SCHEDULE
 property displays NULL
 for Triggered Tasks.

In the output of the task_history view of the information_schema and account_usage schemas, the SCHEDULED_FROM column displays TRIGGER.

If the stream or table that the stream is tracking is dropped or re-created, the triggered task automatically suspends. After the table or stream is re-created, the user can run ALTER
 TASK
 <task_name>
 RESUME
 to resume triggered processing.

Highlight:Triggered Tasks Limitations¶

The following are limitations of Triggered Tasks:

Streams on Data Shares, Directory Tables, External Tables, and Hybrid Tables are not supported.

Serverless Tasks are not supported.

Highlight:To retrieve the history of task versions, query TASK_VERSIONS
 Account Usage view
 (in the SNOWFLAKE shared database).

Highlight:If any task completes in a FAILED state, Snowflake can automatically retry the task. The automatic task retry is disabled by default. To enable this feature, set TASK_AUTO_RETRY_ATTEMPTS to a value greater than 0.

Highlight:You can set session parameters for the session in which a task runs. To do so, modify an existing task and set the desired parameter values using ALTER TASK
 … SET
 session_parameter
 =
 value
[,
 session_parameter
 =
 value
 ...
 ]
 or edit the task in Snowsight.

Highlight:Viewing task history¶

To view tasks, you must have one or more of the following privileges:

The ACCOUNTADMIN role

The OWNERSHIP privilege on the task

The global MONITOR EXECUTION privilege

Highlight:Drop a task owner role¶

When you delete the owner role of a task, the task transfers ownership to the role that dropped the owner role. When a task transfers ownership, it is automatically paused and new executions aren’t scheduled until the new owner resumes the task.

Highlight:A task can execute any one of the following types of functions:

Single SQL statement

Call to a stored procedure

Procedural logic using Snowflake Scripting",false
834201050,Manage task dependencies with task graphs | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/tasks-graphs,,2024-08-13T08:03:24.230Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Ftasks-graphs,"Highlight:When you transfer ownership of the tasks in a task graph using these methods, the tasks in the task graph retain their relationships to each other.

Highlight:The behavior is controlled by the ALLOW_OVERLAPPING_EXECUTION parameter on the root task; the default value is FALSE. Setting the parameter value to TRUE permits task graph runs to overlap.

Highlight:Task graph considerations

A task graph is limited to a maximum of 1000 tasks.

A single task can have a maximum of 100 parent tasks and 100 child tasks.

The compute running the task graph must be sized to handle concurrent task runs. For more information see, Compute resources.",false
834201018,Working with Joins | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/querying-joins,,2024-08-13T08:03:07.154Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquerying-joins,"Highlight:If two tables have multiple columns in common, then all the common columns are used in the ON
 clause.",false
834200983,Working with Subqueries | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/querying-subqueries,,2024-08-13T08:02:44.633Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquerying-subqueries,"Highlight:A subquery is a query within another query. Subqueries in a FROM
 or WHERE
 clause are used to provide data that will be used to limit or compare/evaluate the data returned by the containing query.

Highlight:Snowflake currently supports the following types of subqueries:

Uncorrelated scalar subqueries in any place that a value expression can be used.

Correlated scalar subqueries in WHERE clauses.

EXISTS, ANY / ALL, and IN subqueries in WHERE clauses. These subqueries can be correlated or uncorrelated.",false
834200959,Querying Hierarchical Data | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/queries-hierarchical,,2024-08-13T08:02:28.179Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fqueries-hierarchical,"Highlight:This topic describes how to store and query hierarchical data using:

JOINs

Recursive CTEs (common table expressions)

CONNECT BY

Highlight:Snowflake provides two ways to query hierarchical data in which the number of levels is not known in advance:

Recursive CTEs (common table expressions).

CONNECT BY clauses.

A recursive CTE allows you to create a WITH clause that can refer to itself. This lets you iterate through each level of your hierarchy and accumulate results.

A CONNECT BY clause allows you to create a type of JOIN operation that processes the hierarchy one level at a time, and allows each level to refer to data in the prior level.",false
834200915,Constructing SQL at runtime | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/querying-construct-at-runtime,,2024-08-13T08:02:02.689Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquerying-construct-at-runtime,"Highlight:The following techniques are available for constructing SQL statements dynamically at runtime:

The TO_QUERY function
 - This function takes a SQL string with optional parameters as input.

Dynamic SQL
 - Code in a stored procedure or application takes input and constructs a dynamic SQL statement using this input. The code can be part of a Snowflake Scripting
 or Javascript
 stored procedure, or a Snowflake Scripting anonymous block. You can also use this technique in your application code that uses a Snowflake driver
 or the Snowflake SQL REST API
.",false
834200866,Analyzing time-series data | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/querying-time-series-data,,2024-08-13T08:01:38.675Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fquerying-time-series-data,"Highlight:You can downsample a data set that is stored in a table by using the TIME_SLICE function. This function calculates the start and end times of fixed-width “buckets” so that individual records can be grouped and summarized, using standard aggregate functions, such as SUM and AVG.

Similarly, the DATE_TRUNC
 function truncates part of a series of date or timestamp values, reducing their granularity.",false
834200838,Monitor query activity with Query History | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/ui-snowsight-activity,,2024-08-13T08:01:18.378Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fui-snowsight-activity,"Highlight:Common query problems identified by Query Profile¶

This section describes some of the problems you can identify and troubleshoot using Query Profile.

“Exploding” joins¶

One of the common mistakes SQL users make is joining tables without providing a join condition (resulting in a “Cartesian product”), or providing a condition where records from one table match multiple records from another table. For such queries, the Join
 operator produces significantly (often by orders of magnitude) more tuples than it consumes.

This can be observed by looking at the number of records produced by a Join
 operator, and typically is also reflected in Join
 operator consuming a lot of time.

UNION without ALL¶

In SQL, it is possible to combine two sets of data with either UNION or UNION ALL constructs. The difference between them is that UNION ALL simply concatenates inputs, while UNION does the same, but also performs duplicate elimination.

A common mistake is to use UNION when the UNION ALL semantics are sufficient. These queries show in Query Profile as a UnionAll
 operator with an extra Aggregate
 operator on top (which performs duplicate elimination).

Queries too large to fit in memory¶

For some operations (e.g. duplicate elimination for a huge data set), the amount of memory available for the servers used to execute the operation might not be sufficient to hold intermediate results. As a result, the query processing engine will start spilling
 the data to local disk. If the local disk space is not sufficient, the spilled data is then saved to remote disks.

This spilling can have a profound effect on query performance (especially if remote disk is used for spilling). To alleviate this, we recommend:

Using a larger warehouse (effectively increasing the available memory/local disk space for the operation), and/or

Processing data in smaller batches.

Inefficient pruning¶

Snowflake collects rich statistics on data allowing it not to read unnecessary parts of a table based on the query filters. However, for this to have an effect, the data storage order needs to be correlated with the query filter attributes.

The efficiency of pruning can be observed by comparing Partitions scanned
 and Partitions total
 statistics in the TableScan
 operators. If the former is a small fraction of the latter, pruning is efficient. If not, the pruning did not have an effect.

Of course, pruning can only help for queries that actually filter out a significant amount of data. If the pruning statistics do not show data reduction, but there is a Filter
 operator above TableScan
 which filters out a number of records, this might signal that a different data organization might be beneficial for this query.

For more information about pruning, see Understanding Snowflake Table Structures
.

Highlight:To monitor query activity in your account, you can use:

The Query History page in Snowsight.

Highlight:With the Query History page in Snowsight, you can do the following:

Monitor queries executed by users in your account.

View details about queries, including performance data. In some cases, query details are unavailable.

Explore each step of an executed query in the query profile.",false
834200794,Data sharing and collaboration in Snowflake | Snowflake Documentation,,,https://docs.snowflake.com/en/guides-overview-sharing,,2024-08-13T08:00:55.885Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fguides-overview-sharing,"Highlight:Direct share¶

Use a direct share to share data with one or more accounts in the same Snowflake region. You don’t need to copy or move data shared with a direct share.",false
834200700,About listing providers | Snowflake Documentation,,,https://other-docs.snowflake.com/en/collaboration/provider-becoming,,2024-08-13T08:00:15.251Z,https://rdl.ink/render/https%3A%2F%2Fother-docs.snowflake.com%2Fen%2Fcollaboration%2Fprovider-becoming,"Highlight:If your account is in a U.S. government region, you must also accept the cross-region disclaimer.

Highlight:To offer paid listings or any listings on the Snowflake Marketplace, you must create a provider profile and have it approved by Snowflake.

Highlight:To offer paid listings, you must set up a Stripe account to get paid for listings.",false
834200466,Overview of Access Control | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-access-control-overview,,2024-08-13T07:58:13.036Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-access-control-overview,"Highlight:Discretionary Access Control (DAC):
 Each object has an owner, who can in turn grant access to that object.

Role-based Access Control (RBAC):
 Access privileges are assigned to roles, which are in turn assigned to users.

Highlight:Securable object:
 An entity to which access can be granted. Unless allowed by a grant, access is denied

Highlight:There are a small number of system-defined roles
 in a Snowflake account. System-defined roles cannot be dropped. In addition, the privileges granted to these roles by Snowflake cannot be revoked.

Highlight:SECURITYADMIN:

(aka Security Administrator)

Role that can manage any object grant globally, as well as create, monitor, and manage users and roles

Highlight:ACCOUNTADMIN:

(aka Account Administrator)

Role that encapsulates the SYSADMIN and SECURITYADMIN system-defined roles. It is the top-level role in the system and should be granted only to a limited/controlled number of users in your account.

Highlight:USERADMIN:

(aka User and Role Administrator)

Role that is dedicated to user and role management only.

Highlight:Role hierarchy and privilege inheritance

Highlight:If a role was specified as part of the connection and that role is a role that has already been granted to the connecting user, the specified role becomes the current role.

If no role was specified and a default role has been set for the connecting user, that role becomes the current role.

If no role was specified and a default role has not been set for the connecting user, the system role PUBLIC is used.

Highlight:To own an object means that a role has the OWNERSHIP privilege on the object.

Highlight:in a managed access schema, object owners lose the ability to make grant decisions. Only the schema owner (i.e. the role with the OWNERSHIP privilege on the schema) or a role with the MANAGE GRANTS privilege can grant privileges on objects in the schema.",false
834200399,Summary of data types | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/intro-summary-data-types,,2024-08-13T07:57:43.153Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fintro-summary-data-types,"Highlight:FLOAT, FLOAT4, FLOAT8

	

[1]




DOUBLE, DOUBLE PRECISION, REAL

	

Synonymous with FLOAT. [1]

Highlight:VARCHAR

	

Default (and maximum) is 16,777,216 bytes.




CHAR, CHARACTER

	

Synonymous with VARCHAR except default length is VARCHAR(1).




STRING

	

Synonymous with VARCHAR.




TEXT

	

Synonymous with VARCHAR.",false
834200123,Snowflake Sessions & Session Policies | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/session-policies,,2024-08-13T07:57:01.360Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsession-policies,"Highlight:A session policy defines the idle
 session timeout period in minutes and provides the option to override the default idle timeout value. The timeout period begins upon a successful authentication to Snowflake. The minimum configurable idle timeout value for a session policy is 5
 minutes.

If a session policy is not set, Snowflake uses a default value of 240
 minutes (four hours).",false
834200042,Access control privileges | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/security-access-control-privileges,,2024-08-13T07:56:14.899Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsecurity-access-control-privileges,"Highlight:CREATE <object_type>

	

Global, Database, Schema

	

Grants the ability to create an object of <object_type>
 (e.g. CREATE TABLE grants the ability to create a table within a schema).

Highlight:Grants the ability to drop, alter, and grant or revoke access to an object. Required to rename an object and create a temporary object with the same name as the object itself. OWNERSHIP is a special privilege on an object that is automatically granted to the role that created the object, but can also be transferred using the GRANT OWNERSHIP command to a different role by the owning role or any role with the MANAGE GRANTS privilege.

Highlight:OWNERSHIP

Highlight:ADD SEARCH OPTIMIZATION

	

Enables adding search optimization to a table in a schema.",false
834199709,Parameters | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/parameters,,2024-08-13T07:54:39.312Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fparameters,"Highlight:ALLOW_CLIENT_MFA_CACHING

Highlight:Type:

Account — Can only be set for Account

Highlight:ABORT_DETACHED_QUERY

Highlight:Specifies the action that Snowflake performs for in-progress queries if connectivity is lost due to abrupt termination of a session (e.g. network outage, browser termination, service interruption).

Highlight:TRUE: In-progress queries are aborted 5 minutes after connectivity is lost.

FALSE: In-progress queries are completed.

Highlight:When the value is set to FALSE, asynchronous queries continue to run until they complete, until they are canceled, or until the time limit specified for the STATEMENT_TIMEOUT_IN_SECONDS parameter expires. The default for the STATEMENT_TIMEOUT_IN_SECONDS parameter is two days.

Highlight:Values:

0 to 604800 (i.e. 7 days) — a value of 0 specifies that the maximum timeout value is enforced.

Default:

172800 (i.e. 2 days)",false
834199588,Understanding & using Time Travel | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/data-time-travel,,2024-08-13T07:54:02.688Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fdata-time-travel,"Highlight:The standard retention period is 1 day (24 hours) and is automatically enabled for all Snowflake accounts:

Highlight:To support Time Travel, the following SQL extensions have been implemented:

AT | BEFORE clause which can be specified in SELECT statements and CREATE … CLONE commands

Highlight:UNDROP command for tables, schemas, and databases.

Highlight:Snowflake Time Travel enables accessing historical data (i.e. data that has been changed or deleted) at any point within a defined period. It serves as a powerful tool for performing the following tasks:

Restoring data-related objects (tables, schemas, and databases) that might have been accidentally or intentionally deleted.

Duplicating and backing up data from key points in the past.

Analyzing data usage/manipulation over specified periods of time.

Highlight:The MIN_DATA_RETENTION_TIME_IN_DAYS account parameter can be set by users with the ACCOUNTADMIN role to set a minimum retention period for the account. This parameter does not alter or replace the DATA_RETENTION_TIME_IN_DAYS parameter value. However it may change the effective data retention time. When this parameter is set at the account level, the effective minimum data retention period for an object is determined by MAX(DATA_RETENTION_TIME_IN_DAYS, MIN_DATA_RETENTION_TIME_IN_DAYS).",false
834199498,USE SECONDARY ROLES | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/use-secondary-roles,,2024-08-13T07:53:16.720Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fuse-secondary-roles,"Highlight:Note that authorization to execute CREATE <object>
 statements to create objects is provided by the primary role.",false
834199474,DESCRIBE USER | Snowflake Documentation,,,https://docs.snowflake.com/en/sql-reference/sql/desc-user,,2024-08-13T07:52:59.823Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fsql-reference%2Fsql%2Fdesc-user,"Highlight:The user object property MINS_TO_BYPASS_NETWORK_POLICY
 defines the number of minutes in which a user can access Snowflake without conforming to an existing network policy
. The number of minutes can only be set by Snowflake (Default: NULL
) and is intended as a temporary workaround to allow user access to Snowflake. To set a value for this property, please contact Snowflake Support
.",false
834199144,Connecting through SnowSQL | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/snowsql-start,,2024-08-13T07:52:03.344Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fsnowsql-start,"Highlight:Connection syntax¶
$ 
snowsql 
<connection_parameters>


Where <connection_parameters>
 are one or more of the following. For detailed descriptions of each parameter, see Connection parameters reference
 (in this topic).

Parameter

	

Description




-a,
 --accountname
 TEXT

	

Your account identifier. Honors $SNOWSQL_ACCOUNT.




-u,
 --username
 TEXT

	

Username to connect to Snowflake. Honors $SNOWSQL_USER.




-d,
 --dbname
 TEXT

	

Database to use. Honors $SNOWSQL_DATABASE.




-s,
 --schemaname
 TEXT

	

Schema in the database to use. Honors $SNOWSQL_SCHEMA.




-r,
 --rolename
 TEXT

	

Role name to use. Honors $SNOWSQL_ROLE.




-w,
 --warehouse
 TEXT

	

Warehouse to use. Honors $SNOWSQL_WAREHOUSE.




-h,
 --host
 TEXT

	

Host address for the connection. Honors $SNOWSQL_HOST. (Deprecated)




-p,
 --port
 INTEGER

	

Port number for the connection. Honors $SNOWSQL_PORT. (Deprecated)




--region
 TEXT

	

Region. Honors $SNOWSQL_REGION. (Deprecated; use -a or –accountname instead)




-m,
 --mfa-passcode
 TEXT

	

Token to use for multi-factor authentication (MFA)




--mfa-passcode-in-password

	

Appends the MFA passcode to the end of the password.




--abort-detached-query

	

Aborts a query if the connection between the client and server is lost. By default, it won’t abort even if the connection is lost.




--probe-connection

	

Test connectivity to Snowflake. This option is mainly used to print out the TLS (Transport Layer Security) certificate chain.




--proxy-host
 TEXT

	

(DEPRECATED. Use HTTPS_PROXY and HTTP_PROXY environment variables.) Proxy server hostname. Honors $SNOWSQL_PROXY_HOST.




--proxy-port
 INTEGER

	

(DEPRECATED. Use HTTPS_PROXY and HTTP_PROXY environment variables.) Proxy server port number. Honors $SNOWSQL_PROXY_PORT.




--proxy-user
 TEXT

	

(DEPRECATED. Use HTTPS_PROXY and HTTP_PROXY environment variables.) Proxy server username. Honors $SNOWSQL_PROXY_USER. Set $SNOWSQL_PROXY_PWD for the proxy server password.




--authenticator
 TEXT

	

Authenticator: ‘snowflake’, ‘externalbrowser’ (to use any IdP and a web browser), https:/
/<okta_account_name>.okta.com (to use Okta natively), or ‘oauth’ to authenticate using OAuth.




-v,
 --version

	

Shows the current SnowSQL version, or uses a specific version if provided as a value.




--noup

	

Disables auto-upgrade for this run. If no version is specified for -v, the latest version in ~/.snowsql/ is used.




-D,
 --variable
 TEXT

	

Sets a variable to be referred by &<var>. -D tablename=CENUSTRACKONE or –variable db_key=$DB_KEY




-o,
 --option
 TEXT

	

Set SnowSQL options. See the options reference in the Snowflake documentation.




-f,
 --filename
 PATH

	

File to execute.




-q,
 --query
 TEXT

	

Query to execute.




--query_tags
 TEXT

	

Tags to use when running queries. By default, --query_tag
 reads the value of the SNOWSQL_QUERY_TAG
 environment variable.




--config
 PATH

	

Path and name of the SnowSQL configuration file. By default, ~/.snowsql/config.




-P,
 --prompt

	

Forces an interactive password prompt
 to allow you to specify a password that differs from the one stored in the $SNOWSQL_PWD environment variable.




-M,
 --mfa-prompt

	

Forces a prompt for the second token for MFA.




-c,
 --connection
 TEXT

	

Named set of connection parameters to use.




--single-transaction

	

Connects with autocommit disabled. Wraps BEGIN/COMMIT around statements to execute them as a single transaction, ensuring all commands complete successfully or no change is applied.




--private-key-path
 PATH

	

Path to private key file.




--disable-request-pooling

	

Disables connection pooling.




-U,
 --upgrade

	

Force upgrade of SnowSQL to the latest version.




-K,
 --client-session-keep-alive

	

Keep the session active indefinitely, even if there is no activity from the user.




--include_connector_version

	

Display the version of the Snowflake Connector for Python software that is packaged in the SnowSQL binary.




-?,
 --help

	

Show this message and exit.

Highlight:-a, --accountname TEXT

	

Your account identifier. Honors $SNOWSQL_ACCOUNT.

Highlight:-d, --dbname TEXT

	

Database to use. Honors $SNOWSQL_DATABASE.",false
834199099,Identifier-first login | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/identifier-first-login,,2024-08-13T07:51:36.841Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fidentifier-first-login,"Highlight:An identifier-first login reduces confusion and login issues by only showing users’ valid authentication options.

Highlight:A user with the ACCOUNTADMIN role can use the ENABLE_IDENTIFIER_FIRST_LOGIN
 parameter to enable the identifier-first login flow for an account. For example:",false
834198963,Authentication policies | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/authentication-policies,,2024-08-13T07:50:08.014Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fauthentication-policies,"Highlight:Authentication policies provide you with control over how a client or user authenticates by allowing you to specify:

Whether users must enroll in multi-factor authentication (MFA)
.

Which authentication methods require multi-factor authentication.

The allowed authentication methods, such as SAML
, passwords, OAuth
, or Key pair authentication
.

The SAML2 security integrations
 that are available to users during the login experience. For example, if there are multiple security integrations, you can specify which identity provider (IdP) can be selected and used to authenticate.

If you are using authentication policies to control which IdP a user can use to authenticate, you can further refine that control using the ALLOWED_USER_DOMAINS
 and ALLOWED_EMAIL_PATTERNS
 properties of the SAML2 security integrations associated with the IdPs. For more details, see Using multiple identity providers for federated authentication
.

The clients that users can use to connect to Snowflake, such as Snowsight
, drivers
, or SnowSQL (CLI client)
. The CLIENT_TYPES
 property of an authentication policy is a best effort method to block user logins based on specific clients. It should not be used as the sole control to establish a security boundary.

You can set authentication policies on the account or users in the account. If you set an authentication policy on the account, then the authentication policy applies to all users in the account. If you set an authentication policy on both an account and a user, then the user-level authentication policy overrides the account-level authentication policy.

Highlight:Ensure authentication methods and security integrations listed in your authentication policies do not conflict. For example, if you add a SAML2 security integration in the list of allowed security integrations, and you only allow OAuth as an allowed authentication method, then you cannot create an authentication policy.

Highlight:The following list describes the order in which security policies are evaluated:

Network policies
: Allow or deny IP addresses, VPC IDs, and VPCE IDs.

Authentication policies - Allow or deny clients, authentication methods, and security integrations.

Password policies
 (For local authentication only): Specify password requirements such as character length, characters, password age, retries, and lockout time.

Session policies
: Require users to re-authenticate after a period of inactivity

If a policy is assigned to both the account and the user authenticating, the user-level policy is enforced.

Highlight:An administrator can use the CREATE AUTHENTICATION POLICY
 command to create a new authentication policy, specifying which clients can connect to Snowflake, which authentication methods can be used, and which security integrations are available to users.

Highlight:You can use the ALTER ACCOUNT
 or ALTER USER
 commands to set an authentication policy on an account or user.

Highlight:Only a security administrator (a user with the SECURITYADMIN role) or users with a role that has the APPLY AUTHENTICATION POLICY privilege can set authentication policies on accounts or users.

Highlight:If you set the MFA_ENROLLMENT
 parameter to REQUIRED
, then the CLIENT_TYPES
 parameter must include SNOWFLAKE_UI
, because Snowsight is the only place users can enroll in multi-factor authentication (MFA)
.

Highlight:CREATE

	

Enables creating a new authentication policy in a schema.




APPLY AUTHENTICATION POLICY

	

Enables applying an authentication policy at the account or user level.




OWNERSHIP

	

Grants full control over the authentication policy. Required to alter most properties of an authentication policy.",false
834198816,External API authentication and secrets | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/api-authentication,,2024-08-13T07:48:14.791Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fapi-authentication,"Highlight:External API authentication provides a pathway to authenticate to a service that is hosted outside of Snowflake. The API request to access the service requires the API request to be authenticated. Snowflake supports the following methods of authentication while using External API Authentication:

Basic authentication.

OAuth with code grant flow.

OAuth with client credentials flow.

Highlight:A secret is a schema-level object that stores sensitive information, limits access to the sensitive information using RBAC
, and is encrypted using the Snowflake key encryption hierarchy
.

Highlight:After you create a secret, only dedicated Snowflake components such as integrations and external functions can read the sensitive information.",false
834198100,Overview of federated authentication and SSO | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/admin-security-fed-auth-overview,,2024-08-13T07:41:19.889Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fadmin-security-fed-auth-overview,"Highlight:In a federated environment, user authentication is separated from user access through the use of one or more external entities that provide independent authentication of user credentials.

Highlight:A federated environment consists of the following components:

Service provider (SP):

In a Snowflake federated environment, Snowflake serves as the SP.

Identity provider (IdP):

The external, independent entity responsible for providing the following services to the SP:

Creating and maintaining user credentials and other profile information.

Authenticating users for SSO access to the SP.

Snowflake supports most
 SAML 2.0-compliant vendors as an IdP; however, certain vendors include native support for Snowflake (see below for details).

Highlight:The following vendors provide native
 Snowflake support for federated authentication and SSO:

Okta
 — hosted service

Microsoft AD FS
 (Active Directory Federation Services) — on-premises software (installed on Windows Server)

Highlight:Note

To use an IdP other than Okta or AD FS, you must define a custom application for Snowflake in the IdP.

Highlight:Currently, only a subset of Snowflake drivers support the use of multiple identity providers. These drivers include JDBC, ODBC, and Python.

Highlight:Federated authentication enables the following SSO workflows:

Logging into Snowflake.

Logging out of Snowflake.

System timeout due to inactivity.

Highlight:When a user logs out, the available options are dictated by whether the IdP supports global
 logout or only standard
 logout:

Standard:

Requires users to explicitly log out of both the IdP and Snowflake to completely disconnect. All IdPs support standard logout.

Global:

Enables a user to log out of the IdP and subsequently all their Snowflake sessions. Support for global logout is IdP-dependent.

Highlight:In addition, the behavior of the system is determined by whether the logout is initiated through Snowflake or the IdP:

Snowflake-initiated logout:

Global logout is not supported from within Snowflake, regardless of whether the IdP supports it. When a user logs out of a Snowflake session, they are logged out of that session only. All their other current Snowflake sessions stay open, as does their IdP session. As a result, they can continue working in their other sessions or they can initiate additional sessions without having to re-authenticate through the IdP.

To completely disconnect, users must explicitly log out of both Snowflake and the IdP.

IdP-initiated logout:

When a user logs out through an IdP, the behavior depends on whether the IdP supports standard logout only or also global logout:

AD FS supports both standard and global logout. If global logout is enabled, the AD FS IdP login page provides an option for signing out from all sites that the user has accessed. Selecting this option and clicking Sign Out
 logs the user out of AD FS and all their Snowflake sessions. To access Snowflake again, they must re-authenticate using AD FS.

Okta supports standard logout only. When a user logs out of Okta, they are not automatically logged out of any of their active Snowflake sessions and they can continue working. However, to initiate any new Snowflake sessions, they must authenticate again through Okta.

All custom providers support standard logout; support for global logout varies by provider.

Note

For a web-based IdP (e.g. Okta), closing the browser tab/window does not necessarily end the IdP session. If a user’s IdP session is still active, they can still access Snowflake until the IdP session times out.

Highlight:Snowflake supports SSO with private connectivity to the Snowflake service for Snowflake accounts on Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP).

Currently, for any given Snowflake account, SSO works with only one account URL at a time: either the public account URL or the URL associated with the private connectivity service on AWS, Microsoft Azure, or Google Cloud Platform.

Highlight:Snowflake supports replication and failover/failback of the SAML2 security integration
 from a source account to a target account.

Highlight:Federated authentication enables the following SSO workflows:

Logging into Snowflake.

Logging out of Snowflake.

System timeout due to inactivity.

The behavior for each workflow is determined by whether the action is initiated within Snowflake or your IdP.",false
834198062,Managing/Using federated authentication | Snowflake Documentation,,,https://docs.snowflake.com/en/user-guide/admin-security-fed-auth-use,,2024-08-13T07:40:52.685Z,https://rdl.ink/render/https%3A%2F%2Fdocs.snowflake.com%2Fen%2Fuser-guide%2Fadmin-security-fed-auth-use,"Highlight:With federated authentication enabled for your account, Snowflake still allows maintaining and using Snowflake user credentials (login name and password). In other words:

Account and security administrators can still create users with passwords maintained in Snowflake.

Users can still log into Snowflake using their Snowflake credentials.

However, if federated authentication is enabled for your account, Snowflake does not
 recommend maintaining user passwords in Snowflake. Instead, user passwords should be maintained solely in your IdP.

Highlight:Specifically, we recommend that you disable Snowflake authentication for all non-administrator users.",false
